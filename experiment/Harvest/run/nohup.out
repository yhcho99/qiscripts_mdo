⠋ [Compustat API] : Cleaning past days cache✓ [Compustat API] : Cleaning past days cache done in 0.0 secs
auto caching이 활성화 되었습니다. /home/sronly/sr-storage/kirin_cache/kirin_api_cache/2.19.17_2021-11-23에 캐쉬를 저장합니다.
⠋ [Compustat API] : Loading meta table⠙ [Compustat API] : Loading meta table⠹ [Compustat API] : Loading meta table⠸ [Compustat API] : Loading meta table⠼ [Compustat API] : Loading meta table⠴ [Compustat API] : Loading meta table⠦ [Compustat API] : Loading meta table⠧ [Compustat API] : Loading meta table⠇ [Compustat API] : Loading meta table⠏ [Compustat API] : Loading meta table⠋ [Compustat API] : Loading meta table⠙ [Compustat API] : Loading meta table⠹ [Compustat API] : Loading meta table⠸ [Compustat API] : Loading meta table⠼ [Compustat API] : Loading meta table⠴ [Compustat API] : Loading meta table⠦ [Compustat API] : Loading meta table⠧ [Compustat API] : Loading meta table⠇ [Compustat API] : Loading meta table⠏ [Compustat API] : Loading meta table⠋ [Compustat API] : Loading meta table⠙ [Compustat API] : Loading meta table⠹ [Compustat API] : Loading meta table⠸ [Compustat API] : Loading meta table⠼ [Compustat API] : Loading meta table⠴ [Compustat API] : Loading meta table⠦ [Compustat API] : Loading meta table⠧ [Compustat API] : Loading meta table⠇ [Compustat API] : Loading meta table⠏ [Compustat API] : Loading meta table⠋ [Compustat API] : Loading meta table⠙ [Compustat API] : Loading meta table⠹ [Compustat API] : Loading meta table⠸ [Compustat API] : Loading meta table⠼ [Compustat API] : Loading meta table⠴ [Compustat API] : Loading meta table⠦ [Compustat API] : Loading meta table⠧ [Compustat API] : Loading meta table⠇ [Compustat API] : Loading meta table⠏ [Compustat API] : Loading meta table⠋ [Compustat API] : Loading meta table⠙ [Compustat API] : Loading meta table⠹ [Compustat API] : Loading meta table⠸ [Compustat API] : Loading meta table⠼ [Compustat API] : Loading meta table⠴ [Compustat API] : Loading meta table⠦ [Compustat API] : Loading meta table⠧ [Compustat API] : Loading meta table⠇ [Compustat API] : Loading meta table⠏ [Compustat API] : Loading meta table⠋ [Compustat API] : Loading meta table⠙ [Compustat API] : Loading meta table⠹ [Compustat API] : Loading meta table⠸ [Compustat API] : Loading meta table⠼ [Compustat API] : Loading meta table⠴ [Compustat API] : Loading meta table⠦ [Compustat API] : Loading meta table⠧ [Compustat API] : Loading meta table⠇ [Compustat API] : Loading meta table⠏ [Compustat API] : Loading meta table⠋ [Compustat API] : Loading meta table⠙ [Compustat API] : Loading meta table⠹ [Compustat API] : Loading meta table⠸ [Compustat API] : Loading meta table⠼ [Compustat API] : Loading meta table⠴ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 29.798 secs
⠋ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading masking data⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading masking data⠦ [Compustat API] : Loading masking data⠧ [Compustat API] : Loading masking data⠇ [Compustat API] : Loading masking data⠏ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading masking data⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading masking data⠦ [Compustat API] : Loading masking data⠧ [Compustat API] : Loading masking data⠇ [Compustat API] : Loading masking data⠏ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading masking data⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading masking data⠦ [Compustat API] : Loading masking data⠧ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 8.609 secs
setting tensorflow random seed failed
load_data: op_lev
load_data: roe
load_data: std_u_e
load_data: t1y_ff
load_data: t5y_ff
load_data: t10y_t2y
load_data: retail_mfr
load_data: m1
load_data: m2
load_data: pr_1m_0m
load_data: mv
load_data: btm
load_data: t3m
load_data: t6m
load_data: t2y
load_data: t3y
load_data: t5y
load_data: t7y
load_data: aaa_t10y
load_data: baa_t10y
load_data: aaa_ff
load_data: export_growth
load_data: import_growth
load_data: real_gig
load_data: gpa
load_data: rev_surp
load_data: cash_at
load_data: core_ppi
load_data: cpi
load_data: pce
load_data: corporate_tg
load_data: industrial_prod
load_data: home_pr
load_data: res_mom_12m_1m_0m
load_data: res_vol_6m_3m_0m
load_data: at
load_data: mom_12m_1m
load_data: ram_12m_0m
load_data: vol_3m
load_data: t10y
load_data: aaa
load_data: baa
load_data: baa_ff
load_data: core_cpi
load_data: core_pce
load_data: real_pig
load_data: federal_tg
load_data: real_gdp
load_data: ae_m
load_data: ia_ta
load_data: rc_a
load_data: snp500_pr
load_data: wilshire500_pr
load_data: ted
load_data: ppi
load_data: trimmed_pce
load_data: unemploy
load_data: snp500_vol
load_data: ret_noa
load_data: etm
load_data: ia_mv
load_data: r_s
load_data: r_a
load_data: fred_ff
load_data: wti
load_data: capa_util
load_data: snp500_pe
load_data: sector_values
load_data: mv
load_data: pr_1m_0m
load_data: mv
making sample. (inference: True, date:2015-12-31 00:00:00, date_number: 348)
making sample. (inference: False, date:2012-12-31 00:00:00, date_number: 312)
making sample. (inference: False, date:2013-01-31 00:00:00, date_number: 313)
making sample. (inference: False, date:2013-02-28 00:00:00, date_number: 314)
making sample. (inference: False, date:2013-03-31 00:00:00, date_number: 315)
making sample. (inference: False, date:2013-04-30 00:00:00, date_number: 316)
making sample. (inference: False, date:2013-05-31 00:00:00, date_number: 317)
making sample. (inference: False, date:2013-06-30 00:00:00, date_number: 318)
making sample. (inference: False, date:2013-07-31 00:00:00, date_number: 319)
making sample. (inference: False, date:2013-08-31 00:00:00, date_number: 320)
making sample. (inference: False, date:2013-09-30 00:00:00, date_number: 321)
making sample. (inference: False, date:2013-10-31 00:00:00, date_number: 322)
making sample. (inference: False, date:2013-11-30 00:00:00, date_number: 323)
making sample. (inference: False, date:2013-12-31 00:00:00, date_number: 324)
making sample. (inference: False, date:2014-01-31 00:00:00, date_number: 325)
making sample. (inference: False, date:2014-02-28 00:00:00, date_number: 326)
making sample. (inference: False, date:2014-03-31 00:00:00, date_number: 327)
making sample. (inference: False, date:2014-04-30 00:00:00, date_number: 328)
making sample. (inference: False, date:2014-05-31 00:00:00, date_number: 329)
making sample. (inference: False, date:2014-06-30 00:00:00, date_number: 330)
making sample. (inference: False, date:2014-07-31 00:00:00, date_number: 331)
making sample. (inference: False, date:2014-08-31 00:00:00, date_number: 332)
making sample. (inference: False, date:2014-09-30 00:00:00, date_number: 333)
making sample. (inference: False, date:2014-10-31 00:00:00, date_number: 334)
making sample. (inference: False, date:2014-11-30 00:00:00, date_number: 335)
making sample. (inference: False, date:2014-12-31 00:00:00, date_number: 336)
making sample. (inference: False, date:2015-01-31 00:00:00, date_number: 337)
making sample. (inference: False, date:2015-02-28 00:00:00, date_number: 338)
making sample. (inference: False, date:2015-03-31 00:00:00, date_number: 339)
making sample. (inference: False, date:2015-04-30 00:00:00, date_number: 340)
making sample. (inference: False, date:2015-05-31 00:00:00, date_number: 341)
making sample. (inference: False, date:2015-06-30 00:00:00, date_number: 342)
making sample. (inference: False, date:2015-07-31 00:00:00, date_number: 343)
making sample. (inference: False, date:2015-08-31 00:00:00, date_number: 344)
making sample. (inference: False, date:2015-09-30 00:00:00, date_number: 345)
making sample. (inference: False, date:2015-10-31 00:00:00, date_number: 346)
making sample. (inference: False, date:2015-11-30 00:00:00, date_number: 347)
[AttentionModel/2015-12-31 | get_logger | INFO]: ====================AttentionModel/2015-12-31 start====================
making sample. (inference: True, date:2016-01-31 00:00:00, date_number: 349)
making sample. (inference: False, date:2015-12-31 00:00:00, date_number: 348)
[AttentionModel/2016-01-31 | get_logger | INFO]: ====================AttentionModel/2016-01-31 start====================
[strategy_integration.py] set_seed(2015-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5876)training: 2/10 (0.5876)validation : 2/10 (0.5849)training: 3/10 (0.5849)validation : 3/10 (0.5816)training: 4/10 (0.5816)validation : 4/10 (0.5816)training: 5/10 (0.5816)validation : 5/10 (0.5816)training: 6/10 (0.5816)validation : 6/10 (0.5816)early stopping at 6 with loss 0.5816
AttentionModel-training is done: 6/10
2015-12-31 | reset count: 0 | final loss: 0.5816 at epoch 3
making sample. (inference: True, date:2016-02-29 00:00:00, date_number: 350)
making sample. (inference: False, date:2016-01-31 00:00:00, date_number: 349)
[AttentionModel/2016-02-29 | get_logger | INFO]: ====================AttentionModel/2016-02-29 start====================
[strategy_integration.py] set_seed(2016-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5811)training: 2/10 (0.5811)validation : 2/10 (0.5769)training: 3/10 (0.5769)validation : 3/10 (0.5736)training: 4/10 (0.5736)validation : 4/10 (0.5736)training: 5/10 (0.5736)validation : 5/10 (0.5736)training: 6/10 (0.5736)validation : 6/10 (0.5736)early stopping at 6 with loss 0.5736
AttentionModel-training is done: 6/10
2016-01-31 | reset count: 0 | final loss: 0.5736 at epoch 3
making sample. (inference: True, date:2016-03-31 00:00:00, date_number: 351)
making sample. (inference: False, date:2016-02-29 00:00:00, date_number: 350)
[AttentionModel/2016-03-31 | get_logger | INFO]: ====================AttentionModel/2016-03-31 start====================
[strategy_integration.py] set_seed(2016-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5774)training: 2/10 (0.5774)validation : 2/10 (0.5732)training: 3/10 (0.5732)validation : 3/10 (0.5732)training: 4/10 (0.5732)validation : 4/10 (0.5732)training: 5/10 (0.5732)validation : 5/10 (0.5732)training: 6/10 (0.5732)validation : 6/10 (0.5732)early stopping at 6 with loss 0.5732
AttentionModel-training is done: 6/10
2016-02-29 | reset count: 0 | final loss: 0.5732 at epoch 2
making sample. (inference: True, date:2016-04-30 00:00:00, date_number: 352)
making sample. (inference: False, date:2016-03-31 00:00:00, date_number: 351)
[AttentionModel/2016-04-30 | get_logger | INFO]: ====================AttentionModel/2016-04-30 start====================
[strategy_integration.py] set_seed(2016-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5733)training: 2/10 (0.5733)validation : 2/10 (0.5733)training: 3/10 (0.5733)validation : 3/10 (0.5733)training: 4/10 (0.5733)validation : 4/10 (0.5733)training: 5/10 (0.5733)validation : 5/10 (0.5701)training: 6/10 (0.5701)validation : 6/10 (0.5701)training: 7/10 (0.5701)validation : 7/10 (0.5701)training: 8/10 (0.5701)validation : 8/10 (0.5701)early stopping at 8 with loss 0.5701
AttentionModel-training is done: 8/10
2016-03-31 | reset count: 0 | final loss: 0.5701 at epoch 5
making sample. (inference: True, date:2016-05-31 00:00:00, date_number: 353)
making sample. (inference: False, date:2016-04-30 00:00:00, date_number: 352)
[strategy_integration.py] set_seed(2016-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5791)training: 2/10 (0.5791)validation : 2/10 (0.5791)training: 3/10 (0.5791)validation : 3/10 (0.5780)training: 4/10 (0.5780)validation : 4/10 (0.5780)training: 5/10 (0.5780)validation : 5/10 (0.5780)training: 6/10 (0.5780)validation : 6/10 (0.5780)early stopping at 6 with loss 0.5780
AttentionModel-training is done: 6/10
2016-04-30 | reset count: 0 | final loss: 0.5780 at epoch 3
[AttentionModel/2016-05-31 | get_logger | INFO]: ====================AttentionModel/2016-05-31 start====================
making sample. (inference: True, date:2016-06-30 00:00:00, date_number: 354)
making sample. (inference: False, date:2016-05-31 00:00:00, date_number: 353)
[AttentionModel/2016-06-30 | get_logger | INFO]: ====================AttentionModel/2016-06-30 start====================
[strategy_integration.py] set_seed(2016-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5834)training: 2/10 (0.5834)validation : 2/10 (0.5794)training: 3/10 (0.5794)validation : 3/10 (0.5789)training: 4/10 (0.5789)validation : 4/10 (0.5752)training: 5/10 (0.5752)validation : 5/10 (0.5752)training: 6/10 (0.5752)validation : 6/10 (0.5752)early stopping at 6 with loss 0.5752
AttentionModel-training is done: 6/10
2016-05-31 | reset count: 0 | final loss: 0.5752 at epoch 4
making sample. (inference: True, date:2016-07-31 00:00:00, date_number: 355)
making sample. (inference: False, date:2016-06-30 00:00:00, date_number: 354)
[AttentionModel/2016-07-31 | get_logger | INFO]: ====================AttentionModel/2016-07-31 start====================
[strategy_integration.py] set_seed(2016-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5778)training: 2/10 (0.5778)validation : 2/10 (0.5774)training: 3/10 (0.5774)validation : 3/10 (0.5764)training: 4/10 (0.5764)validation : 4/10 (0.5764)training: 5/10 (0.5764)validation : 5/10 (0.5764)training: 6/10 (0.5764)validation : 6/10 (0.5764)early stopping at 6 with loss 0.5764
AttentionModel-training is done: 6/10
2016-06-30 | reset count: 0 | final loss: 0.5764 at epoch 3
making sample. (inference: True, date:2016-08-31 00:00:00, date_number: 356)
making sample. (inference: False, date:2016-07-31 00:00:00, date_number: 355)
[AttentionModel/2016-08-31 | get_logger | INFO]: ====================AttentionModel/2016-08-31 start====================
[strategy_integration.py] set_seed(2016-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5849)training: 2/10 (0.5849)validation : 2/10 (0.5801)training: 3/10 (0.5801)validation : 3/10 (0.5801)training: 4/10 (0.5801)validation : 4/10 (0.5777)training: 5/10 (0.5777)validation : 5/10 (0.5777)training: 6/10 (0.5777)validation : 6/10 (0.5777)training: 7/10 (0.5777)validation : 7/10 (0.5774)training: 8/10 (0.5774)validation : 8/10 (0.5774)early stopping at 8 with loss 0.5774
AttentionModel-training is done: 8/10
2016-07-31 | reset count: 0 | final loss: 0.5774 at epoch 7
[strategy_integration.py] set_seed(2016-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5743)training: 2/10 (0.5743)validation : 2/10 (0.5733)training: 3/10 (0.5733)validation : 3/10 (0.5733)training: 4/10 (0.5733)validation : 4/10 (0.5729)training: 5/10 (0.5729)validation : 5/10 (0.5723)training: 6/10 (0.5723)validation : 6/10 (0.5723)training: 7/10 (0.5723)validation : 7/10 (0.5723)early stopping at 7 with loss 0.5723
AttentionModel-training is done: 7/10
2016-08-31 | reset count: 0 | final loss: 0.5723 at epoch 5
making sample. (inference: True, date:2016-09-30 00:00:00, date_number: 357)
making sample. (inference: False, date:2016-08-31 00:00:00, date_number: 356)
[AttentionModel/2016-09-30 | get_logger | INFO]: ====================AttentionModel/2016-09-30 start====================
making sample. (inference: True, date:2016-10-31 00:00:00, date_number: 358)
making sample. (inference: False, date:2016-09-30 00:00:00, date_number: 357)
[AttentionModel/2016-10-31 | get_logger | INFO]: ====================AttentionModel/2016-10-31 start====================
[strategy_integration.py] set_seed(2016-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5799)training: 2/10 (0.5799)validation : 2/10 (0.5799)training: 3/10 (0.5799)validation : 3/10 (0.5786)training: 4/10 (0.5786)validation : 4/10 (0.5782)training: 5/10 (0.5782)validation : 5/10 (0.5766)training: 6/10 (0.5766)validation : 6/10 (0.5725)training: 7/10 (0.5725)validation : 7/10 (0.5725)training: 8/10 (0.5725)validation : 8/10 (0.5725)early stopping at 8 with loss 0.5725
AttentionModel-training is done: 8/10
2016-09-30 | reset count: 0 | final loss: 0.5725 at epoch 6
[strategy_integration.py] set_seed(2016-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5773)training: 2/10 (0.5773)validation : 2/10 (0.5769)training: 3/10 (0.5769)validation : 3/10 (0.5744)training: 4/10 (0.5744)validation : 4/10 (0.5744)training: 5/10 (0.5744)validation : 5/10 (0.5731)training: 6/10 (0.5731)validation : 6/10 (0.5731)early stopping at 6 with loss 0.5731
AttentionModel-training is done: 6/10
2016-10-31 | reset count: 0 | final loss: 0.5731 at epoch 5
making sample. (inference: True, date:2016-11-30 00:00:00, date_number: 359)
making sample. (inference: False, date:2016-10-31 00:00:00, date_number: 358)
[AttentionModel/2016-11-30 | get_logger | INFO]: ====================AttentionModel/2016-11-30 start====================
making sample. (inference: True, date:2016-12-31 00:00:00, date_number: 360)
making sample. (inference: False, date:2016-11-30 00:00:00, date_number: 359)
[AttentionModel/2016-12-31 | get_logger | INFO]: ====================AttentionModel/2016-12-31 start====================
[strategy_integration.py] set_seed(2016-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5724)training: 2/10 (0.5724)validation : 2/10 (0.5707)training: 3/10 (0.5707)validation : 3/10 (0.5680)training: 4/10 (0.5680)validation : 4/10 (0.5680)training: 5/10 (0.5680)validation : 5/10 (0.5680)training: 6/10 (0.5680)validation : 6/10 (0.5680)early stopping at 6 with loss 0.5680
AttentionModel-training is done: 6/10
2016-11-30 | reset count: 0 | final loss: 0.5680 at epoch 3
[strategy_integration.py] set_seed(2016-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5700)training: 2/10 (0.5700)validation : 2/10 (0.5690)training: 3/10 (0.5690)validation : 3/10 (0.5655)training: 4/10 (0.5655)validation : 4/10 (0.5655)training: 5/10 (0.5655)validation : 5/10 (0.5642)training: 6/10 (0.5642)validation : 6/10 (0.5642)early stopping at 6 with loss 0.5642
AttentionModel-training is done: 6/10
2016-12-31 | reset count: 0 | final loss: 0.5642 at epoch 5
making sample. (inference: True, date:2017-01-31 00:00:00, date_number: 361)
making sample. (inference: False, date:2016-12-31 00:00:00, date_number: 360)
[AttentionModel/2017-01-31 | get_logger | INFO]: ====================AttentionModel/2017-01-31 start====================
making sample. (inference: True, date:2017-02-28 00:00:00, date_number: 362)
making sample. (inference: False, date:2017-01-31 00:00:00, date_number: 361)
[AttentionModel/2017-02-28 | get_logger | INFO]: ====================AttentionModel/2017-02-28 start====================
[strategy_integration.py] set_seed(2017-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5702)training: 2/10 (0.5702)validation : 2/10 (0.5702)training: 3/10 (0.5702)validation : 3/10 (0.5702)training: 4/10 (0.5702)validation : 4/10 (0.5702)training: 5/10 (0.5702)validation : 5/10 (0.5702)early stopping at 5 with loss 0.5702
AttentionModel-training is done: 5/10
2017-01-31 | reset count: 0 | final loss: 0.5702 at epoch 1
[strategy_integration.py] set_seed(2017-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5631)training: 2/10 (0.5631)validation : 2/10 (0.5622)training: 3/10 (0.5622)validation : 3/10 (0.5622)training: 4/10 (0.5622)validation : 4/10 (0.5622)training: 5/10 (0.5622)validation : 5/10 (0.5622)early stopping at 5 with loss 0.5622
AttentionModel-training is done: 5/10
2017-02-28 | reset count: 0 | final loss: 0.5622 at epoch 2
making sample. (inference: True, date:2017-03-31 00:00:00, date_number: 363)
making sample. (inference: False, date:2017-02-28 00:00:00, date_number: 362)
[AttentionModel/2017-03-31 | get_logger | INFO]: ====================AttentionModel/2017-03-31 start====================
making sample. (inference: True, date:2017-04-30 00:00:00, date_number: 364)
making sample. (inference: False, date:2017-03-31 00:00:00, date_number: 363)
[AttentionModel/2017-04-30 | get_logger | INFO]: ====================AttentionModel/2017-04-30 start====================
Traceback (most recent call last):
  File "qrft_harvest_pick88_test01.py", line 532, in <module>
    run_proc.run_process(proc)
  File "/home/sronly/Projects/qiscripts/utils/multiprocess.py", line 20, in run_process
    time.sleep(0.1)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/engineio/client.py", line 39, in signal_handler
    return original_signal_handler(sig, frame)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/socketio/client.py", line 26, in signal_handler
    return original_signal_handler(sig, frame)
KeyboardInterrupt
[strategy_integration.py] set_seed(2017-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)Process Process-25:
Traceback (most recent call last):
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "qrft_harvest_pick88_test01.py", line 375, in qrft_running
    step_info = di.validation(data)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/strategy_integration/components/integration/deep/deep_integration.py", line 559, in validation
    step_info = model.validate(data)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/strategy_integration/components/models/deep_models/base_model.py", line 210, in validate
    step_info = self.calculate_step_info_with_loss(data, is_infer=is_infer)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/strategy_integration/components/models/deep_models/attention_model.py", line 325, in calculate_step_info_with_loss
    loss = self.calculate_loss(data, is_infer)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/strategy_integration/components/models/deep_models/attention_model.py", line 298, in calculate_loss
    out = self.net(org_x)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/strategy_integration/components/models/deep_models/attention_model.py", line 179, in forward
    cx = self.cs_layers(cx)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/strategy_integration/components/models/deep_models/attention_model.py", line 69, in forward
    attn = self.attn_dropout(self.o_fc(v))
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/strategy_integration/components/models/deep_models/attention_model.py", line 28, in forward
    return self._linear(x)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/torch/nn/functional.py", line 1692, in linear
    output = input.matmul(weight.t())
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/engineio/client.py", line 39, in signal_handler
    return original_signal_handler(sig, frame)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/socketio/client.py", line 26, in signal_handler
    return original_signal_handler(sig, frame)
KeyboardInterrupt
[strategy_integration.py] set_seed(2017-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5747)training: 2/10 (0.5747)validation : 2/10 (0.5747)training: 3/10 (0.5747)validation : 3/10 (0.5722)training: 4/10 (0.5722)validation : 4/10 (0.5689)training: 5/10 (0.5689)validation : 5/10 (0.5686)Process Process-24:
Traceback (most recent call last):
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "qrft_harvest_pick88_test01.py", line 350, in qrft_running
    step_info = di.train(data)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/strategy_integration/components/integration/deep/deep_integration.py", line 538, in train
    step_info = model.fit(
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/strategy_integration/components/models/deep_models/base_model.py", line 233, in fit
    step_info = self._fit_if_no_adversarial(data, params)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/strategy_integration/components/models/deep_models/base_model.py", line 265, in _fit_if_no_adversarial
    step_info = self.calculate_step_info_with_loss(data)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/strategy_integration/components/models/deep_models/attention_model.py", line 325, in calculate_step_info_with_loss
    loss = self.calculate_loss(data, is_infer)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/strategy_integration/components/models/deep_models/attention_model.py", line 262, in calculate_loss
    org_x[torch.isnan(org_x)] = 0
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/engineio/client.py", line 39, in signal_handler
    return original_signal_handler(sig, frame)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/socketio/client.py", line 26, in signal_handler
    return original_signal_handler(sig, frame)
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/engineio/client.py", line 39, in signal_handler
    return original_signal_handler(sig, frame)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/socketio/client.py", line 26, in signal_handler
    return original_signal_handler(sig, frame)
KeyboardInterrupt
Traceback (most recent call last):
  File "qrft_harvest_pick88_test02.py", line 5, in <module>
    import pandas as pd
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/__init__.py", line 11, in <module>
    __import__(dependency)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/numpy/__init__.py", line 143, in <module>
    from . import lib
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/numpy/lib/__init__.py", line 25, in <module>
    from .index_tricks import *
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/numpy/lib/index_tricks.py", line 11, in <module>
    import numpy.matrixlib as matrixlib
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/numpy/matrixlib/__init__.py", line 4, in <module>
    from .defmatrix import *
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/numpy/matrixlib/defmatrix.py", line 11, in <module>
    from numpy.linalg import matrix_power
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/numpy/linalg/__init__.py", line 73, in <module>
    from .linalg import *
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/numpy/linalg/linalg.py", line 32, in <module>
    from numpy.lib.twodim_base import triu, eye
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 779, in exec_module
  File "<frozen importlib._bootstrap_external>", line 911, in get_code
  File "<frozen importlib._bootstrap_external>", line 580, in _compile_bytecode
KeyboardInterrupt
Traceback (most recent call last):
  File "qrft_harvest_pick88_test03.py", line 5, in <module>
    import pandas as pd
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/__init__.py", line 51, in <module>
    from pandas.core.api import (
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/api.py", line 31, in <module>
    from pandas.core.groupby import Grouper, NamedAgg
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/groupby/__init__.py", line 1, in <module>
    from pandas.core.groupby.generic import DataFrameGroupBy, NamedAgg, SeriesGroupBy
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/groupby/generic.py", line 65, in <module>
    from pandas.core.frame import DataFrame
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/frame.py", line 119, in <module>
    from pandas.core import algorithms, common as com, generic, nanops, ops
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 657, in _load_unlocked
  File "<frozen importlib._bootstrap>", line 561, in module_from_spec
  File "<frozen importlib._bootstrap>", line 36, in _new_module
KeyboardInterrupt
Traceback (most recent call last):
  File "qrft_harvest_pick108_test01.py", line 5, in <module>
    import pandas as pd
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/__init__.py", line 51, in <module>
    from pandas.core.api import (
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/api.py", line 6, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py", line 97, in <module>
    class CategoricalDtype(PandasExtensionDtype, ExtensionDtype):
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py", line 450, in CategoricalDtype
    def construct_array_type(cls) -> Type["Categorical"]:
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/typing.py", line 258, in inner
    return cached(*args, **kwds)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/typing.py", line 687, in __getitem__
    return _subs_tvars(self, self.__parameters__, params)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/typing.py", line 201, in _subs_tvars
    if tp.__origin__ is Union:
KeyboardInterrupt
Traceback (most recent call last):
  File "qrft_harvest_pick108_test02.py", line 5, in <module>
    import pandas as pd
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/__init__.py", line 51, in <module>
    from pandas.core.api import (
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/api.py", line 15, in <module>
    from pandas.core.arrays import Categorical
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/arrays/__init__.py", line 1, in <module>
    from pandas.core.arrays.base import (
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/arrays/base.py", line 47, in <module>
    from pandas.core import ops
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/ops/__init__.py", line 21, in <module>
    from pandas.core.ops.array_ops import (  # noqa:F401
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/ops/array_ops.py", line 36, in <module>
    from pandas.core.ops.invalid import invalid_comparison
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 971, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 917, in _find_spec
KeyboardInterrupt
Traceback (most recent call last):
  File "qrft_harvest_pick108_test03.py", line 5, in <module>
    import pandas as pd
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/__init__.py", line 51, in <module>
    from pandas.core.api import (
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/api.py", line 14, in <module>
    from pandas.core.algorithms import factorize, unique, value_counts
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/algorithms.py", line 754, in <module>
    def value_counts(
KeyboardInterrupt
Traceback (most recent call last):
  File "qrft_harvest_pick188_test01.py", line 5, in <module>
    import pandas as pd
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/__init__.py", line 51, in <module>
    from pandas.core.api import (
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/api.py", line 15, in <module>
    from pandas.core.arrays import Categorical
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/arrays/__init__.py", line 1, in <module>
    from pandas.core.arrays.base import (
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/arrays/base.py", line 47, in <module>
    from pandas.core import ops
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/ops/__init__.py", line 21, in <module>
    from pandas.core.ops.array_ops import (  # noqa:F401
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/ops/array_ops.py", line 13, in <module>
    from pandas._libs import Timedelta, Timestamp, lib, ops as libops
  File "pandas/_libs/ops.pyx", line 1, in init pandas._libs.ops
  File "<frozen importlib._bootstrap>", line 194, in _lock_unlock_module
KeyboardInterrupt
setting tensorflow random seed failed
Traceback (most recent call last):
  File "qrft_harvest_pick188_test02.py", line 24, in <module>
    from strategy_simulation.strategy import Strategy
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/strategy_simulation/__init__.py", line 4, in <module>
    from .strategy import Strategy
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/strategy_simulation/strategy/__init__.py", line 1, in <module>
    from .strategy import Strategy
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/strategy_simulation/strategy/strategy.py", line 18, in <module>
    from strategy_simulation.analyzer.daily import backtest_daily_with_given_portfolio, _ASSUMED_R
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/strategy_simulation/analyzer/daily.py", line 13, in <module>
    from strategy_simulation.helper.date import OPEN_DAYS, LAST_DATE
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/strategy_simulation/helper/date.py", line 9, in <module>
    NYSE = tc.get_calendar('XNYS')
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/trading_calendars/calendar_utils.py", line 199, in get_calendar
    calendar = self._calendars[canonical_name] = factory()
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/trading_calendars/trading_calendar.py", line 109, in __init__
    _all_days = date_range(start, end, freq=self.day, tz=UTC)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/indexes/datetimes.py", line 1069, in date_range
    dtarr = DatetimeArray._generate_range(
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/arrays/datetimes.py", line 416, in _generate_range
    values = np.array([x.value for x in xdr], dtype=np.int64)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/arrays/datetimes.py", line 416, in <listcomp>
    values = np.array([x.value for x in xdr], dtype=np.int64)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/arrays/datetimes.py", line 2454, in generate_range
    next_date = offset.apply(cur)
  File "pandas/_libs/tslibs/offsets.pyx", line 168, in pandas._libs.tslibs.offsets.apply_wraps.wrapper
  File "pandas/_libs/tslibs/offsets.pyx", line 3232, in pandas._libs.tslibs.offsets.CustomBusinessDay.apply
  File "<__array_function__ internals>", line 5, in busday_offset
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/engineio/client.py", line 39, in signal_handler
    return original_signal_handler(sig, frame)
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/socketio/client.py", line 26, in signal_handler
    return original_signal_handler(sig, frame)
KeyboardInterrupt
Traceback (most recent call last):
  File "qrft_harvest_pick188_test03.py", line 5, in <module>
    import pandas as pd
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/__init__.py", line 51, in <module>
    from pandas.core.api import (
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/api.py", line 15, in <module>
    from pandas.core.arrays import Categorical
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/arrays/__init__.py", line 8, in <module>
    from pandas.core.arrays.datetimes import DatetimeArray
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 779, in exec_module
  File "<frozen importlib._bootstrap_external>", line 911, in get_code
  File "<frozen importlib._bootstrap_external>", line 580, in _compile_bytecode
KeyboardInterrupt
Traceback (most recent call last):
  File "qrft_harvest_pick250_test01.py", line 5, in <module>
    import pandas as pd
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/__init__.py", line 51, in <module>
    from pandas.core.api import (
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/api.py", line 15, in <module>
    from pandas.core.arrays import Categorical
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/arrays/__init__.py", line 17, in <module>
    from pandas.core.arrays.timedeltas import TimedeltaArray
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 779, in exec_module
  File "<frozen importlib._bootstrap_external>", line 874, in get_code
  File "<frozen importlib._bootstrap_external>", line 972, in get_data
KeyboardInterrupt
Traceback (most recent call last):
  File "qrft_harvest_pick250_test02.py", line 5, in <module>
    import pandas as pd
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/__init__.py", line 51, in <module>
    from pandas.core.api import (
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/api.py", line 6, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py", line 27, in <module>
    from pandas.core.dtypes.base import ExtensionDtype, register_extension_dtype
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/dtypes/base.py", line 12, in <module>
    from pandas.core.dtypes.generic import ABCDataFrame, ABCIndexClass, ABCSeries
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/dtypes/generic.py", line 111, in <module>
    ABCTimedeltaArray = create_pandas_abc_type(
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/dtypes/generic.py", line 36, in create_pandas_abc_type
    return meta(name, (), dct)
KeyboardInterrupt
Traceback (most recent call last):
  File "qrft_harvest_pick250_test03.py", line 5, in <module>
    import pandas as pd
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/__init__.py", line 51, in <module>
    from pandas.core.api import (
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/api.py", line 15, in <module>
    from pandas.core.arrays import Categorical
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/arrays/__init__.py", line 1, in <module>
    from pandas.core.arrays.base import (
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/arrays/base.py", line 47, in <module>
    from pandas.core import ops
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/ops/__init__.py", line 21, in <module>
    from pandas.core.ops.array_ops import (  # noqa:F401
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/ops/array_ops.py", line 13, in <module>
    from pandas._libs import Timedelta, Timestamp, lib, ops as libops
  File "pandas/_libs/ops.pyx", line 1, in init pandas._libs.ops
  File "<frozen importlib._bootstrap>", line 194, in _lock_unlock_module
KeyboardInterrupt
Traceback (most recent call last):
  File "qrft_harvest_pick350_test01.py", line 5, in <module>
    import pandas as pd
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/__init__.py", line 51, in <module>
    from pandas.core.api import (
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/api.py", line 15, in <module>
    from pandas.core.arrays import Categorical
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/arrays/__init__.py", line 11, in <module>
    from pandas.core.arrays.interval import IntervalArray
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/arrays/interval.py", line 53, in <module>
    from pandas.core.indexes.base import ensure_index
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/indexes/base.py", line 150, in <module>
    class Index(IndexOpsMixin, PandasObject):
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/indexes/base.py", line 3143, in Index
    def get_indexer(
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/util/_decorators.py", line 477, in __call__
    func.__doc__ = dedent(self.join.join(docitems))
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/textwrap.py", line 431, in dedent
    indents = _leading_whitespace_re.findall(text)
KeyboardInterrupt
Traceback (most recent call last):
  File "qrft_harvest_pick350_test02.py", line 5, in <module>
    import pandas as pd
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/__init__.py", line 51, in <module>
    from pandas.core.api import (
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/api.py", line 15, in <module>
    from pandas.core.arrays import Categorical
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/arrays/__init__.py", line 11, in <module>
    from pandas.core.arrays.interval import IntervalArray
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/arrays/interval.py", line 53, in <module>
    from pandas.core.indexes.base import ensure_index
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/indexes/base.py", line 67, in <module>
    from pandas.core.dtypes.concat import concat_compat
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 971, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 914, in _find_spec
  File "<frozen importlib._bootstrap_external>", line 1342, in find_spec
  File "<frozen importlib._bootstrap_external>", line 1314, in _get_spec
  File "<frozen importlib._bootstrap_external>", line 1470, in find_spec
  File "<frozen importlib._bootstrap_external>", line 1427, in _get_spec
  File "<frozen importlib._bootstrap_external>", line 939, in __init__
KeyboardInterrupt
Traceback (most recent call last):
  File "qrft_harvest_pick350_test03.py", line 5, in <module>
    import pandas as pd
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/__init__.py", line 51, in <module>
    from pandas.core.api import (
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/api.py", line 15, in <module>
    from pandas.core.arrays import Categorical
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/arrays/__init__.py", line 11, in <module>
    from pandas.core.arrays.interval import IntervalArray
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/arrays/interval.py", line 53, in <module>
    from pandas.core.indexes.base import ensure_index
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/indexes/base.py", line 67, in <module>
    from pandas.core.dtypes.concat import concat_compat
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/dtypes/concat.py", line 20, in <module>
    from pandas.core.arrays.sparse import SparseArray
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/arrays/sparse/__init__.py", line 3, in <module>
    from pandas.core.arrays.sparse.accessor import SparseAccessor, SparseFrameAccessor
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/arrays/sparse/accessor.py", line 10, in <module>
    from pandas.core.arrays.sparse.array import SparseArray
  File "/home/sronly/miniconda3/envs/etf/lib/python3.8/site-packages/pandas/core/arrays/sparse/array.py", line 13, in <module>
    import pandas._libs.sparse as splib
  File "<frozen importlib._bootstrap>", line 389, in parent
KeyboardInterrupt
⠋ [Compustat API] : Cleaning past days cache✓ [Compustat API] : Cleaning past days cache done in 0.0 secs
auto caching이 활성화 되었습니다. /home/sronly/sr-storage/kirin_cache/kirin_api_cache/2.19.17_2021-11-23에 캐쉬를 저장합니다.
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.144 secs
⠋ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading masking data⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading masking data⠦ [Compustat API] : Loading masking data⠧ [Compustat API] : Loading masking data⠇ [Compustat API] : Loading masking data⠏ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading masking data⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading masking data⠦ [Compustat API] : Loading masking data⠧ [Compustat API] : Loading masking data⠇ [Compustat API] : Loading masking data⠏ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading masking data⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading masking data⠦ [Compustat API] : Loading masking data⠧ [Compustat API] : Loading masking data⠇ [Compustat API] : Loading masking data⠏ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 9.16 secs
setting tensorflow random seed failed
load_data: mom_12m_1m
load_data: ram_12m_0m
load_data: vol_3m
load_data: snp500_pr
load_data: wilshire500_pr
load_data: ted
load_data: retail_mfr
load_data: m1
load_data: m2
load_data: res_mom_12m_1m_0m
load_data: res_vol_6m_3m_0m
load_data: at
load_data: ae_m
load_data: ia_ta
load_data: rc_a
load_data: t1y_ff
load_data: t5y_ff
load_data: t10y_t2y
load_data: export_growth
load_data: import_growth
load_data: real_gig
load_data: ret_noa
load_data: etm
load_data: ia_mv
load_data: pr_1m_0m
load_data: mv
load_data: btm
load_data: t3y
load_data: t5y
load_data: t7y
load_data: baa_ff
load_data: core_cpi
load_data: core_pce
load_data: wti
load_data: capa_util
load_data: snp500_pe
load_data: r_s
load_data: r_a
load_data: fred_ff
load_data: t3m
load_data: t6m
load_data: t2y
load_data: t10y
load_data: aaa
load_data: baa
load_data: core_ppi
load_data: cpi
load_data: pce
load_data: corporate_tg
load_data: industrial_prod
load_data: home_pr
load_data: gpa
load_data: rev_surp
load_data: cash_at
load_data: aaa_t10y
load_data: baa_t10y
load_data: aaa_ff
load_data: real_pig
load_data: federal_tg
load_data: real_gdp
load_data: sector_values
load_data: mv
load_data: pr_1m_0m
load_data: mv
making sample. (inference: True, date:2015-12-31 00:00:00, date_number: 348)
making sample. (inference: False, date:2012-12-31 00:00:00, date_number: 312)
making sample. (inference: False, date:2013-01-31 00:00:00, date_number: 313)
making sample. (inference: False, date:2013-02-28 00:00:00, date_number: 314)
making sample. (inference: False, date:2013-03-31 00:00:00, date_number: 315)
making sample. (inference: False, date:2013-04-30 00:00:00, date_number: 316)
making sample. (inference: False, date:2013-05-31 00:00:00, date_number: 317)
making sample. (inference: False, date:2013-06-30 00:00:00, date_number: 318)
making sample. (inference: False, date:2013-07-31 00:00:00, date_number: 319)
making sample. (inference: False, date:2013-08-31 00:00:00, date_number: 320)
making sample. (inference: False, date:2013-09-30 00:00:00, date_number: 321)
making sample. (inference: False, date:2013-10-31 00:00:00, date_number: 322)
making sample. (inference: False, date:2013-11-30 00:00:00, date_number: 323)
making sample. (inference: False, date:2013-12-31 00:00:00, date_number: 324)
making sample. (inference: False, date:2014-01-31 00:00:00, date_number: 325)
making sample. (inference: False, date:2014-02-28 00:00:00, date_number: 326)
making sample. (inference: False, date:2014-03-31 00:00:00, date_number: 327)
making sample. (inference: False, date:2014-04-30 00:00:00, date_number: 328)
making sample. (inference: False, date:2014-05-31 00:00:00, date_number: 329)
making sample. (inference: False, date:2014-06-30 00:00:00, date_number: 330)
making sample. (inference: False, date:2014-07-31 00:00:00, date_number: 331)
making sample. (inference: False, date:2014-08-31 00:00:00, date_number: 332)
making sample. (inference: False, date:2014-09-30 00:00:00, date_number: 333)
making sample. (inference: False, date:2014-10-31 00:00:00, date_number: 334)
making sample. (inference: False, date:2014-11-30 00:00:00, date_number: 335)
making sample. (inference: False, date:2014-12-31 00:00:00, date_number: 336)
making sample. (inference: False, date:2015-01-31 00:00:00, date_number: 337)
making sample. (inference: False, date:2015-02-28 00:00:00, date_number: 338)
making sample. (inference: False, date:2015-03-31 00:00:00, date_number: 339)
making sample. (inference: False, date:2015-04-30 00:00:00, date_number: 340)
making sample. (inference: False, date:2015-05-31 00:00:00, date_number: 341)
making sample. (inference: False, date:2015-06-30 00:00:00, date_number: 342)
making sample. (inference: False, date:2015-07-31 00:00:00, date_number: 343)
making sample. (inference: False, date:2015-08-31 00:00:00, date_number: 344)
making sample. (inference: False, date:2015-09-30 00:00:00, date_number: 345)
making sample. (inference: False, date:2015-10-31 00:00:00, date_number: 346)
making sample. (inference: False, date:2015-11-30 00:00:00, date_number: 347)
[AttentionModel/2015-12-31 | get_logger | INFO]: ====================AttentionModel/2015-12-31 start====================
making sample. (inference: True, date:2016-01-31 00:00:00, date_number: 349)
making sample. (inference: False, date:2015-12-31 00:00:00, date_number: 348)
[AttentionModel/2016-01-31 | get_logger | INFO]: ====================AttentionModel/2016-01-31 start====================
[strategy_integration.py] set_seed(2015-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5731)training: 2/10 (0.5731)validation : 2/10 (0.5731)training: 3/10 (0.5731)validation : 3/10 (0.5731)training: 4/10 (0.5731)validation : 4/10 (0.5731)training: 5/10 (0.5731)validation : 5/10 (0.5731)early stopping at 5 with loss 0.5731
AttentionModel-training is done: 5/10
2015-12-31 | reset count: 0 | final loss: 0.5731 at epoch 1
[strategy_integration.py] set_seed(2016-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5747)training: 2/10 (0.5747)validation : 2/10 (0.5747)training: 3/10 (0.5747)validation : 3/10 (0.5747)training: 4/10 (0.5747)validation : 4/10 (0.5747)training: 5/10 (0.5747)validation : 5/10 (0.5747)early stopping at 5 with loss 0.5747
AttentionModel-training is done: 5/10
2016-01-31 | reset count: 0 | final loss: 0.5747 at epoch 5
making sample. (inference: True, date:2016-02-29 00:00:00, date_number: 350)
making sample. (inference: False, date:2016-01-31 00:00:00, date_number: 349)
[AttentionModel/2016-02-29 | get_logger | INFO]: ====================AttentionModel/2016-02-29 start====================
making sample. (inference: True, date:2016-03-31 00:00:00, date_number: 351)
making sample. (inference: False, date:2016-02-29 00:00:00, date_number: 350)
[AttentionModel/2016-03-31 | get_logger | INFO]: ====================AttentionModel/2016-03-31 start====================
[strategy_integration.py] set_seed(2016-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5843)training: 2/10 (0.5843)validation : 2/10 (0.5784)training: 3/10 (0.5784)validation : 3/10 (0.5784)training: 4/10 (0.5784)validation : 4/10 (0.5784)training: 5/10 (0.5784)validation : 5/10 (0.5784)training: 6/10 (0.5784)validation : 6/10 (0.5784)early stopping at 6 with loss 0.5784
AttentionModel-training is done: 6/10
2016-02-29 | reset count: 0 | final loss: 0.5784 at epoch 2
making sample. (inference: True, date:2016-04-30 00:00:00, date_number: 352)
making sample. (inference: False, date:2016-03-31 00:00:00, date_number: 351)
[AttentionModel/2016-04-30 | get_logger | INFO]: ====================AttentionModel/2016-04-30 start====================
[strategy_integration.py] set_seed(2016-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5724)training: 2/10 (0.5724)validation : 2/10 (0.5724)training: 3/10 (0.5724)validation : 3/10 (0.5724)training: 4/10 (0.5724)validation : 4/10 (0.5724)training: 5/10 (0.5724)validation : 5/10 (0.5716)training: 6/10 (0.5716)validation : 6/10 (0.5716)training: 7/10 (0.5716)validation : 7/10 (0.5716)training: 8/10 (0.5716)validation : 8/10 (0.5716)early stopping at 8 with loss 0.5716
AttentionModel-training is done: 8/10
2016-03-31 | reset count: 0 | final loss: 0.5716 at epoch 7
making sample. (inference: True, date:2016-05-31 00:00:00, date_number: 353)
making sample. (inference: False, date:2016-04-30 00:00:00, date_number: 352)
[AttentionModel/2016-05-31 | get_logger | INFO]: ====================AttentionModel/2016-05-31 start====================
[strategy_integration.py] set_seed(2016-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5747)training: 2/10 (0.5747)validation : 2/10 (0.5747)training: 3/10 (0.5747)validation : 3/10 (0.5747)training: 4/10 (0.5747)validation : 4/10 (0.5747)training: 5/10 (0.5747)validation : 5/10 (0.5747)training: 6/10 (0.5747)validation : 6/10 (0.5747)early stopping at 6 with loss 0.5747
AttentionModel-training is done: 6/10
2016-04-30 | reset count: 0 | final loss: 0.5747 at epoch 1
making sample. (inference: True, date:2016-06-30 00:00:00, date_number: 354)
making sample. (inference: False, date:2016-05-31 00:00:00, date_number: 353)
[AttentionModel/2016-06-30 | get_logger | INFO]: ====================AttentionModel/2016-06-30 start====================
[strategy_integration.py] set_seed(2016-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5905)training: 2/10 (0.5905)validation : 2/10 (0.5837)training: 3/10 (0.5837)validation : 3/10 (0.5837)training: 4/10 (0.5837)validation : 4/10 (0.5837)training: 5/10 (0.5837)validation : 5/10 (0.5830)training: 6/10 (0.5830)validation : 6/10 (0.5808)training: 7/10 (0.5808)validation : 7/10 (0.5808)training: 8/10 (0.5808)validation : 8/10 (0.5794)training: 9/10 (0.5794)validation : 9/10 (0.5794)early stopping at 9 with loss 0.5794
AttentionModel-training is done: 9/10
2016-05-31 | reset count: 0 | final loss: 0.5794 at epoch 8
[strategy_integration.py] set_seed(2016-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5774)training: 2/10 (0.5774)validation : 2/10 (0.5770)training: 3/10 (0.5770)validation : 3/10 (0.5770)training: 4/10 (0.5770)validation : 4/10 (0.5770)training: 5/10 (0.5770)validation : 5/10 (0.5770)early stopping at 5 with loss 0.5770
AttentionModel-training is done: 5/10
2016-06-30 | reset count: 0 | final loss: 0.5770 at epoch 5
making sample. (inference: True, date:2016-07-31 00:00:00, date_number: 355)
making sample. (inference: False, date:2016-06-30 00:00:00, date_number: 354)
[AttentionModel/2016-07-31 | get_logger | INFO]: ====================AttentionModel/2016-07-31 start====================
making sample. (inference: True, date:2016-08-31 00:00:00, date_number: 356)
making sample. (inference: False, date:2016-07-31 00:00:00, date_number: 355)
[AttentionModel/2016-08-31 | get_logger | INFO]: ====================AttentionModel/2016-08-31 start====================
[strategy_integration.py] set_seed(2016-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5765)training: 2/10 (0.5765)validation : 2/10 (0.5765)training: 3/10 (0.5765)validation : 3/10 (0.5765)training: 4/10 (0.5765)validation : 4/10 (0.5743)training: 5/10 (0.5743)validation : 5/10 (0.5743)training: 6/10 (0.5743)validation : 6/10 (0.5743)training: 7/10 (0.5743)validation : 7/10 (0.5743)early stopping at 7 with loss 0.5743
AttentionModel-training is done: 7/10
2016-07-31 | reset count: 0 | final loss: 0.5743 at epoch 4
[strategy_integration.py] set_seed(2016-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5807)training: 2/10 (0.5807)validation : 2/10 (0.5736)training: 3/10 (0.5736)validation : 3/10 (0.5736)training: 4/10 (0.5736)validation : 4/10 (0.5736)training: 5/10 (0.5736)validation : 5/10 (0.5736)training: 6/10 (0.5736)validation : 6/10 (0.5736)early stopping at 6 with loss 0.5736
AttentionModel-training is done: 6/10
2016-08-31 | reset count: 0 | final loss: 0.5736 at epoch 2
making sample. (inference: True, date:2016-09-30 00:00:00, date_number: 357)
making sample. (inference: False, date:2016-08-31 00:00:00, date_number: 356)
[AttentionModel/2016-09-30 | get_logger | INFO]: ====================AttentionModel/2016-09-30 start====================
making sample. (inference: True, date:2016-10-31 00:00:00, date_number: 358)
making sample. (inference: False, date:2016-09-30 00:00:00, date_number: 357)
[AttentionModel/2016-10-31 | get_logger | INFO]: ====================AttentionModel/2016-10-31 start====================
[strategy_integration.py] set_seed(2016-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5723)training: 2/10 (0.5723)validation : 2/10 (0.5723)training: 3/10 (0.5723)validation : 3/10 (0.5723)training: 4/10 (0.5723)validation : 4/10 (0.5723)training: 5/10 (0.5723)validation : 5/10 (0.5723)early stopping at 5 with loss 0.5723
AttentionModel-training is done: 5/10
2016-09-30 | reset count: 0 | final loss: 0.5723 at epoch 1
making sample. (inference: True, date:2016-11-30 00:00:00, date_number: 359)
making sample. (inference: False, date:2016-10-31 00:00:00, date_number: 358)
[strategy_integration.py] set_seed(2016-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5859)training: 2/10 (0.5859)validation : 2/10 (0.5831)training: 3/10 (0.5831)validation : 3/10 (0.5829)training: 4/10 (0.5829)validation : 4/10 (0.5829)training: 5/10 (0.5829)validation : 5/10 (0.5829)early stopping at 5 with loss 0.5829
AttentionModel-training is done: 5/10
2016-10-31 | reset count: 0 | final loss: 0.5829 at epoch 3
[AttentionModel/2016-11-30 | get_logger | INFO]: ====================AttentionModel/2016-11-30 start====================
making sample. (inference: True, date:2016-12-31 00:00:00, date_number: 360)
making sample. (inference: False, date:2016-11-30 00:00:00, date_number: 359)
[AttentionModel/2016-12-31 | get_logger | INFO]: ====================AttentionModel/2016-12-31 start====================
[strategy_integration.py] set_seed(2016-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5740)training: 2/10 (0.5740)validation : 2/10 (0.5740)training: 3/10 (0.5740)validation : 3/10 (0.5740)training: 4/10 (0.5740)validation : 4/10 (0.5740)training: 5/10 (0.5740)validation : 5/10 (0.5740)early stopping at 5 with loss 0.5740
AttentionModel-training is done: 5/10
2016-11-30 | reset count: 0 | final loss: 0.5740 at epoch 1
making sample. (inference: True, date:2017-01-31 00:00:00, date_number: 361)
making sample. (inference: False, date:2016-12-31 00:00:00, date_number: 360)
[AttentionModel/2017-01-31 | get_logger | INFO]: ====================AttentionModel/2017-01-31 start====================
[strategy_integration.py] set_seed(2016-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5693)training: 2/10 (0.5693)validation : 2/10 (0.5683)training: 3/10 (0.5683)validation : 3/10 (0.5620)training: 4/10 (0.5620)validation : 4/10 (0.5620)training: 5/10 (0.5620)validation : 5/10 (0.5620)training: 6/10 (0.5620)validation : 6/10 (0.5620)early stopping at 6 with loss 0.5620
AttentionModel-training is done: 6/10
2016-12-31 | reset count: 0 | final loss: 0.5620 at epoch 3
making sample. (inference: True, date:2017-02-28 00:00:00, date_number: 362)
making sample. (inference: False, date:2017-01-31 00:00:00, date_number: 361)
[AttentionModel/2017-02-28 | get_logger | INFO]: ====================AttentionModel/2017-02-28 start====================
[strategy_integration.py] set_seed(2017-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5735)training: 2/10 (0.5735)validation : 2/10 (0.5735)training: 3/10 (0.5735)validation : 3/10 (0.5735)training: 4/10 (0.5735)validation : 4/10 (0.5714)training: 5/10 (0.5714)validation : 5/10 (0.5714)training: 6/10 (0.5714)validation : 6/10 (0.5665)training: 7/10 (0.5665)validation : 7/10 (0.5665)training: 8/10 (0.5665)validation : 8/10 (0.5665)training: 9/10 (0.5665)validation : 9/10 (0.5665)early stopping at 9 with loss 0.5665
AttentionModel-training is done: 9/10
2017-01-31 | reset count: 0 | final loss: 0.5665 at epoch 6
[strategy_integration.py] set_seed(2017-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5693)training: 2/10 (0.5693)validation : 2/10 (0.5660)training: 3/10 (0.5660)validation : 3/10 (0.5660)training: 4/10 (0.5660)validation : 4/10 (0.5660)training: 5/10 (0.5660)validation : 5/10 (0.5660)training: 6/10 (0.5660)validation : 6/10 (0.5660)training: 7/10 (0.5660)validation : 7/10 (0.5660)early stopping at 7 with loss 0.5660
AttentionModel-training is done: 7/10
2017-02-28 | reset count: 0 | final loss: 0.5660 at epoch 2
making sample. (inference: True, date:2017-03-31 00:00:00, date_number: 363)
making sample. (inference: False, date:2017-02-28 00:00:00, date_number: 362)
[AttentionModel/2017-03-31 | get_logger | INFO]: ====================AttentionModel/2017-03-31 start====================
making sample. (inference: True, date:2017-04-30 00:00:00, date_number: 364)
making sample. (inference: False, date:2017-03-31 00:00:00, date_number: 363)
[AttentionModel/2017-04-30 | get_logger | INFO]: ====================AttentionModel/2017-04-30 start====================
[strategy_integration.py] set_seed(2017-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5723)training: 2/10 (0.5723)validation : 2/10 (0.5669)training: 3/10 (0.5669)validation : 3/10 (0.5661)training: 4/10 (0.5661)validation : 4/10 (0.5661)training: 5/10 (0.5661)validation : 5/10 (0.5661)training: 6/10 (0.5661)validation : 6/10 (0.5661)early stopping at 6 with loss 0.5661
AttentionModel-training is done: 6/10
2017-03-31 | reset count: 0 | final loss: 0.5661 at epoch 3
[strategy_integration.py] set_seed(2017-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5584)training: 2/10 (0.5584)validation : 2/10 (0.5584)training: 3/10 (0.5584)validation : 3/10 (0.5584)training: 4/10 (0.5584)validation : 4/10 (0.5584)training: 5/10 (0.5584)validation : 5/10 (0.5584)early stopping at 5 with loss 0.5584
AttentionModel-training is done: 5/10
2017-04-30 | reset count: 0 | final loss: 0.5584 at epoch 1
making sample. (inference: True, date:2017-05-31 00:00:00, date_number: 365)
making sample. (inference: False, date:2017-04-30 00:00:00, date_number: 364)
[AttentionModel/2017-05-31 | get_logger | INFO]: ====================AttentionModel/2017-05-31 start====================
making sample. (inference: True, date:2017-06-30 00:00:00, date_number: 366)
making sample. (inference: False, date:2017-05-31 00:00:00, date_number: 365)
[AttentionModel/2017-06-30 | get_logger | INFO]: ====================AttentionModel/2017-06-30 start====================
[strategy_integration.py] set_seed(2017-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5641)training: 2/10 (0.5641)validation : 2/10 (0.5603)training: 3/10 (0.5603)validation : 3/10 (0.5573)training: 4/10 (0.5573)validation : 4/10 (0.5573)training: 5/10 (0.5573)validation : 5/10 (0.5573)early stopping at 5 with loss 0.5573
AttentionModel-training is done: 5/10
2017-05-31 | reset count: 0 | final loss: 0.5573 at epoch 3
making sample. (inference: True, date:2017-07-31 00:00:00, date_number: 367)
making sample. (inference: False, date:2017-06-30 00:00:00, date_number: 366)
[AttentionModel/2017-07-31 | get_logger | INFO]: ====================AttentionModel/2017-07-31 start====================
[strategy_integration.py] set_seed(2017-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5674)training: 2/10 (0.5674)validation : 2/10 (0.5635)training: 3/10 (0.5635)validation : 3/10 (0.5635)training: 4/10 (0.5635)validation : 4/10 (0.5635)training: 5/10 (0.5635)validation : 5/10 (0.5614)training: 6/10 (0.5614)validation : 6/10 (0.5604)training: 7/10 (0.5604)validation : 7/10 (0.5604)training: 8/10 (0.5604)validation : 8/10 (0.5604)early stopping at 8 with loss 0.5604
AttentionModel-training is done: 8/10
2017-06-30 | reset count: 0 | final loss: 0.5604 at epoch 6
[strategy_integration.py] set_seed(2017-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5673)training: 2/10 (0.5673)validation : 2/10 (0.5673)training: 3/10 (0.5673)validation : 3/10 (0.5673)training: 4/10 (0.5673)validation : 4/10 (0.5673)training: 5/10 (0.5673)validation : 5/10 (0.5673)early stopping at 5 with loss 0.5673
AttentionModel-training is done: 5/10
2017-07-31 | reset count: 0 | final loss: 0.5673 at epoch 1
making sample. (inference: True, date:2017-08-31 00:00:00, date_number: 368)
making sample. (inference: False, date:2017-07-31 00:00:00, date_number: 367)
[AttentionModel/2017-08-31 | get_logger | INFO]: ====================AttentionModel/2017-08-31 start====================
making sample. (inference: True, date:2017-09-30 00:00:00, date_number: 369)
making sample. (inference: False, date:2017-08-31 00:00:00, date_number: 368)
[AttentionModel/2017-09-30 | get_logger | INFO]: ====================AttentionModel/2017-09-30 start====================
[strategy_integration.py] set_seed(2017-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5664)training: 2/10 (0.5664)validation : 2/10 (0.5664)training: 3/10 (0.5664)validation : 3/10 (0.5664)training: 4/10 (0.5664)validation : 4/10 (0.5664)training: 5/10 (0.5664)validation : 5/10 (0.5664)early stopping at 5 with loss 0.5664
AttentionModel-training is done: 5/10
2017-08-31 | reset count: 0 | final loss: 0.5664 at epoch 1
making sample. (inference: True, date:2017-10-31 00:00:00, date_number: 370)
making sample. (inference: False, date:2017-09-30 00:00:00, date_number: 369)
[AttentionModel/2017-10-31 | get_logger | INFO]: ====================AttentionModel/2017-10-31 start====================
[strategy_integration.py] set_seed(2017-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5647)training: 2/10 (0.5647)validation : 2/10 (0.5616)training: 3/10 (0.5616)validation : 3/10 (0.5597)training: 4/10 (0.5597)validation : 4/10 (0.5563)training: 5/10 (0.5563)validation : 5/10 (0.5563)training: 6/10 (0.5563)validation : 6/10 (0.5563)training: 7/10 (0.5563)validation : 7/10 (0.5563)early stopping at 7 with loss 0.5563
AttentionModel-training is done: 7/10
2017-09-30 | reset count: 0 | final loss: 0.5563 at epoch 4
making sample. (inference: True, date:2017-11-30 00:00:00, date_number: 371)
making sample. (inference: False, date:2017-10-31 00:00:00, date_number: 370)
[AttentionModel/2017-11-30 | get_logger | INFO]: ====================AttentionModel/2017-11-30 start====================
[strategy_integration.py] set_seed(2017-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5687)training: 2/10 (0.5687)validation : 2/10 (0.5636)training: 3/10 (0.5636)validation : 3/10 (0.5636)training: 4/10 (0.5636)validation : 4/10 (0.5636)training: 5/10 (0.5636)validation : 5/10 (0.5636)early stopping at 5 with loss 0.5636
AttentionModel-training is done: 5/10
2017-10-31 | reset count: 0 | final loss: 0.5636 at epoch 2
making sample. (inference: True, date:2017-12-31 00:00:00, date_number: 372)
making sample. (inference: False, date:2017-11-30 00:00:00, date_number: 371)
[AttentionModel/2017-12-31 | get_logger | INFO]: ====================AttentionModel/2017-12-31 start====================
[strategy_integration.py] set_seed(2017-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5664)training: 2/10 (0.5664)validation : 2/10 (0.5664)training: 3/10 (0.5664)validation : 3/10 (0.5650)training: 4/10 (0.5650)validation : 4/10 (0.5648)training: 5/10 (0.5648)validation : 5/10 (0.5648)training: 6/10 (0.5648)validation : 6/10 (0.5648)early stopping at 6 with loss 0.5648
AttentionModel-training is done: 6/10
2017-11-30 | reset count: 0 | final loss: 0.5648 at epoch 4
making sample. (inference: True, date:2018-01-31 00:00:00, date_number: 373)
making sample. (inference: False, date:2017-12-31 00:00:00, date_number: 372)
[AttentionModel/2018-01-31 | get_logger | INFO]: ====================AttentionModel/2018-01-31 start====================
[strategy_integration.py] set_seed(2017-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5584)training: 2/10 (0.5584)validation : 2/10 (0.5584)training: 3/10 (0.5584)validation : 3/10 (0.5559)training: 4/10 (0.5559)validation : 4/10 (0.5559)training: 5/10 (0.5559)validation : 5/10 (0.5559)training: 6/10 (0.5559)validation : 6/10 (0.5556)training: 7/10 (0.5556)validation : 7/10 (0.5556)early stopping at 7 with loss 0.5556
AttentionModel-training is done: 7/10
2017-12-31 | reset count: 0 | final loss: 0.5556 at epoch 6
making sample. (inference: True, date:2018-02-28 00:00:00, date_number: 374)
making sample. (inference: False, date:2018-01-31 00:00:00, date_number: 373)
[AttentionModel/2018-02-28 | get_logger | INFO]: ====================AttentionModel/2018-02-28 start====================
[strategy_integration.py] set_seed(2018-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5701)training: 2/10 (0.5701)validation : 2/10 (0.5662)training: 3/10 (0.5662)validation : 3/10 (0.5658)training: 4/10 (0.5658)validation : 4/10 (0.5658)training: 5/10 (0.5658)validation : 5/10 (0.5658)training: 6/10 (0.5658)validation : 6/10 (0.5658)early stopping at 6 with loss 0.5658
AttentionModel-training is done: 6/10
2018-01-31 | reset count: 0 | final loss: 0.5658 at epoch 3
making sample. (inference: True, date:2018-03-31 00:00:00, date_number: 375)
making sample. (inference: False, date:2018-02-28 00:00:00, date_number: 374)
[AttentionModel/2018-03-31 | get_logger | INFO]: ====================AttentionModel/2018-03-31 start====================
[strategy_integration.py] set_seed(2018-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5697)training: 2/10 (0.5697)validation : 2/10 (0.5643)training: 3/10 (0.5643)validation : 3/10 (0.5643)training: 4/10 (0.5643)validation : 4/10 (0.5633)training: 5/10 (0.5633)validation : 5/10 (0.5633)training: 6/10 (0.5633)validation : 6/10 (0.5633)training: 7/10 (0.5633)validation : 7/10 (0.5633)early stopping at 7 with loss 0.5633
AttentionModel-training is done: 7/10
2018-02-28 | reset count: 0 | final loss: 0.5633 at epoch 4
[strategy_integration.py] set_seed(2018-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5684)training: 2/10 (0.5684)validation : 2/10 (0.5628)training: 3/10 (0.5628)validation : 3/10 (0.5626)training: 4/10 (0.5626)validation : 4/10 (0.5626)training: 5/10 (0.5626)validation : 5/10 (0.5625)training: 6/10 (0.5625)validation : 6/10 (0.5625)early stopping at 6 with loss 0.5625
AttentionModel-training is done: 6/10
2018-03-31 | reset count: 0 | final loss: 0.5625 at epoch 5
making sample. (inference: True, date:2018-04-30 00:00:00, date_number: 376)
making sample. (inference: False, date:2018-03-31 00:00:00, date_number: 375)
[AttentionModel/2018-04-30 | get_logger | INFO]: ====================AttentionModel/2018-04-30 start====================
making sample. (inference: True, date:2018-05-31 00:00:00, date_number: 377)
making sample. (inference: False, date:2018-04-30 00:00:00, date_number: 376)
[AttentionModel/2018-05-31 | get_logger | INFO]: ====================AttentionModel/2018-05-31 start====================
[strategy_integration.py] set_seed(2018-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5643)training: 2/10 (0.5643)validation : 2/10 (0.5629)training: 3/10 (0.5629)validation : 3/10 (0.5604)training: 4/10 (0.5604)validation : 4/10 (0.5604)training: 5/10 (0.5604)validation : 5/10 (0.5581)training: 6/10 (0.5581)validation : 6/10 (0.5581)early stopping at 6 with loss 0.5581
AttentionModel-training is done: 6/10
2018-04-30 | reset count: 0 | final loss: 0.5581 at epoch 5
making sample. (inference: True, date:2018-06-30 00:00:00, date_number: 378)
making sample. (inference: False, date:2018-05-31 00:00:00, date_number: 377)
[AttentionModel/2018-06-30 | get_logger | INFO]: ====================AttentionModel/2018-06-30 start====================
[strategy_integration.py] set_seed(2018-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5701)training: 2/10 (0.5701)validation : 2/10 (0.5640)training: 3/10 (0.5640)validation : 3/10 (0.5640)training: 4/10 (0.5640)validation : 4/10 (0.5625)training: 5/10 (0.5625)validation : 5/10 (0.5625)training: 6/10 (0.5625)validation : 6/10 (0.5625)training: 7/10 (0.5625)validation : 7/10 (0.5625)early stopping at 7 with loss 0.5625
AttentionModel-training is done: 7/10
2018-05-31 | reset count: 0 | final loss: 0.5625 at epoch 4
making sample. (inference: True, date:2018-07-31 00:00:00, date_number: 379)
making sample. (inference: False, date:2018-06-30 00:00:00, date_number: 378)
[AttentionModel/2018-07-31 | get_logger | INFO]: ====================AttentionModel/2018-07-31 start====================
[strategy_integration.py] set_seed(2018-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5619)training: 2/10 (0.5619)validation : 2/10 (0.5619)training: 3/10 (0.5619)validation : 3/10 (0.5619)training: 4/10 (0.5619)validation : 4/10 (0.5619)training: 5/10 (0.5619)validation : 5/10 (0.5619)early stopping at 5 with loss 0.5619
AttentionModel-training is done: 5/10
2018-06-30 | reset count: 0 | final loss: 0.5619 at epoch 1
making sample. (inference: True, date:2018-08-31 00:00:00, date_number: 380)
making sample. (inference: False, date:2018-07-31 00:00:00, date_number: 379)
[AttentionModel/2018-08-31 | get_logger | INFO]: ====================AttentionModel/2018-08-31 start====================
[strategy_integration.py] set_seed(2018-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5774)training: 2/10 (0.5774)validation : 2/10 (0.5774)training: 3/10 (0.5774)validation : 3/10 (0.5774)training: 4/10 (0.5774)validation : 4/10 (0.5769)training: 5/10 (0.5769)validation : 5/10 (0.5769)training: 6/10 (0.5769)validation : 6/10 (0.5769)training: 7/10 (0.5769)validation : 7/10 (0.5766)training: 8/10 (0.5766)validation : 8/10 (0.5766)early stopping at 8 with loss 0.5766
AttentionModel-training is done: 8/10
2018-07-31 | reset count: 0 | final loss: 0.5766 at epoch 7
making sample. (inference: True, date:2018-09-30 00:00:00, date_number: 381)
making sample. (inference: False, date:2018-08-31 00:00:00, date_number: 380)
[AttentionModel/2018-09-30 | get_logger | INFO]: ====================AttentionModel/2018-09-30 start====================
[strategy_integration.py] set_seed(2018-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5752)training: 2/10 (0.5752)validation : 2/10 (0.5671)training: 3/10 (0.5671)validation : 3/10 (0.5671)training: 4/10 (0.5671)validation : 4/10 (0.5637)training: 5/10 (0.5637)validation : 5/10 (0.5637)training: 6/10 (0.5637)validation : 6/10 (0.5637)training: 7/10 (0.5637)validation : 7/10 (0.5637)early stopping at 7 with loss 0.5637
AttentionModel-training is done: 7/10
2018-08-31 | reset count: 0 | final loss: 0.5637 at epoch 4
[strategy_integration.py] set_seed(2018-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5632)training: 2/10 (0.5632)validation : 2/10 (0.5583)training: 3/10 (0.5583)validation : 3/10 (0.5583)training: 4/10 (0.5583)validation : 4/10 (0.5583)training: 5/10 (0.5583)validation : 5/10 (0.5576)early stopping at 5 with loss 0.5576
AttentionModel-training is done: 5/10
2018-09-30 | reset count: 0 | final loss: 0.5576 at epoch 5
making sample. (inference: True, date:2018-10-31 00:00:00, date_number: 382)
making sample. (inference: False, date:2018-09-30 00:00:00, date_number: 381)
[AttentionModel/2018-10-31 | get_logger | INFO]: ====================AttentionModel/2018-10-31 start====================
making sample. (inference: True, date:2018-11-30 00:00:00, date_number: 383)
making sample. (inference: False, date:2018-10-31 00:00:00, date_number: 382)
[AttentionModel/2018-11-30 | get_logger | INFO]: ====================AttentionModel/2018-11-30 start====================
[strategy_integration.py] set_seed(2018-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5808)training: 2/10 (0.5808)validation : 2/10 (0.5793)training: 3/10 (0.5793)validation : 3/10 (0.5793)training: 4/10 (0.5793)validation : 4/10 (0.5793)training: 5/10 (0.5793)validation : 5/10 (0.5793)early stopping at 5 with loss 0.5793
AttentionModel-training is done: 5/10
2018-10-31 | reset count: 0 | final loss: 0.5793 at epoch 2
making sample. (inference: True, date:2018-12-31 00:00:00, date_number: 384)
making sample. (inference: False, date:2018-11-30 00:00:00, date_number: 383)
[AttentionModel/2018-12-31 | get_logger | INFO]: ====================AttentionModel/2018-12-31 start====================
[strategy_integration.py] set_seed(2018-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5770)training: 2/10 (0.5770)validation : 2/10 (0.5748)training: 3/10 (0.5748)validation : 3/10 (0.5748)training: 4/10 (0.5748)validation : 4/10 (0.5748)training: 5/10 (0.5748)validation : 5/10 (0.5745)early stopping at 5 with loss 0.5745
AttentionModel-training is done: 5/10
2018-11-30 | reset count: 0 | final loss: 0.5745 at epoch 5
making sample. (inference: True, date:2019-01-31 00:00:00, date_number: 385)
making sample. (inference: False, date:2018-12-31 00:00:00, date_number: 384)
[AttentionModel/2019-01-31 | get_logger | INFO]: ====================AttentionModel/2019-01-31 start====================
[strategy_integration.py] set_seed(2018-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5841)training: 2/10 (0.5841)validation : 2/10 (0.5827)training: 3/10 (0.5827)validation : 3/10 (0.5789)training: 4/10 (0.5789)validation : 4/10 (0.5784)training: 5/10 (0.5784)validation : 5/10 (0.5784)training: 6/10 (0.5784)validation : 6/10 (0.5784)early stopping at 6 with loss 0.5784
AttentionModel-training is done: 6/10
2018-12-31 | reset count: 0 | final loss: 0.5784 at epoch 4
making sample. (inference: True, date:2019-02-28 00:00:00, date_number: 386)
making sample. (inference: False, date:2019-01-31 00:00:00, date_number: 385)
[AttentionModel/2019-02-28 | get_logger | INFO]: ====================AttentionModel/2019-02-28 start====================
[strategy_integration.py] set_seed(2019-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5883)training: 2/10 (0.5883)validation : 2/10 (0.5864)training: 3/10 (0.5864)validation : 3/10 (0.5831)training: 4/10 (0.5831)validation : 4/10 (0.5831)training: 5/10 (0.5831)validation : 5/10 (0.5831)training: 6/10 (0.5831)validation : 6/10 (0.5807)training: 7/10 (0.5807)validation : 7/10 (0.5807)early stopping at 7 with loss 0.5807
AttentionModel-training is done: 7/10
2019-01-31 | reset count: 0 | final loss: 0.5807 at epoch 6
making sample. (inference: True, date:2019-03-31 00:00:00, date_number: 387)
making sample. (inference: False, date:2019-02-28 00:00:00, date_number: 386)
[AttentionModel/2019-03-31 | get_logger | INFO]: ====================AttentionModel/2019-03-31 start====================
[strategy_integration.py] set_seed(2019-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5861)training: 2/10 (0.5861)validation : 2/10 (0.5861)training: 3/10 (0.5861)validation : 3/10 (0.5836)training: 4/10 (0.5836)validation : 4/10 (0.5811)training: 5/10 (0.5811)validation : 5/10 (0.5804)training: 6/10 (0.5804)validation : 6/10 (0.5804)training: 7/10 (0.5804)validation : 7/10 (0.5804)early stopping at 7 with loss 0.5804
AttentionModel-training is done: 7/10
2019-02-28 | reset count: 0 | final loss: 0.5804 at epoch 5
[strategy_integration.py] set_seed(2019-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5762)training: 2/10 (0.5762)validation : 2/10 (0.5762)training: 3/10 (0.5762)validation : 3/10 (0.5744)training: 4/10 (0.5744)validation : 4/10 (0.5744)training: 5/10 (0.5744)validation : 5/10 (0.5744)early stopping at 5 with loss 0.5744
AttentionModel-training is done: 5/10
2019-03-31 | reset count: 0 | final loss: 0.5744 at epoch 3
making sample. (inference: True, date:2019-04-30 00:00:00, date_number: 388)
making sample. (inference: False, date:2019-03-31 00:00:00, date_number: 387)
[AttentionModel/2019-04-30 | get_logger | INFO]: ====================AttentionModel/2019-04-30 start====================
making sample. (inference: True, date:2019-05-31 00:00:00, date_number: 389)
making sample. (inference: False, date:2019-04-30 00:00:00, date_number: 388)
[AttentionModel/2019-05-31 | get_logger | INFO]: ====================AttentionModel/2019-05-31 start====================
[strategy_integration.py] set_seed(2019-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5892)training: 2/10 (0.5892)validation : 2/10 (0.5846)training: 3/10 (0.5846)validation : 3/10 (0.5846)training: 4/10 (0.5846)validation : 4/10 (0.5846)training: 5/10 (0.5846)validation : 5/10 (0.5822)training: 6/10 (0.5822)validation : 6/10 (0.5822)training: 7/10 (0.5822)validation : 7/10 (0.5822)training: 8/10 (0.5822)validation : 8/10 (0.5822)early stopping at 8 with loss 0.5822
AttentionModel-training is done: 8/10
2019-04-30 | reset count: 0 | final loss: 0.5822 at epoch 5
[strategy_integration.py] set_seed(2019-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5879)training: 2/10 (0.5879)validation : 2/10 (0.5859)training: 3/10 (0.5859)validation : 3/10 (0.5829)training: 4/10 (0.5829)validation : 4/10 (0.5810)training: 5/10 (0.5810)validation : 5/10 (0.5810)training: 6/10 (0.5810)validation : 6/10 (0.5810)early stopping at 6 with loss 0.5810
AttentionModel-training is done: 6/10
2019-05-31 | reset count: 0 | final loss: 0.5810 at epoch 4
making sample. (inference: True, date:2019-06-30 00:00:00, date_number: 390)
making sample. (inference: False, date:2019-05-31 00:00:00, date_number: 389)
[AttentionModel/2019-06-30 | get_logger | INFO]: ====================AttentionModel/2019-06-30 start====================
making sample. (inference: True, date:2019-07-31 00:00:00, date_number: 391)
making sample. (inference: False, date:2019-06-30 00:00:00, date_number: 390)
[AttentionModel/2019-07-31 | get_logger | INFO]: ====================AttentionModel/2019-07-31 start====================
[strategy_integration.py] set_seed(2019-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5848)training: 2/10 (0.5848)validation : 2/10 (0.5848)training: 3/10 (0.5848)validation : 3/10 (0.5801)training: 4/10 (0.5801)validation : 4/10 (0.5801)training: 5/10 (0.5801)validation : 5/10 (0.5801)training: 6/10 (0.5801)validation : 6/10 (0.5801)early stopping at 6 with loss 0.5801
AttentionModel-training is done: 6/10
2019-06-30 | reset count: 0 | final loss: 0.5801 at epoch 3
[strategy_integration.py] set_seed(2019-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5863)training: 2/10 (0.5863)validation : 2/10 (0.5863)training: 3/10 (0.5863)validation : 3/10 (0.5837)training: 4/10 (0.5837)validation : 4/10 (0.5837)training: 5/10 (0.5837)validation : 5/10 (0.5837)early stopping at 5 with loss 0.5837
AttentionModel-training is done: 5/10
2019-07-31 | reset count: 0 | final loss: 0.5837 at epoch 3
making sample. (inference: True, date:2019-08-31 00:00:00, date_number: 392)
making sample. (inference: False, date:2019-07-31 00:00:00, date_number: 391)
[AttentionModel/2019-08-31 | get_logger | INFO]: ====================AttentionModel/2019-08-31 start====================
making sample. (inference: True, date:2019-09-30 00:00:00, date_number: 393)
making sample. (inference: False, date:2019-08-31 00:00:00, date_number: 392)
[AttentionModel/2019-09-30 | get_logger | INFO]: ====================AttentionModel/2019-09-30 start====================
[strategy_integration.py] set_seed(2019-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5887)training: 2/10 (0.5887)validation : 2/10 (0.5849)training: 3/10 (0.5849)validation : 3/10 (0.5835)training: 4/10 (0.5835)validation : 4/10 (0.5814)training: 5/10 (0.5814)validation : 5/10 (0.5814)training: 6/10 (0.5814)validation : 6/10 (0.5814)early stopping at 6 with loss 0.5814
AttentionModel-training is done: 6/10
2019-08-31 | reset count: 0 | final loss: 0.5814 at epoch 4
making sample. (inference: True, date:2019-10-31 00:00:00, date_number: 394)
making sample. (inference: False, date:2019-09-30 00:00:00, date_number: 393)
[AttentionModel/2019-10-31 | get_logger | INFO]: ====================AttentionModel/2019-10-31 start====================
[strategy_integration.py] set_seed(2019-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5940)training: 2/10 (0.5940)validation : 2/10 (0.5928)training: 3/10 (0.5928)validation : 3/10 (0.5928)training: 4/10 (0.5928)validation : 4/10 (0.5928)training: 5/10 (0.5928)validation : 5/10 (0.5912)training: 6/10 (0.5912)validation : 6/10 (0.5888)training: 7/10 (0.5888)validation : 7/10 (0.5888)training: 8/10 (0.5888)validation : 8/10 (0.5888)training: 9/10 (0.5888)validation : 9/10 (0.5888)early stopping at 9 with loss 0.5888
AttentionModel-training is done: 9/10
2019-09-30 | reset count: 0 | final loss: 0.5888 at epoch 6
[strategy_integration.py] set_seed(2019-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5813)training: 2/10 (0.5813)validation : 2/10 (0.5767)training: 3/10 (0.5767)validation : 3/10 (0.5767)training: 4/10 (0.5767)validation : 4/10 (0.5767)training: 5/10 (0.5767)validation : 5/10 (0.5767)early stopping at 5 with loss 0.5767
AttentionModel-training is done: 5/10
2019-10-31 | reset count: 0 | final loss: 0.5767 at epoch 2
making sample. (inference: True, date:2019-11-30 00:00:00, date_number: 395)
making sample. (inference: False, date:2019-10-31 00:00:00, date_number: 394)
[AttentionModel/2019-11-30 | get_logger | INFO]: ====================AttentionModel/2019-11-30 start====================
making sample. (inference: True, date:2019-12-31 00:00:00, date_number: 396)
making sample. (inference: False, date:2019-11-30 00:00:00, date_number: 395)
[AttentionModel/2019-12-31 | get_logger | INFO]: ====================AttentionModel/2019-12-31 start====================
[strategy_integration.py] set_seed(2019-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5932)training: 2/10 (0.5932)validation : 2/10 (0.5900)training: 3/10 (0.5900)validation : 3/10 (0.5871)training: 4/10 (0.5871)validation : 4/10 (0.5871)training: 5/10 (0.5871)validation : 5/10 (0.5871)training: 6/10 (0.5871)validation : 6/10 (0.5871)early stopping at 6 with loss 0.5871
AttentionModel-training is done: 6/10
2019-11-30 | reset count: 0 | final loss: 0.5871 at epoch 3
making sample. (inference: True, date:2020-01-31 00:00:00, date_number: 397)
making sample. (inference: False, date:2019-12-31 00:00:00, date_number: 396)
[AttentionModel/2020-01-31 | get_logger | INFO]: ====================AttentionModel/2020-01-31 start====================
[strategy_integration.py] set_seed(2019-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5890)training: 2/10 (0.5890)validation : 2/10 (0.5871)training: 3/10 (0.5871)validation : 3/10 (0.5857)training: 4/10 (0.5857)validation : 4/10 (0.5838)training: 5/10 (0.5838)validation : 5/10 (0.5838)training: 6/10 (0.5838)validation : 6/10 (0.5838)early stopping at 6 with loss 0.5838
AttentionModel-training is done: 6/10
2019-12-31 | reset count: 0 | final loss: 0.5838 at epoch 4
making sample. (inference: True, date:2020-02-29 00:00:00, date_number: 398)
making sample. (inference: False, date:2020-01-31 00:00:00, date_number: 397)
[AttentionModel/2020-02-29 | get_logger | INFO]: ====================AttentionModel/2020-02-29 start====================
[strategy_integration.py] set_seed(2020-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5868)training: 2/10 (0.5868)validation : 2/10 (0.5838)training: 3/10 (0.5838)validation : 3/10 (0.5838)training: 4/10 (0.5838)validation : 4/10 (0.5831)training: 5/10 (0.5831)validation : 5/10 (0.5831)training: 6/10 (0.5831)validation : 6/10 (0.5831)training: 7/10 (0.5831)validation : 7/10 (0.5831)early stopping at 7 with loss 0.5831
AttentionModel-training is done: 7/10
2020-01-31 | reset count: 0 | final loss: 0.5831 at epoch 6
[strategy_integration.py] set_seed(2020-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5854)training: 2/10 (0.5854)validation : 2/10 (0.5854)training: 3/10 (0.5854)validation : 3/10 (0.5854)training: 4/10 (0.5854)validation : 4/10 (0.5832)training: 5/10 (0.5832)validation : 5/10 (0.5810)training: 6/10 (0.5810)validation : 6/10 (0.5810)early stopping at 6 with loss 0.5810
AttentionModel-training is done: 6/10
2020-02-29 | reset count: 0 | final loss: 0.5810 at epoch 5
making sample. (inference: True, date:2020-03-31 00:00:00, date_number: 399)
making sample. (inference: False, date:2020-02-29 00:00:00, date_number: 398)
[AttentionModel/2020-03-31 | get_logger | INFO]: ====================AttentionModel/2020-03-31 start====================
making sample. (inference: True, date:2020-04-30 00:00:00, date_number: 400)
making sample. (inference: False, date:2020-03-31 00:00:00, date_number: 399)
[AttentionModel/2020-04-30 | get_logger | INFO]: ====================AttentionModel/2020-04-30 start====================
[strategy_integration.py] set_seed(2020-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5701)training: 2/10 (0.5701)validation : 2/10 (0.5701)training: 3/10 (0.5701)validation : 3/10 (0.5701)training: 4/10 (0.5701)validation : 4/10 (0.5701)training: 5/10 (0.5701)validation : 5/10 (0.5689)training: 6/10 (0.5689)validation : 6/10 (0.5689)training: 7/10 (0.5689)validation : 7/10 (0.5689)early stopping at 7 with loss 0.5689
AttentionModel-training is done: 7/10
2020-03-31 | reset count: 0 | final loss: 0.5689 at epoch 5
making sample. (inference: True, date:2020-05-31 00:00:00, date_number: 401)
making sample. (inference: False, date:2020-04-30 00:00:00, date_number: 400)
[AttentionModel/2020-05-31 | get_logger | INFO]: ====================AttentionModel/2020-05-31 start====================
[strategy_integration.py] set_seed(2020-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5781)training: 2/10 (0.5781)validation : 2/10 (0.5753)training: 3/10 (0.5753)validation : 3/10 (0.5746)training: 4/10 (0.5746)validation : 4/10 (0.5746)training: 5/10 (0.5746)validation : 5/10 (0.5731)training: 6/10 (0.5731)validation : 6/10 (0.5717)training: 7/10 (0.5717)validation : 7/10 (0.5717)training: 8/10 (0.5717)validation : 8/10 (0.5717)training: 9/10 (0.5717)validation : 9/10 (0.5717)early stopping at 9 with loss 0.5717
AttentionModel-training is done: 9/10
2020-04-30 | reset count: 0 | final loss: 0.5717 at epoch 6
making sample. (inference: True, date:2020-06-30 00:00:00, date_number: 402)
making sample. (inference: False, date:2020-05-31 00:00:00, date_number: 401)
[AttentionModel/2020-06-30 | get_logger | INFO]: ====================AttentionModel/2020-06-30 start====================
[strategy_integration.py] set_seed(2020-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5791)training: 2/10 (0.5791)validation : 2/10 (0.5760)training: 3/10 (0.5760)validation : 3/10 (0.5760)training: 4/10 (0.5760)validation : 4/10 (0.5758)training: 5/10 (0.5758)validation : 5/10 (0.5758)training: 6/10 (0.5758)validation : 6/10 (0.5758)training: 7/10 (0.5758)validation : 7/10 (0.5758)early stopping at 7 with loss 0.5758
AttentionModel-training is done: 7/10
2020-05-31 | reset count: 0 | final loss: 0.5758 at epoch 5
making sample. (inference: True, date:2020-07-31 00:00:00, date_number: 403)
making sample. (inference: False, date:2020-06-30 00:00:00, date_number: 402)
[AttentionModel/2020-07-31 | get_logger | INFO]: ====================AttentionModel/2020-07-31 start====================
[strategy_integration.py] set_seed(2020-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5818)training: 2/10 (0.5818)validation : 2/10 (0.5817)training: 3/10 (0.5817)validation : 3/10 (0.5750)training: 4/10 (0.5750)validation : 4/10 (0.5750)training: 5/10 (0.5750)validation : 5/10 (0.5750)training: 6/10 (0.5750)validation : 6/10 (0.5750)early stopping at 6 with loss 0.5750
AttentionModel-training is done: 6/10
2020-06-30 | reset count: 0 | final loss: 0.5750 at epoch 3
making sample. (inference: True, date:2020-08-31 00:00:00, date_number: 404)
making sample. (inference: False, date:2020-07-31 00:00:00, date_number: 403)
[AttentionModel/2020-08-31 | get_logger | INFO]: ====================AttentionModel/2020-08-31 start====================
[strategy_integration.py] set_seed(2020-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5811)training: 2/10 (0.5811)validation : 2/10 (0.5807)training: 3/10 (0.5807)validation : 3/10 (0.5748)training: 4/10 (0.5748)validation : 4/10 (0.5748)training: 5/10 (0.5748)validation : 5/10 (0.5733)training: 6/10 (0.5733)validation : 6/10 (0.5733)early stopping at 6 with loss 0.5733
AttentionModel-training is done: 6/10
2020-07-31 | reset count: 0 | final loss: 0.5733 at epoch 5
making sample. (inference: True, date:2020-09-30 00:00:00, date_number: 405)
making sample. (inference: False, date:2020-08-31 00:00:00, date_number: 404)
[AttentionModel/2020-09-30 | get_logger | INFO]: ====================AttentionModel/2020-09-30 start====================
[strategy_integration.py] set_seed(2020-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5677)training: 2/10 (0.5677)validation : 2/10 (0.5649)training: 3/10 (0.5649)validation : 3/10 (0.5649)training: 4/10 (0.5649)validation : 4/10 (0.5649)training: 5/10 (0.5649)validation : 5/10 (0.5649)early stopping at 5 with loss 0.5649
AttentionModel-training is done: 5/10
2020-08-31 | reset count: 0 | final loss: 0.5649 at epoch 2
making sample. (inference: True, date:2020-10-31 00:00:00, date_number: 406)
making sample. (inference: False, date:2020-09-30 00:00:00, date_number: 405)
[AttentionModel/2020-10-31 | get_logger | INFO]: ====================AttentionModel/2020-10-31 start====================
[strategy_integration.py] set_seed(2020-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5812)training: 2/10 (0.5812)validation : 2/10 (0.5803)training: 3/10 (0.5803)validation : 3/10 (0.5785)training: 4/10 (0.5785)validation : 4/10 (0.5785)training: 5/10 (0.5785)validation : 5/10 (0.5785)early stopping at 5 with loss 0.5785
AttentionModel-training is done: 5/10
2020-09-30 | reset count: 0 | final loss: 0.5785 at epoch 3
making sample. (inference: True, date:2020-11-30 00:00:00, date_number: 407)
making sample. (inference: False, date:2020-10-31 00:00:00, date_number: 406)
[AttentionModel/2020-11-30 | get_logger | INFO]: ====================AttentionModel/2020-11-30 start====================
[strategy_integration.py] set_seed(2020-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5725)training: 2/10 (0.5725)validation : 2/10 (0.5725)training: 3/10 (0.5725)validation : 3/10 (0.5725)training: 4/10 (0.5725)validation : 4/10 (0.5725)training: 5/10 (0.5725)validation : 5/10 (0.5725)early stopping at 5 with loss 0.5725
AttentionModel-training is done: 5/10
2020-10-31 | reset count: 0 | final loss: 0.5725 at epoch 1
making sample. (inference: True, date:2020-12-31 00:00:00, date_number: 408)
making sample. (inference: False, date:2020-11-30 00:00:00, date_number: 407)
[AttentionModel/2020-12-31 | get_logger | INFO]: ====================AttentionModel/2020-12-31 start====================
[strategy_integration.py] set_seed(2020-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5808)training: 2/10 (0.5808)validation : 2/10 (0.5764)training: 3/10 (0.5764)validation : 3/10 (0.5764)training: 4/10 (0.5764)validation : 4/10 (0.5764)training: 5/10 (0.5764)validation : 5/10 (0.5764)training: 6/10 (0.5764)validation : 6/10 (0.5736)training: 7/10 (0.5736)validation : 7/10 (0.5736)training: 8/10 (0.5736)validation : 8/10 (0.5736)training: 9/10 (0.5736)validation : 9/10 (0.5736)early stopping at 9 with loss 0.5736
AttentionModel-training is done: 9/10
2020-11-30 | reset count: 0 | final loss: 0.5736 at epoch 6
[strategy_integration.py] set_seed(2020-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5649)training: 2/10 (0.5649)validation : 2/10 (0.5592)training: 3/10 (0.5592)validation : 3/10 (0.5592)training: 4/10 (0.5592)validation : 4/10 (0.5592)training: 5/10 (0.5592)validation : 5/10 (0.5592)early stopping at 5 with loss 0.5592
AttentionModel-training is done: 5/10
2020-12-31 | reset count: 0 | final loss: 0.5592 at epoch 2
making sample. (inference: True, date:2021-01-31 00:00:00, date_number: 409)
making sample. (inference: False, date:2020-12-31 00:00:00, date_number: 408)
[AttentionModel/2021-01-31 | get_logger | INFO]: ====================AttentionModel/2021-01-31 start====================
making sample. (inference: True, date:2021-02-28 00:00:00, date_number: 410)
making sample. (inference: False, date:2021-01-31 00:00:00, date_number: 409)
[AttentionModel/2021-02-28 | get_logger | INFO]: ====================AttentionModel/2021-02-28 start====================
[strategy_integration.py] set_seed(2021-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5609)training: 2/10 (0.5609)validation : 2/10 (0.5540)training: 3/10 (0.5540)validation : 3/10 (0.5540)training: 4/10 (0.5540)validation : 4/10 (0.5540)training: 5/10 (0.5540)validation : 5/10 (0.5540)training: 6/10 (0.5540)validation : 6/10 (0.5540)training: 7/10 (0.5540)validation : 7/10 (0.5540)early stopping at 7 with loss 0.5540
AttentionModel-training is done: 7/10
2021-01-31 | reset count: 0 | final loss: 0.5540 at epoch 2
making sample. (inference: True, date:2021-03-31 00:00:00, date_number: 411)
making sample. (inference: False, date:2021-02-28 00:00:00, date_number: 410)
[strategy_integration.py] set_seed(2021-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5729)training: 2/10 (0.5729)validation : 2/10 (0.5707)training: 3/10 (0.5707)validation : 3/10 (0.5707)training: 4/10 (0.5707)validation : 4/10 (0.5707)training: 5/10 (0.5707)validation : 5/10 (0.5697)training: 6/10 (0.5697)validation : 6/10 (0.5647)training: 7/10 (0.5647)validation : 7/10 (0.5647)training: 8/10 (0.5647)validation : 8/10 (0.5647)early stopping at 8 with loss 0.5647
AttentionModel-training is done: 8/10
2021-02-28 | reset count: 0 | final loss: 0.5647 at epoch 6
[AttentionModel/2021-03-31 | get_logger | INFO]: ====================AttentionModel/2021-03-31 start====================
making sample. (inference: True, date:2021-04-30 00:00:00, date_number: 412)
making sample. (inference: False, date:2021-03-31 00:00:00, date_number: 411)
[AttentionModel/2021-04-30 | get_logger | INFO]: ====================AttentionModel/2021-04-30 start====================
[strategy_integration.py] set_seed(2021-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5763)training: 2/10 (0.5763)validation : 2/10 (0.5763)training: 3/10 (0.5763)validation : 3/10 (0.5718)training: 4/10 (0.5718)validation : 4/10 (0.5718)training: 5/10 (0.5718)validation : 5/10 (0.5718)training: 6/10 (0.5718)validation : 6/10 (0.5718)early stopping at 6 with loss 0.5718
AttentionModel-training is done: 6/10
2021-03-31 | reset count: 0 | final loss: 0.5718 at epoch 3
making sample. (inference: True, date:2021-05-31 00:00:00, date_number: 413)
making sample. (inference: False, date:2021-04-30 00:00:00, date_number: 412)
[AttentionModel/2021-05-31 | get_logger | INFO]: ====================AttentionModel/2021-05-31 start====================
[strategy_integration.py] set_seed(2021-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5693)training: 2/10 (0.5693)validation : 2/10 (0.5664)training: 3/10 (0.5664)validation : 3/10 (0.5604)training: 4/10 (0.5604)validation : 4/10 (0.5604)training: 5/10 (0.5604)validation : 5/10 (0.5604)training: 6/10 (0.5604)validation : 6/10 (0.5604)early stopping at 6 with loss 0.5604
AttentionModel-training is done: 6/10
2021-04-30 | reset count: 0 | final loss: 0.5604 at epoch 3
making sample. (inference: True, date:2021-06-30 00:00:00, date_number: 414)
making sample. (inference: False, date:2021-05-31 00:00:00, date_number: 413)
[strategy_integration.py] set_seed(2021-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5730)training: 2/10 (0.5730)validation : 2/10 (0.5691)training: 3/10 (0.5691)validation : 3/10 (0.5691)training: 4/10 (0.5691)validation : 4/10 (0.5691)training: 5/10 (0.5691)validation : 5/10 (0.5691)early stopping at 5 with loss 0.5691
AttentionModel-training is done: 5/10
2021-05-31 | reset count: 0 | final loss: 0.5691 at epoch 2
[AttentionModel/2021-06-30 | get_logger | INFO]: ====================AttentionModel/2021-06-30 start====================
making sample. (inference: True, date:2021-07-31 00:00:00, date_number: 415)
making sample. (inference: False, date:2021-06-30 00:00:00, date_number: 414)
[AttentionModel/2021-07-31 | get_logger | INFO]: ====================AttentionModel/2021-07-31 start====================
[strategy_integration.py] set_seed(2021-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5731)training: 2/10 (0.5731)validation : 2/10 (0.5686)training: 3/10 (0.5686)validation : 3/10 (0.5685)training: 4/10 (0.5685)validation : 4/10 (0.5685)training: 5/10 (0.5685)validation : 5/10 (0.5678)training: 6/10 (0.5678)validation : 6/10 (0.5658)training: 7/10 (0.5658)validation : 7/10 (0.5658)training: 8/10 (0.5658)validation : 8/10 (0.5658)early stopping at 8 with loss 0.5658
AttentionModel-training is done: 8/10
2021-06-30 | reset count: 0 | final loss: 0.5658 at epoch 6
[strategy_integration.py] set_seed(2021-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5761)training: 2/10 (0.5761)validation : 2/10 (0.5729)training: 3/10 (0.5729)validation : 3/10 (0.5689)training: 4/10 (0.5689)validation : 4/10 (0.5689)training: 5/10 (0.5689)validation : 5/10 (0.5689)training: 6/10 (0.5689)validation : 6/10 (0.5689)early stopping at 6 with loss 0.5689
AttentionModel-training is done: 6/10
2021-07-31 | reset count: 0 | final loss: 0.5689 at epoch 3
making sample. (inference: True, date:2021-08-31 00:00:00, date_number: 416)
making sample. (inference: False, date:2021-07-31 00:00:00, date_number: 415)
[AttentionModel/2021-08-31 | get_logger | INFO]: ====================AttentionModel/2021-08-31 start====================
making sample. (inference: True, date:2021-09-30 00:00:00, date_number: 417)
making sample. (inference: False, date:2021-08-31 00:00:00, date_number: 416)
[AttentionModel/2021-09-30 | get_logger | INFO]: ====================AttentionModel/2021-09-30 start====================
[strategy_integration.py] set_seed(2021-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5604)training: 2/10 (0.5604)validation : 2/10 (0.5604)training: 3/10 (0.5604)validation : 3/10 (0.5604)training: 4/10 (0.5604)validation : 4/10 (0.5604)training: 5/10 (0.5604)validation : 5/10 (0.5604)early stopping at 5 with loss 0.5604
AttentionModel-training is done: 5/10
2021-08-31 | reset count: 0 | final loss: 0.5604 at epoch 1
making sample. (inference: True, date:2021-10-31 00:00:00, date_number: 418)
making sample. (inference: False, date:2021-09-30 00:00:00, date_number: 417)
[AttentionModel/2021-10-31 | get_logger | INFO]: ====================AttentionModel/2021-10-31 start====================
[strategy_integration.py] set_seed(2021-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5836)training: 2/10 (0.5836)validation : 2/10 (0.5828)training: 3/10 (0.5828)validation : 3/10 (0.5828)training: 4/10 (0.5828)validation : 4/10 (0.5791)training: 5/10 (0.5791)validation : 5/10 (0.5791)training: 6/10 (0.5791)validation : 6/10 (0.5791)training: 7/10 (0.5791)validation : 7/10 (0.5791)early stopping at 7 with loss 0.5791
AttentionModel-training is done: 7/10
2021-09-30 | reset count: 0 | final loss: 0.5791 at epoch 4
[strategy_integration.py] set_seed(2021-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5786)training: 2/10 (0.5786)validation : 2/10 (0.5786)training: 3/10 (0.5786)validation : 3/10 (0.5755)training: 4/10 (0.5755)validation : 4/10 (0.5718)training: 5/10 (0.5718)validation : 5/10 (0.5718)training: 6/10 (0.5718)validation : 6/10 (0.5718)training: 7/10 (0.5718)validation : 7/10 (0.5718)early stopping at 7 with loss 0.5718
AttentionModel-training is done: 7/10
2021-10-31 | reset count: 0 | final loss: 0.5718 at epoch 4
[strategy | get_logger | INFO]: ====================strategy start====================
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.155 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠹ [Compustat API] : Loading masking data⠹ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache✓ [Compustat API] : Loading get_historic_universe from cache done in 1.738 secs
⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.039 secs
⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data✓ [Compustat API] : Loading monthly price data done in 18.974 secs
⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data✓ [Compustat API] : Loading monthly price data done in 25.034 secs
⠋ [Compustat API] : Loading monthly market value⠙ [Compustat API] : Loading monthly market value⠹ [Compustat API] : Loading monthly market value⠸ [Compustat API] : Loading monthly market value⠼ [Compustat API] : Loading monthly market value⠴ [Compustat API] : Loading monthly market value⠦ [Compustat API] : Loading monthly market value⠧ [Compustat API] : Loading monthly market value⠇ [Compustat API] : Loading monthly market value⠏ [Compustat API] : Loading monthly market value⠋ [Compustat API] : Loading monthly market value⠙ [Compustat API] : Loading monthly market value⠹ [Compustat API] : Loading monthly market value⠸ [Compustat API] : Loading monthly market value⠼ [Compustat API] : Loading monthly market value⠴ [Compustat API] : Loading monthly market value⠦ [Compustat API] : Loading monthly market value⠧ [Compustat API] : Loading monthly market value⠇ [Compustat API] : Loading monthly market value⠏ [Compustat API] : Loading monthly market value⠋ [Compustat API] : Loading monthly market value⠙ [Compustat API] : Loading monthly market value⠹ [Compustat API] : Loading monthly market value⠸ [Compustat API] : Loading monthly market value⠼ [Compustat API] : Loading monthly market value⠴ [Compustat API] : Loading monthly market value⠦ [Compustat API] : Loading monthly market value⠧ [Compustat API] : Loading monthly market value⠇ [Compustat API] : Loading monthly market value⠏ [Compustat API] : Loading monthly market valueOMP: Info #274: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.
⠋ [Compustat API] : Loading monthly market value⠙ [Compustat API] : Loading monthly market value⠹ [Compustat API] : Loading monthly market value⠸ [Compustat API] : Loading monthly market value⠼ [Compustat API] : Loading monthly market value⠴ [Compustat API] : Loading monthly market value⠦ [Compustat API] : Loading monthly market value⠧ [Compustat API] : Loading monthly market value⠇ [Compustat API] : Loading monthly market value⠏ [Compustat API] : Loading monthly market value⠋ [Compustat API] : Loading monthly market value⠙ [Compustat API] : Loading monthly market value⠹ [Compustat API] : Loading monthly market value⠸ [Compustat API] : Loading monthly market value⠼ [Compustat API] : Loading monthly market value⠴ [Compustat API] : Loading monthly market value⠦ [Compustat API] : Loading monthly market value⠧ [Compustat API] : Loading monthly market value⠇ [Compustat API] : Loading monthly market value⠏ [Compustat API] : Loading monthly market value⠋ [Compustat API] : Loading monthly market value⠙ [Compustat API] : Loading monthly market value⠹ [Compustat API] : Loading monthly market value⠸ [Compustat API] : Loading monthly market value⠼ [Compustat API] : Loading monthly market value⠴ [Compustat API] : Loading monthly market value⠦ [Compustat API] : Loading monthly market value⠧ [Compustat API] : Loading monthly market value⠇ [Compustat API] : Loading monthly market value⠏ [Compustat API] : Loading monthly market value⠋ [Compustat API] : Loading monthly market value⠙ [Compustat API] : Loading monthly market value⠹ [Compustat API] : Loading monthly market value⠸ [Compustat API] : Loading monthly market value⠼ [Compustat API] : Loading monthly market value⠴ [Compustat API] : Loading monthly market value⠦ [Compustat API] : Loading monthly market value⠧ [Compustat API] : Loading monthly market value⠇ [Compustat API] : Loading monthly market value⠏ [Compustat API] : Loading monthly market value⠋ [Compustat API] : Loading monthly market value⠙ [Compustat API] : Loading monthly market value⠹ [Compustat API] : Loading monthly market value⠸ [Compustat API] : Loading monthly market value⠼ [Compustat API] : Loading monthly market value⠴ [Compustat API] : Loading monthly market value⠦ [Compustat API] : Loading monthly market value⠧ [Compustat API] : Loading monthly market value⠇ [Compustat API] : Loading monthly market value⠏ [Compustat API] : Loading monthly market value⠋ [Compustat API] : Loading monthly market value⠙ [Compustat API] : Loading monthly market value⠹ [Compustat API] : Loading monthly market value⠸ [Compustat API] : Loading monthly market value⠼ [Compustat API] : Loading monthly market value⠴ [Compustat API] : Loading monthly market value⠦ [Compustat API] : Loading monthly market value⠧ [Compustat API] : Loading monthly market value⠇ [Compustat API] : Loading monthly market value⠏ [Compustat API] : Loading monthly market value⠋ [Compustat API] : Loading monthly market value⠙ [Compustat API] : Loading monthly market value⠹ [Compustat API] : Loading monthly market value⠸ [Compustat API] : Loading monthly market value⠼ [Compustat API] : Loading monthly market value⠴ [Compustat API] : Loading monthly market value⠦ [Compustat API] : Loading monthly market value⠧ [Compustat API] : Loading monthly market value⠇ [Compustat API] : Loading monthly market value⠏ [Compustat API] : Loading monthly market value⠋ [Compustat API] : Loading monthly market value⠙ [Compustat API] : Loading monthly market value⠹ [Compustat API] : Loading monthly market value⠸ [Compustat API] : Loading monthly market value⠼ [Compustat API] : Loading monthly market value⠴ [Compustat API] : Loading monthly market value⠦ [Compustat API] : Loading monthly market value⠧ [Compustat API] : Loading monthly market value⠇ [Compustat API] : Loading monthly market value✓ [Compustat API] : Loading monthly market value done in 39.982 secs
⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data✓ [Compustat API] : Loading monthly price data done in 18.607 secs
⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data✓ [Compustat API] : Loading monthly price data done in 12.408 secs
⠋ [Compustat API] : Loading monthly volume data⠙ [Compustat API] : Loading monthly volume data⠹ [Compustat API] : Loading monthly volume data⠸ [Compustat API] : Loading monthly volume data⠼ [Compustat API] : Loading monthly volume data[strategy | _get_infer_dirs | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick88_test01/infer 이 strategy 대상에 추가됨
[strategy | _backtest | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick88_test01/infer is starting
⠴ [Compustat API] : Loading monthly volume data⠦ [Compustat API] : Loading monthly volume data⠧ [Compustat API] : Loading monthly volume data⠇ [Compustat API] : Loading monthly volume data⠏ [Compustat API] : Loading monthly volume data⠋ [Compustat API] : Loading monthly volume data⠙ [Compustat API] : Loading monthly volume data⠹ [Compustat API] : Loading monthly volume data⠸ [Compustat API] : Loading monthly volume data⠼ [Compustat API] : Loading monthly volume data⠴ [Compustat API] : Loading monthly volume data⠦ [Compustat API] : Loading monthly volume data⠧ [Compustat API] : Loading monthly volume data⠇ [Compustat API] : Loading monthly volume data⠏ [Compustat API] : Loading monthly volume data⠋ [Compustat API] : Loading monthly volume data⠙ [Compustat API] : Loading monthly volume data⠹ [Compustat API] : Loading monthly volume data⠸ [Compustat API] : Loading monthly volume data⠼ [Compustat API] : Loading monthly volume data⠴ [Compustat API] : Loading monthly volume data⠦ [Compustat API] : Loading monthly volume data⠧ [Compustat API] : Loading monthly volume data⠇ [Compustat API] : Loading monthly volume data✓ [Compustat API] : Loading monthly volume data done in 12.579 secs
[strategy | _allocating | INFO]: [STRATEGY] allocating start
2015-12-31
2016-01-31
2016-02-29
2016-03-31
2016-04-30
2016-05-31
2016-06-30
2016-07-31
2016-08-31
2016-09-30
2016-10-31
2016-11-30
2016-12-31
2017-01-31
2017-02-28
2017-03-31
2017-04-30
2017-05-31
2017-06-30
2017-07-31
2017-08-31
2017-09-30
2017-10-31
2017-11-30
2017-12-31
2018-01-31
2018-02-28
2018-03-31
2018-04-30
2018-05-31
2018-06-30
2018-07-31
2018-08-31
2018-09-30
2018-10-31
2018-11-30
2018-12-31
2019-01-31
2019-02-28
2019-03-31
2019-04-30
2019-05-31
2019-06-30
2019-07-31
2019-08-31
2019-09-30
2019-10-31
2019-11-30
2019-12-31
2020-01-31
2020-02-29
2020-03-31
2020-04-30
2020-05-31
2020-06-30
2020-07-31
2020-08-31
2020-09-30
2020-10-31
2020-11-30
2020-12-31
2021-01-31
2021-02-28
2021-03-31
2021-04-30
2021-05-31
2021-06-30
2021-07-31
2021-08-31
2021-09-30
2021-10-31
⠋ [Compustat API] : Loading GICS⠙ [Compustat API] : Loading GICS⠹ [Compustat API] : Loading GICS⠸ [Compustat API] : Loading GICS⠼ [Compustat API] : Loading GICS✓ [Compustat API] : Loading GICS done in 1.373 secs
⠋ [Compustat API] : Loading GICS⠙ [Compustat API] : Loading GICS⠹ [Compustat API] : Loading GICS⠸ [Compustat API] : Loading GICS⠼ [Compustat API] : Loading GICS✓ [Compustat API] : Loading GICS done in 1.329 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.07 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.09 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.08 secs
⠋ [Compustat API] : Loading fundamental data ni / seq as roe⠙ [Compustat API] : Loading fundamental data ni / seq as roe⠹ [Compustat API] : Loading fundamental data ni / seq as roe⠸ [Compustat API] : Loading fundamental data ni / seq as roe⠼ [Compustat API] : Loading fundamental data ni / seq as roe⠴ [Compustat API] : Loading fundamental data ni / seq as roe⠦ [Compustat API] : Loading fundamental data ni / seq as roe⠧ [Compustat API] : Loading fundamental data ni / seq as roe⠇ [Compustat API] : Loading fundamental data ni / seq as roe⠏ [Compustat API] : Loading fundamental data ni / seq as roe⠋ [Compustat API] : Loading fundamental data ni / seq as roe⠙ [Compustat API] : Loading fundamental data ni / seq as roe⠹ [Compustat API] : Loading fundamental data ni / seq as roe⠸ [Compustat API] : Loading fundamental data ni / seq as roe⠼ [Compustat API] : Loading fundamental data ni / seq as roe⠴ [Compustat API] : Loading fundamental data ni / seq as roe✓ [Compustat API] : Loading fundamental data ni / seq as roe done in 4.853 secs
⠋ [Compustat API] : Loading fundamental data ni / at as roa⠙ [Compustat API] : Loading fundamental data ni / at as roa⠹ [Compustat API] : Loading fundamental data ni / at as roa⠸ [Compustat API] : Loading fundamental data ni / at as roa⠼ [Compustat API] : Loading fundamental data ni / at as roa⠴ [Compustat API] : Loading fundamental data ni / at as roa⠦ [Compustat API] : Loading fundamental data ni / at as roa⠧ [Compustat API] : Loading fundamental data ni / at as roa⠇ [Compustat API] : Loading fundamental data ni / at as roa⠏ [Compustat API] : Loading fundamental data ni / at as roa⠋ [Compustat API] : Loading fundamental data ni / at as roa⠙ [Compustat API] : Loading fundamental data ni / at as roa⠹ [Compustat API] : Loading fundamental data ni / at as roa⠸ [Compustat API] : Loading fundamental data ni / at as roa⠼ [Compustat API] : Loading fundamental data ni / at as roa⠴ [Compustat API] : Loading fundamental data ni / at as roa⠦ [Compustat API] : Loading fundamental data ni / at as roa⠧ [Compustat API] : Loading fundamental data ni / at as roa⠇ [Compustat API] : Loading fundamental data ni / at as roa✓ [Compustat API] : Loading fundamental data ni / at as roa done in 5.78 secs
⠋ [Compustat API] : Loading fundamental data sale-cogs as gp⠙ [Compustat API] : Loading fundamental data sale-cogs as gp⠹ [Compustat API] : Loading fundamental data sale-cogs as gp⠸ [Compustat API] : Loading fundamental data sale-cogs as gp⠼ [Compustat API] : Loading fundamental data sale-cogs as gp⠴ [Compustat API] : Loading fundamental data sale-cogs as gp⠦ [Compustat API] : Loading fundamental data sale-cogs as gp⠧ [Compustat API] : Loading fundamental data sale-cogs as gp⠇ [Compustat API] : Loading fundamental data sale-cogs as gp⠏ [Compustat API] : Loading fundamental data sale-cogs as gp⠋ [Compustat API] : Loading fundamental data sale-cogs as gp⠙ [Compustat API] : Loading fundamental data sale-cogs as gp⠹ [Compustat API] : Loading fundamental data sale-cogs as gp⠸ [Compustat API] : Loading fundamental data sale-cogs as gp⠼ [Compustat API] : Loading fundamental data sale-cogs as gp⠴ [Compustat API] : Loading fundamental data sale-cogs as gp⠦ [Compustat API] : Loading fundamental data sale-cogs as gp⠧ [Compustat API] : Loading fundamental data sale-cogs as gp⠇ [Compustat API] : Loading fundamental data sale-cogs as gp✓ [Compustat API] : Loading fundamental data sale-cogs as gp done in 5.666 secs
⠋ [Compustat API] : Loading fundamental data at as at⠙ [Compustat API] : Loading fundamental data at as at⠹ [Compustat API] : Loading fundamental data at as at⠸ [Compustat API] : Loading fundamental data at as at⠼ [Compustat API] : Loading fundamental data at as at⠴ [Compustat API] : Loading fundamental data at as at⠦ [Compustat API] : Loading fundamental data at as at⠧ [Compustat API] : Loading fundamental data at as at⠇ [Compustat API] : Loading fundamental data at as at⠏ [Compustat API] : Loading fundamental data at as at⠋ [Compustat API] : Loading fundamental data at as at⠙ [Compustat API] : Loading fundamental data at as at⠹ [Compustat API] : Loading fundamental data at as at⠸ [Compustat API] : Loading fundamental data at as at⠼ [Compustat API] : Loading fundamental data at as at✓ [Compustat API] : Loading fundamental data at as at done in 4.504 secs
⠋ [Compustat API] : Loading fundamental data ib as ib⠙ [Compustat API] : Loading fundamental data ib as ib⠹ [Compustat API] : Loading fundamental data ib as ib⠸ [Compustat API] : Loading fundamental data ib as ib⠼ [Compustat API] : Loading fundamental data ib as ib⠴ [Compustat API] : Loading fundamental data ib as ib⠦ [Compustat API] : Loading fundamental data ib as ib⠧ [Compustat API] : Loading fundamental data ib as ib⠇ [Compustat API] : Loading fundamental data ib as ib⠏ [Compustat API] : Loading fundamental data ib as ib⠋ [Compustat API] : Loading fundamental data ib as ib⠙ [Compustat API] : Loading fundamental data ib as ib⠹ [Compustat API] : Loading fundamental data ib as ib⠸ [Compustat API] : Loading fundamental data ib as ib⠼ [Compustat API] : Loading fundamental data ib as ib⠴ [Compustat API] : Loading fundamental data ib as ib✓ [Compustat API] : Loading fundamental data ib as ib done in 4.589 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.082 secs
⠋ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠙ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠹ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠸ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠼ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠴ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠦ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠧ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠇ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠏ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠋ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠙ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠹ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠸ [Compustat API] : Loading fundamental data pstkrv as pstkrv[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_weight_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] performance.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] combined.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] weight.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] universe.csv is saved.
[comparison | get_logger | INFO]: ====================comparison start====================
[comparison | compare | INFO]: [5. COMPARISON] comparison is saved.
[comparison | compare | INFO]:                        Harvest_Green_concept001_pick88_test01_price_return  ...  SPCOMP_total_win
2016-01-31_2021-04-30                                           0.223687    ...          0.765625

[1 rows x 27 columns]
⠼ [Compustat API] : Loading fundamental data pstkrv as pstkrv✓ [Compustat API] : Loading fundamental data pstkrv as pstkrv done in 4.417 secs
⠋ [Compustat API] : Loading fundamental data pstkl as pstkl⠙ [Compustat API] : Loading fundamental data pstkl as pstkl⠹ [Compustat API] : Loading fundamental data pstkl as pstkl⠸ [Compustat API] : Loading fundamental data pstkl as pstkl⠼ [Compustat API] : Loading fundamental data pstkl as pstkl⠴ [Compustat API] : Loading fundamental data pstkl as pstkl⠦ [Compustat API] : Loading fundamental data pstkl as pstkl⠧ [Compustat API] : Loading fundamental data pstkl as pstkl⠇ [Compustat API] : Loading fundamental data pstkl as pstkl⠏ [Compustat API] : Loading fundamental data pstkl as pstkl⠋ [Compustat API] : Loading fundamental data pstkl as pstkl⠙ [Compustat API] : Loading fundamental data pstkl as pstkl⠹ [Compustat API] : Loading fundamental data pstkl as pstkl⠸ [Compustat API] : Loading fundamental data pstkl as pstkl⠼ [Compustat API] : Loading fundamental data pstkl as pstkl✓ [Compustat API] : Loading fundamental data pstkl as pstkl done in 4.401 secs
⠋ [Compustat API] : Loading fundamental data pstk as pstk⠙ [Compustat API] : Loading fundamental data pstk as pstk⠹ [Compustat API] : Loading fundamental data pstk as pstk⠸ [Compustat API] : Loading fundamental data pstk as pstk⠼ [Compustat API] : Loading fundamental data pstk as pstk⠴ [Compustat API] : Loading fundamental data pstk as pstk⠦ [Compustat API] : Loading fundamental data pstk as pstk⠧ [Compustat API] : Loading fundamental data pstk as pstk⠇ [Compustat API] : Loading fundamental data pstk as pstk⠏ [Compustat API] : Loading fundamental data pstk as pstk⠋ [Compustat API] : Loading fundamental data pstk as pstk⠙ [Compustat API] : Loading fundamental data pstk as pstk⠹ [Compustat API] : Loading fundamental data pstk as pstk⠸ [Compustat API] : Loading fundamental data pstk as pstk⠼ [Compustat API] : Loading fundamental data pstk as pstk✓ [Compustat API] : Loading fundamental data pstk as pstk done in 4.415 secs
⠋ [Compustat API] : Loading fundamental data seq as seq⠙ [Compustat API] : Loading fundamental data seq as seq⠹ [Compustat API] : Loading fundamental data seq as seq⠸ [Compustat API] : Loading fundamental data seq as seq⠼ [Compustat API] : Loading fundamental data seq as seq⠴ [Compustat API] : Loading fundamental data seq as seq⠦ [Compustat API] : Loading fundamental data seq as seq⠧ [Compustat API] : Loading fundamental data seq as seq⠇ [Compustat API] : Loading fundamental data seq as seq⠏ [Compustat API] : Loading fundamental data seq as seq⠋ [Compustat API] : Loading fundamental data seq as seq⠙ [Compustat API] : Loading fundamental data seq as seq⠹ [Compustat API] : Loading fundamental data seq as seq⠸ [Compustat API] : Loading fundamental data seq as seq⠼ [Compustat API] : Loading fundamental data seq as seq✓ [Compustat API] : Loading fundamental data seq as seq done in 4.427 secs
⠋ [Compustat API] : Loading fundamental data txditc as txditc⠙ [Compustat API] : Loading fundamental data txditc as txditc⠹ [Compustat API] : Loading fundamental data txditc as txditc⠸ [Compustat API] : Loading fundamental data txditc as txditc⠼ [Compustat API] : Loading fundamental data txditc as txditc⠴ [Compustat API] : Loading fundamental data txditc as txditc⠦ [Compustat API] : Loading fundamental data txditc as txditc⠧ [Compustat API] : Loading fundamental data txditc as txditc⠇ [Compustat API] : Loading fundamental data txditc as txditc⠏ [Compustat API] : Loading fundamental data txditc as txditc⠋ [Compustat API] : Loading fundamental data txditc as txditc⠙ [Compustat API] : Loading fundamental data txditc as txditc⠹ [Compustat API] : Loading fundamental data txditc as txditc⠸ [Compustat API] : Loading fundamental data txditc as txditc⠼ [Compustat API] : Loading fundamental data txditc as txditc✓ [Compustat API] : Loading fundamental data txditc as txditc done in 4.413 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.083 secs
⠋ [Compustat API] : Loading fundamental data ib+dp as cf⠙ [Compustat API] : Loading fundamental data ib+dp as cf⠹ [Compustat API] : Loading fundamental data ib+dp as cf⠸ [Compustat API] : Loading fundamental data ib+dp as cf⠼ [Compustat API] : Loading fundamental data ib+dp as cf⠴ [Compustat API] : Loading fundamental data ib+dp as cf⠦ [Compustat API] : Loading fundamental data ib+dp as cf⠧ [Compustat API] : Loading fundamental data ib+dp as cf⠇ [Compustat API] : Loading fundamental data ib+dp as cf⠏ [Compustat API] : Loading fundamental data ib+dp as cf⠋ [Compustat API] : Loading fundamental data ib+dp as cf⠙ [Compustat API] : Loading fundamental data ib+dp as cf⠹ [Compustat API] : Loading fundamental data ib+dp as cf⠸ [Compustat API] : Loading fundamental data ib+dp as cf⠼ [Compustat API] : Loading fundamental data ib+dp as cf⠴ [Compustat API] : Loading fundamental data ib+dp as cf✓ [Compustat API] : Loading fundamental data ib+dp as cf done in 4.68 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.08 secs
⠋ [Compustat API] : Cleaning past days cache✓ [Compustat API] : Cleaning past days cache done in 0.001 secs
auto caching이 활성화 되었습니다. /home/sronly/sr-storage/kirin_cache/kirin_api_cache/2.19.17_2021-11-23에 캐쉬를 저장합니다.
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.149 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading masking data✓ [Compustat API] : Loading get_historic_universe from cache done in 2.015 secs
⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.316 secs
setting tensorflow random seed failed
load_data: op_lev
load_data: roe
load_data: std_u_e
load_data: retail_mfr
load_data: m1
load_data: m2
load_data: ae_m
load_data: ia_ta
load_data: rc_a
load_data: aaa_t10y
load_data: baa_t10y
load_data: aaa_ff
load_data: real_pig
load_data: federal_tg
load_data: real_gdp
load_data: mom_12m_1m
load_data: ram_12m_0m
load_data: vol_3m
load_data: t3y
load_data: t5y
load_data: t7y
load_data: t1y_ff
load_data: t5y_ff
load_data: t10y_t2y
load_data: export_growth
load_data: import_growth
load_data: real_gig
load_data: res_mom_12m_1m_0m
load_data: res_vol_6m_3m_0m
load_data: at
load_data: r_s
load_data: r_a
load_data: fred_ff
load_data: t3m
load_data: t6m
load_data: t2y
load_data: t10y
load_data: aaa
load_data: baa
load_data: core_ppi
load_data: cpi
load_data: pce
load_data: wti
load_data: capa_util
load_data: snp500_pe
load_data: gpa
load_data: rev_surp
load_data: cash_at
load_data: baa_ff
load_data: core_cpi
load_data: core_pce
load_data: corporate_tg
load_data: industrial_prod
load_data: home_pr
load_data: sector_values
load_data: mv
load_data: pr_1m_0m
load_data: mv
making sample. (inference: True, date:2015-12-31 00:00:00, date_number: 348)
making sample. (inference: False, date:2012-12-31 00:00:00, date_number: 312)
making sample. (inference: False, date:2013-01-31 00:00:00, date_number: 313)
making sample. (inference: False, date:2013-02-28 00:00:00, date_number: 314)
making sample. (inference: False, date:2013-03-31 00:00:00, date_number: 315)
making sample. (inference: False, date:2013-04-30 00:00:00, date_number: 316)
making sample. (inference: False, date:2013-05-31 00:00:00, date_number: 317)
making sample. (inference: False, date:2013-06-30 00:00:00, date_number: 318)
making sample. (inference: False, date:2013-07-31 00:00:00, date_number: 319)
making sample. (inference: False, date:2013-08-31 00:00:00, date_number: 320)
making sample. (inference: False, date:2013-09-30 00:00:00, date_number: 321)
making sample. (inference: False, date:2013-10-31 00:00:00, date_number: 322)
making sample. (inference: False, date:2013-11-30 00:00:00, date_number: 323)
making sample. (inference: False, date:2013-12-31 00:00:00, date_number: 324)
making sample. (inference: False, date:2014-01-31 00:00:00, date_number: 325)
making sample. (inference: False, date:2014-02-28 00:00:00, date_number: 326)
making sample. (inference: False, date:2014-03-31 00:00:00, date_number: 327)
making sample. (inference: False, date:2014-04-30 00:00:00, date_number: 328)
making sample. (inference: False, date:2014-05-31 00:00:00, date_number: 329)
making sample. (inference: False, date:2014-06-30 00:00:00, date_number: 330)
making sample. (inference: False, date:2014-07-31 00:00:00, date_number: 331)
making sample. (inference: False, date:2014-08-31 00:00:00, date_number: 332)
making sample. (inference: False, date:2014-09-30 00:00:00, date_number: 333)
making sample. (inference: False, date:2014-10-31 00:00:00, date_number: 334)
making sample. (inference: False, date:2014-11-30 00:00:00, date_number: 335)
making sample. (inference: False, date:2014-12-31 00:00:00, date_number: 336)
making sample. (inference: False, date:2015-01-31 00:00:00, date_number: 337)
making sample. (inference: False, date:2015-02-28 00:00:00, date_number: 338)
making sample. (inference: False, date:2015-03-31 00:00:00, date_number: 339)
making sample. (inference: False, date:2015-04-30 00:00:00, date_number: 340)
making sample. (inference: False, date:2015-05-31 00:00:00, date_number: 341)
making sample. (inference: False, date:2015-06-30 00:00:00, date_number: 342)
making sample. (inference: False, date:2015-07-31 00:00:00, date_number: 343)
making sample. (inference: False, date:2015-08-31 00:00:00, date_number: 344)
making sample. (inference: False, date:2015-09-30 00:00:00, date_number: 345)
making sample. (inference: False, date:2015-10-31 00:00:00, date_number: 346)
making sample. (inference: False, date:2015-11-30 00:00:00, date_number: 347)
[AttentionModel/2015-12-31 | get_logger | INFO]: ====================AttentionModel/2015-12-31 start====================
making sample. (inference: True, date:2016-01-31 00:00:00, date_number: 349)
making sample. (inference: False, date:2015-12-31 00:00:00, date_number: 348)
[AttentionModel/2016-01-31 | get_logger | INFO]: ====================AttentionModel/2016-01-31 start====================
[strategy_integration.py] set_seed(2015-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5701)training: 2/10 (0.5701)validation : 2/10 (0.5695)training: 3/10 (0.5695)validation : 3/10 (0.5695)training: 4/10 (0.5695)validation : 4/10 (0.5675)training: 5/10 (0.5675)validation : 5/10 (0.5675)training: 6/10 (0.5675)validation : 6/10 (0.5675)training: 7/10 (0.5675)validation : 7/10 (0.5675)early stopping at 7 with loss 0.5675
AttentionModel-training is done: 7/10
2015-12-31 | reset count: 0 | final loss: 0.5675 at epoch 4
[strategy_integration.py] set_seed(2016-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5802)training: 2/10 (0.5802)validation : 2/10 (0.5802)training: 3/10 (0.5802)validation : 3/10 (0.5802)training: 4/10 (0.5802)validation : 4/10 (0.5802)training: 5/10 (0.5802)validation : 5/10 (0.5802)early stopping at 5 with loss 0.5802
AttentionModel-training is done: 5/10
2016-01-31 | reset count: 0 | final loss: 0.5802 at epoch 1
making sample. (inference: True, date:2016-02-29 00:00:00, date_number: 350)
making sample. (inference: False, date:2016-01-31 00:00:00, date_number: 349)
[AttentionModel/2016-02-29 | get_logger | INFO]: ====================AttentionModel/2016-02-29 start====================
making sample. (inference: True, date:2016-03-31 00:00:00, date_number: 351)
making sample. (inference: False, date:2016-02-29 00:00:00, date_number: 350)
[AttentionModel/2016-03-31 | get_logger | INFO]: ====================AttentionModel/2016-03-31 start====================
[strategy_integration.py] set_seed(2016-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5881)training: 2/10 (0.5881)validation : 2/10 (0.5851)training: 3/10 (0.5851)validation : 3/10 (0.5828)training: 4/10 (0.5828)validation : 4/10 (0.5828)training: 5/10 (0.5828)validation : 5/10 (0.5828)training: 6/10 (0.5828)validation : 6/10 (0.5828)early stopping at 6 with loss 0.5828
AttentionModel-training is done: 6/10
2016-02-29 | reset count: 0 | final loss: 0.5828 at epoch 3
making sample. (inference: True, date:2016-04-30 00:00:00, date_number: 352)
making sample. (inference: False, date:2016-03-31 00:00:00, date_number: 351)
[AttentionModel/2016-04-30 | get_logger | INFO]: ====================AttentionModel/2016-04-30 start====================
[strategy_integration.py] set_seed(2016-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5779)training: 2/10 (0.5779)validation : 2/10 (0.5779)training: 3/10 (0.5779)validation : 3/10 (0.5779)training: 4/10 (0.5779)validation : 4/10 (0.5779)training: 5/10 (0.5779)validation : 5/10 (0.5766)training: 6/10 (0.5766)validation : 6/10 (0.5763)training: 7/10 (0.5763)validation : 7/10 (0.5741)training: 8/10 (0.5741)validation : 8/10 (0.5741)training: 9/10 (0.5741)validation : 9/10 (0.5741)training: 10/10 (0.5741)validation : 10/10 (0.5741)early stopping at 10 with loss 0.5741
AttentionModel-training is done: 10/10
2016-03-31 | reset count: 0 | final loss: 0.5741 at epoch 7
making sample. (inference: True, date:2016-05-31 00:00:00, date_number: 353)
making sample. (inference: False, date:2016-04-30 00:00:00, date_number: 352)
[AttentionModel/2016-05-31 | get_logger | INFO]: ====================AttentionModel/2016-05-31 start====================
[strategy_integration.py] set_seed(2016-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5835)training: 2/10 (0.5835)validation : 2/10 (0.5787)training: 3/10 (0.5787)validation : 3/10 (0.5781)training: 4/10 (0.5781)validation : 4/10 (0.5781)training: 5/10 (0.5781)validation : 5/10 (0.5775)training: 6/10 (0.5775)validation : 6/10 (0.5763)training: 7/10 (0.5763)validation : 7/10 (0.5763)early stopping at 7 with loss 0.5763
AttentionModel-training is done: 7/10
2016-04-30 | reset count: 0 | final loss: 0.5763 at epoch 6
making sample. (inference: True, date:2016-06-30 00:00:00, date_number: 354)
making sample. (inference: False, date:2016-05-31 00:00:00, date_number: 353)
[AttentionModel/2016-06-30 | get_logger | INFO]: ====================AttentionModel/2016-06-30 start====================
[strategy_integration.py] set_seed(2016-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5703)training: 2/10 (0.5703)validation : 2/10 (0.5683)training: 3/10 (0.5683)validation : 3/10 (0.5673)training: 4/10 (0.5673)validation : 4/10 (0.5673)training: 5/10 (0.5673)validation : 5/10 (0.5672)training: 6/10 (0.5672)validation : 6/10 (0.5672)early stopping at 6 with loss 0.5672
AttentionModel-training is done: 6/10
2016-05-31 | reset count: 0 | final loss: 0.5672 at epoch 5
making sample. (inference: True, date:2016-07-31 00:00:00, date_number: 355)
making sample. (inference: False, date:2016-06-30 00:00:00, date_number: 354)
[AttentionModel/2016-07-31 | get_logger | INFO]: ====================AttentionModel/2016-07-31 start====================
[strategy_integration.py] set_seed(2016-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5710)training: 2/10 (0.5710)validation : 2/10 (0.5710)training: 3/10 (0.5710)validation : 3/10 (0.5710)training: 4/10 (0.5710)validation : 4/10 (0.5677)training: 5/10 (0.5677)validation : 5/10 (0.5677)training: 6/10 (0.5677)validation : 6/10 (0.5677)training: 7/10 (0.5677)validation : 7/10 (0.5677)early stopping at 7 with loss 0.5677
AttentionModel-training is done: 7/10
2016-06-30 | reset count: 0 | final loss: 0.5677 at epoch 4
making sample. (inference: True, date:2016-08-31 00:00:00, date_number: 356)
making sample. (inference: False, date:2016-07-31 00:00:00, date_number: 355)
[AttentionModel/2016-08-31 | get_logger | INFO]: ====================AttentionModel/2016-08-31 start====================
[strategy_integration.py] set_seed(2016-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5715)training: 2/10 (0.5715)validation : 2/10 (0.5715)training: 3/10 (0.5715)validation : 3/10 (0.5715)training: 4/10 (0.5715)validation : 4/10 (0.5715)training: 5/10 (0.5715)validation : 5/10 (0.5683)training: 6/10 (0.5683)validation : 6/10 (0.5683)training: 7/10 (0.5683)validation : 7/10 (0.5683)early stopping at 7 with loss 0.5683
AttentionModel-training is done: 7/10
2016-07-31 | reset count: 0 | final loss: 0.5683 at epoch 5
making sample. (inference: True, date:2016-09-30 00:00:00, date_number: 357)
making sample. (inference: False, date:2016-08-31 00:00:00, date_number: 356)
[AttentionModel/2016-09-30 | get_logger | INFO]: ====================AttentionModel/2016-09-30 start====================
[strategy_integration.py] set_seed(2016-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5731)training: 2/10 (0.5731)validation : 2/10 (0.5717)training: 3/10 (0.5717)validation : 3/10 (0.5717)training: 4/10 (0.5717)validation : 4/10 (0.5704)training: 5/10 (0.5704)validation : 5/10 (0.5660)training: 6/10 (0.5660)validation : 6/10 (0.5660)training: 7/10 (0.5660)validation : 7/10 (0.5660)early stopping at 7 with loss 0.5660
AttentionModel-training is done: 7/10
2016-08-31 | reset count: 0 | final loss: 0.5660 at epoch 5
[strategy_integration.py] set_seed(2016-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5761)training: 2/10 (0.5761)validation : 2/10 (0.5761)training: 3/10 (0.5761)validation : 3/10 (0.5728)training: 4/10 (0.5728)validation : 4/10 (0.5728)training: 5/10 (0.5728)validation : 5/10 (0.5728)early stopping at 5 with loss 0.5728
AttentionModel-training is done: 5/10
2016-09-30 | reset count: 0 | final loss: 0.5728 at epoch 3
making sample. (inference: True, date:2016-10-31 00:00:00, date_number: 358)
making sample. (inference: False, date:2016-09-30 00:00:00, date_number: 357)
[AttentionModel/2016-10-31 | get_logger | INFO]: ====================AttentionModel/2016-10-31 start====================
making sample. (inference: True, date:2016-11-30 00:00:00, date_number: 359)
making sample. (inference: False, date:2016-10-31 00:00:00, date_number: 358)
[AttentionModel/2016-11-30 | get_logger | INFO]: ====================AttentionModel/2016-11-30 start====================
[strategy_integration.py] set_seed(2016-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5684)training: 2/10 (0.5684)validation : 2/10 (0.5684)training: 3/10 (0.5684)validation : 3/10 (0.5684)training: 4/10 (0.5684)validation : 4/10 (0.5679)training: 5/10 (0.5679)validation : 5/10 (0.5679)early stopping at 5 with loss 0.5679
AttentionModel-training is done: 5/10
2016-10-31 | reset count: 0 | final loss: 0.5679 at epoch 4
[strategy_integration.py] set_seed(2016-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5671)training: 2/10 (0.5671)validation : 2/10 (0.5671)training: 3/10 (0.5671)validation : 3/10 (0.5671)training: 4/10 (0.5671)validation : 4/10 (0.5671)training: 5/10 (0.5671)validation : 5/10 (0.5658)early stopping at 5 with loss 0.5658
AttentionModel-training is done: 5/10
2016-11-30 | reset count: 0 | final loss: 0.5658 at epoch 5
making sample. (inference: True, date:2016-12-31 00:00:00, date_number: 360)
making sample. (inference: False, date:2016-11-30 00:00:00, date_number: 359)
[AttentionModel/2016-12-31 | get_logger | INFO]: ====================AttentionModel/2016-12-31 start====================
making sample. (inference: True, date:2017-01-31 00:00:00, date_number: 361)
making sample. (inference: False, date:2016-12-31 00:00:00, date_number: 360)
[AttentionModel/2017-01-31 | get_logger | INFO]: ====================AttentionModel/2017-01-31 start====================
[strategy_integration.py] set_seed(2016-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5648)training: 2/10 (0.5648)validation : 2/10 (0.5638)training: 3/10 (0.5638)validation : 3/10 (0.5599)training: 4/10 (0.5599)validation : 4/10 (0.5599)training: 5/10 (0.5599)validation : 5/10 (0.5589)training: 6/10 (0.5589)validation : 6/10 (0.5589)early stopping at 6 with loss 0.5589
AttentionModel-training is done: 6/10
2016-12-31 | reset count: 0 | final loss: 0.5589 at epoch 5
making sample. (inference: True, date:2017-02-28 00:00:00, date_number: 362)
making sample. (inference: False, date:2017-01-31 00:00:00, date_number: 361)
[AttentionModel/2017-02-28 | get_logger | INFO]: ====================AttentionModel/2017-02-28 start====================
[strategy_integration.py] set_seed(2017-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5738)training: 2/10 (0.5738)validation : 2/10 (0.5715)training: 3/10 (0.5715)validation : 3/10 (0.5694)training: 4/10 (0.5694)validation : 4/10 (0.5694)training: 5/10 (0.5694)validation : 5/10 (0.5694)training: 6/10 (0.5694)validation : 6/10 (0.5694)early stopping at 6 with loss 0.5694
AttentionModel-training is done: 6/10
2017-01-31 | reset count: 0 | final loss: 0.5694 at epoch 3
making sample. (inference: True, date:2017-03-31 00:00:00, date_number: 363)
making sample. (inference: False, date:2017-02-28 00:00:00, date_number: 362)
[AttentionModel/2017-03-31 | get_logger | INFO]: ====================AttentionModel/2017-03-31 start====================
[strategy_integration.py] set_seed(2017-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5590)training: 2/10 (0.5590)validation : 2/10 (0.5560)training: 3/10 (0.5560)validation : 3/10 (0.5557)training: 4/10 (0.5557)validation : 4/10 (0.5517)training: 5/10 (0.5517)validation : 5/10 (0.5510)training: 6/10 (0.5510)validation : 6/10 (0.5510)training: 7/10 (0.5510)validation : 7/10 (0.5510)early stopping at 7 with loss 0.5510
AttentionModel-training is done: 7/10
2017-02-28 | reset count: 0 | final loss: 0.5510 at epoch 5
[strategy_integration.py] set_seed(2017-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5632)training: 2/10 (0.5632)validation : 2/10 (0.5624)training: 3/10 (0.5624)validation : 3/10 (0.5624)training: 4/10 (0.5624)validation : 4/10 (0.5614)training: 5/10 (0.5614)validation : 5/10 (0.5614)early stopping at 5 with loss 0.5614
AttentionModel-training is done: 5/10
2017-03-31 | reset count: 0 | final loss: 0.5614 at epoch 4
making sample. (inference: True, date:2017-04-30 00:00:00, date_number: 364)
making sample. (inference: False, date:2017-03-31 00:00:00, date_number: 363)
[AttentionModel/2017-04-30 | get_logger | INFO]: ====================AttentionModel/2017-04-30 start====================
making sample. (inference: True, date:2017-05-31 00:00:00, date_number: 365)
making sample. (inference: False, date:2017-04-30 00:00:00, date_number: 364)
[AttentionModel/2017-05-31 | get_logger | INFO]: ====================AttentionModel/2017-05-31 start====================
[strategy_integration.py] set_seed(2017-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5610)training: 2/10 (0.5610)validation : 2/10 (0.5610)training: 3/10 (0.5610)validation : 3/10 (0.5610)training: 4/10 (0.5610)validation : 4/10 (0.5610)training: 5/10 (0.5610)validation : 5/10 (0.5610)early stopping at 5 with loss 0.5610
AttentionModel-training is done: 5/10
2017-04-30 | reset count: 0 | final loss: 0.5610 at epoch 4
making sample. (inference: True, date:2017-06-30 00:00:00, date_number: 366)
making sample. (inference: False, date:2017-05-31 00:00:00, date_number: 365)
[AttentionModel/2017-06-30 | get_logger | INFO]: ====================AttentionModel/2017-06-30 start====================
[strategy_integration.py] set_seed(2017-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5741)training: 2/10 (0.5741)validation : 2/10 (0.5684)training: 3/10 (0.5684)validation : 3/10 (0.5684)training: 4/10 (0.5684)validation : 4/10 (0.5684)training: 5/10 (0.5684)validation : 5/10 (0.5684)training: 6/10 (0.5684)validation : 6/10 (0.5684)early stopping at 6 with loss 0.5684
AttentionModel-training is done: 6/10
2017-05-31 | reset count: 0 | final loss: 0.5684 at epoch 2
making sample. (inference: True, date:2017-07-31 00:00:00, date_number: 367)
making sample. (inference: False, date:2017-06-30 00:00:00, date_number: 366)
[AttentionModel/2017-07-31 | get_logger | INFO]: ====================AttentionModel/2017-07-31 start====================
[strategy_integration.py] set_seed(2017-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5576)training: 2/10 (0.5576)validation : 2/10 (0.5545)training: 3/10 (0.5545)validation : 3/10 (0.5545)training: 4/10 (0.5545)validation : 4/10 (0.5545)training: 5/10 (0.5545)validation : 5/10 (0.5545)early stopping at 5 with loss 0.5545
AttentionModel-training is done: 5/10
2017-06-30 | reset count: 0 | final loss: 0.5545 at epoch 2
making sample. (inference: True, date:2017-08-31 00:00:00, date_number: 368)
making sample. (inference: False, date:2017-07-31 00:00:00, date_number: 367)
[AttentionModel/2017-08-31 | get_logger | INFO]: ====================AttentionModel/2017-08-31 start====================
[strategy_integration.py] set_seed(2017-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5670)training: 2/10 (0.5670)validation : 2/10 (0.5662)training: 3/10 (0.5662)validation : 3/10 (0.5638)training: 4/10 (0.5638)validation : 4/10 (0.5638)training: 5/10 (0.5638)validation : 5/10 (0.5621)training: 6/10 (0.5621)validation : 6/10 (0.5621)training: 7/10 (0.5621)validation : 7/10 (0.5621)early stopping at 7 with loss 0.5621
AttentionModel-training is done: 7/10
2017-07-31 | reset count: 0 | final loss: 0.5621 at epoch 5
making sample. (inference: True, date:2017-09-30 00:00:00, date_number: 369)
making sample. (inference: False, date:2017-08-31 00:00:00, date_number: 368)
[AttentionModel/2017-09-30 | get_logger | INFO]: ====================AttentionModel/2017-09-30 start====================
[strategy_integration.py] set_seed(2017-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5693)training: 2/10 (0.5693)validation : 2/10 (0.5657)training: 3/10 (0.5657)validation : 3/10 (0.5657)training: 4/10 (0.5657)validation : 4/10 (0.5628)training: 5/10 (0.5628)validation : 5/10 (0.5628)training: 6/10 (0.5628)validation : 6/10 (0.5628)early stopping at 6 with loss 0.5628
AttentionModel-training is done: 6/10
2017-08-31 | reset count: 0 | final loss: 0.5628 at epoch 4
making sample. (inference: True, date:2017-10-31 00:00:00, date_number: 370)
making sample. (inference: False, date:2017-09-30 00:00:00, date_number: 369)
[AttentionModel/2017-10-31 | get_logger | INFO]: ====================AttentionModel/2017-10-31 start====================
[strategy_integration.py] set_seed(2017-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5690)training: 2/10 (0.5690)validation : 2/10 (0.5647)training: 3/10 (0.5647)validation : 3/10 (0.5638)training: 4/10 (0.5638)validation : 4/10 (0.5638)training: 5/10 (0.5638)validation : 5/10 (0.5638)training: 6/10 (0.5638)validation : 6/10 (0.5638)early stopping at 6 with loss 0.5638
AttentionModel-training is done: 6/10
2017-09-30 | reset count: 0 | final loss: 0.5638 at epoch 3
making sample. (inference: True, date:2017-11-30 00:00:00, date_number: 371)
making sample. (inference: False, date:2017-10-31 00:00:00, date_number: 370)
[AttentionModel/2017-11-30 | get_logger | INFO]: ====================AttentionModel/2017-11-30 start====================
[strategy_integration.py] set_seed(2017-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5611)training: 2/10 (0.5611)validation : 2/10 (0.5611)training: 3/10 (0.5611)validation : 3/10 (0.5611)training: 4/10 (0.5611)validation : 4/10 (0.5611)training: 5/10 (0.5611)validation : 5/10 (0.5611)early stopping at 5 with loss 0.5611
AttentionModel-training is done: 5/10
2017-10-31 | reset count: 0 | final loss: 0.5611 at epoch 1
making sample. (inference: True, date:2017-12-31 00:00:00, date_number: 372)
making sample. (inference: False, date:2017-11-30 00:00:00, date_number: 371)
[AttentionModel/2017-12-31 | get_logger | INFO]: ====================AttentionModel/2017-12-31 start====================
[strategy_integration.py] set_seed(2017-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5728)training: 2/10 (0.5728)validation : 2/10 (0.5692)training: 3/10 (0.5692)validation : 3/10 (0.5657)training: 4/10 (0.5657)validation : 4/10 (0.5657)training: 5/10 (0.5657)validation : 5/10 (0.5657)training: 6/10 (0.5657)validation : 6/10 (0.5657)early stopping at 6 with loss 0.5657
AttentionModel-training is done: 6/10
2017-11-30 | reset count: 0 | final loss: 0.5657 at epoch 3
[strategy_integration.py] set_seed(2017-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5637)training: 2/10 (0.5637)validation : 2/10 (0.5617)training: 3/10 (0.5617)validation : 3/10 (0.5615)training: 4/10 (0.5615)validation : 4/10 (0.5615)training: 5/10 (0.5615)validation : 5/10 (0.5615)training: 6/10 (0.5615)validation : 6/10 (0.5615)early stopping at 6 with loss 0.5615
AttentionModel-training is done: 6/10
2017-12-31 | reset count: 0 | final loss: 0.5615 at epoch 3
making sample. (inference: True, date:2018-01-31 00:00:00, date_number: 373)
making sample. (inference: False, date:2017-12-31 00:00:00, date_number: 372)
[AttentionModel/2018-01-31 | get_logger | INFO]: ====================AttentionModel/2018-01-31 start====================
making sample. (inference: True, date:2018-02-28 00:00:00, date_number: 374)
making sample. (inference: False, date:2018-01-31 00:00:00, date_number: 373)
[AttentionModel/2018-02-28 | get_logger | INFO]: ====================AttentionModel/2018-02-28 start====================
[strategy_integration.py] set_seed(2018-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5767)training: 2/10 (0.5767)validation : 2/10 (0.5669)training: 3/10 (0.5669)validation : 3/10 (0.5669)training: 4/10 (0.5669)validation : 4/10 (0.5669)training: 5/10 (0.5669)validation : 5/10 (0.5669)early stopping at 5 with loss 0.5669
AttentionModel-training is done: 5/10
2018-01-31 | reset count: 0 | final loss: 0.5669 at epoch 2
making sample. (inference: True, date:2018-03-31 00:00:00, date_number: 375)
making sample. (inference: False, date:2018-02-28 00:00:00, date_number: 374)
[strategy_integration.py] set_seed(2018-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5782)training: 2/10 (0.5782)validation : 2/10 (0.5729)training: 3/10 (0.5729)validation : 3/10 (0.5729)training: 4/10 (0.5729)validation : 4/10 (0.5729)training: 5/10 (0.5729)validation : 5/10 (0.5729)training: 6/10 (0.5729)validation : 6/10 (0.5729)early stopping at 6 with loss 0.5729
AttentionModel-training is done: 6/10
2018-02-28 | reset count: 0 | final loss: 0.5729 at epoch 2
[AttentionModel/2018-03-31 | get_logger | INFO]: ====================AttentionModel/2018-03-31 start====================
making sample. (inference: True, date:2018-04-30 00:00:00, date_number: 376)
making sample. (inference: False, date:2018-03-31 00:00:00, date_number: 375)
[AttentionModel/2018-04-30 | get_logger | INFO]: ====================AttentionModel/2018-04-30 start====================
[strategy_integration.py] set_seed(2018-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5736)training: 2/10 (0.5736)validation : 2/10 (0.5681)training: 3/10 (0.5681)validation : 3/10 (0.5681)training: 4/10 (0.5681)validation : 4/10 (0.5662)training: 5/10 (0.5662)validation : 5/10 (0.5662)training: 6/10 (0.5662)validation : 6/10 (0.5662)training: 7/10 (0.5662)validation : 7/10 (0.5662)early stopping at 7 with loss 0.5662
AttentionModel-training is done: 7/10
2018-03-31 | reset count: 0 | final loss: 0.5662 at epoch 4
[strategy_integration.py] set_seed(2018-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5693)training: 2/10 (0.5693)validation : 2/10 (0.5693)training: 3/10 (0.5693)validation : 3/10 (0.5662)training: 4/10 (0.5662)validation : 4/10 (0.5662)training: 5/10 (0.5662)validation : 5/10 (0.5662)training: 6/10 (0.5662)validation : 6/10 (0.5662)early stopping at 6 with loss 0.5662
AttentionModel-training is done: 6/10
2018-04-30 | reset count: 0 | final loss: 0.5662 at epoch 3
making sample. (inference: True, date:2018-05-31 00:00:00, date_number: 377)
making sample. (inference: False, date:2018-04-30 00:00:00, date_number: 376)
[AttentionModel/2018-05-31 | get_logger | INFO]: ====================AttentionModel/2018-05-31 start====================
making sample. (inference: True, date:2018-06-30 00:00:00, date_number: 378)
making sample. (inference: False, date:2018-05-31 00:00:00, date_number: 377)
[AttentionModel/2018-06-30 | get_logger | INFO]: ====================AttentionModel/2018-06-30 start====================
[strategy_integration.py] set_seed(2018-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5723)training: 2/10 (0.5723)validation : 2/10 (0.5704)training: 3/10 (0.5704)validation : 3/10 (0.5704)training: 4/10 (0.5704)validation : 4/10 (0.5674)training: 5/10 (0.5674)validation : 5/10 (0.5674)training: 6/10 (0.5674)validation : 6/10 (0.5674)training: 7/10 (0.5674)validation : 7/10 (0.5674)early stopping at 7 with loss 0.5674
AttentionModel-training is done: 7/10
2018-05-31 | reset count: 0 | final loss: 0.5674 at epoch 4
making sample. (inference: True, date:2018-07-31 00:00:00, date_number: 379)
making sample. (inference: False, date:2018-06-30 00:00:00, date_number: 378)
[AttentionModel/2018-07-31 | get_logger | INFO]: ====================AttentionModel/2018-07-31 start====================
[strategy_integration.py] set_seed(2018-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5667)training: 2/10 (0.5667)validation : 2/10 (0.5634)training: 3/10 (0.5634)validation : 3/10 (0.5634)training: 4/10 (0.5634)validation : 4/10 (0.5634)training: 5/10 (0.5634)validation : 5/10 (0.5599)training: 6/10 (0.5599)validation : 6/10 (0.5599)training: 7/10 (0.5599)validation : 7/10 (0.5599)training: 8/10 (0.5599)validation : 8/10 (0.5599)early stopping at 8 with loss 0.5599
AttentionModel-training is done: 8/10
2018-06-30 | reset count: 0 | final loss: 0.5599 at epoch 5
making sample. (inference: True, date:2018-08-31 00:00:00, date_number: 380)
making sample. (inference: False, date:2018-07-31 00:00:00, date_number: 379)
[AttentionModel/2018-08-31 | get_logger | INFO]: ====================AttentionModel/2018-08-31 start====================
[strategy_integration.py] set_seed(2018-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5611)training: 2/10 (0.5611)validation : 2/10 (0.5611)training: 3/10 (0.5611)validation : 3/10 (0.5611)training: 4/10 (0.5611)validation : 4/10 (0.5611)training: 5/10 (0.5611)validation : 5/10 (0.5611)training: 6/10 (0.5611)validation : 6/10 (0.5611)early stopping at 6 with loss 0.5611
AttentionModel-training is done: 6/10
2018-07-31 | reset count: 0 | final loss: 0.5611 at epoch 1
making sample. (inference: True, date:2018-09-30 00:00:00, date_number: 381)
making sample. (inference: False, date:2018-08-31 00:00:00, date_number: 380)
[AttentionModel/2018-09-30 | get_logger | INFO]: ====================AttentionModel/2018-09-30 start====================
[strategy_integration.py] set_seed(2018-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5674)training: 2/10 (0.5674)validation : 2/10 (0.5674)training: 3/10 (0.5674)validation : 3/10 (0.5625)training: 4/10 (0.5625)validation : 4/10 (0.5625)training: 5/10 (0.5625)validation : 5/10 (0.5625)training: 6/10 (0.5625)validation : 6/10 (0.5582)training: 7/10 (0.5582)validation : 7/10 (0.5582)training: 8/10 (0.5582)validation : 8/10 (0.5582)early stopping at 8 with loss 0.5582
AttentionModel-training is done: 8/10
2018-08-31 | reset count: 0 | final loss: 0.5582 at epoch 6
[strategy_integration.py] set_seed(2018-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5790)training: 2/10 (0.5790)validation : 2/10 (0.5790)training: 3/10 (0.5790)validation : 3/10 (0.5769)training: 4/10 (0.5769)validation : 4/10 (0.5769)training: 5/10 (0.5769)validation : 5/10 (0.5769)training: 6/10 (0.5769)validation : 6/10 (0.5769)early stopping at 6 with loss 0.5769
AttentionModel-training is done: 6/10
2018-09-30 | reset count: 0 | final loss: 0.5769 at epoch 3
making sample. (inference: True, date:2018-10-31 00:00:00, date_number: 382)
making sample. (inference: False, date:2018-09-30 00:00:00, date_number: 381)
[AttentionModel/2018-10-31 | get_logger | INFO]: ====================AttentionModel/2018-10-31 start====================
making sample. (inference: True, date:2018-11-30 00:00:00, date_number: 383)
making sample. (inference: False, date:2018-10-31 00:00:00, date_number: 382)
[AttentionModel/2018-11-30 | get_logger | INFO]: ====================AttentionModel/2018-11-30 start====================
[strategy_integration.py] set_seed(2018-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5798)training: 2/10 (0.5798)validation : 2/10 (0.5763)training: 3/10 (0.5763)validation : 3/10 (0.5737)training: 4/10 (0.5737)validation : 4/10 (0.5737)training: 5/10 (0.5737)validation : 5/10 (0.5737)training: 6/10 (0.5737)validation : 6/10 (0.5723)training: 7/10 (0.5723)validation : 7/10 (0.5723)early stopping at 7 with loss 0.5723
AttentionModel-training is done: 7/10
2018-10-31 | reset count: 0 | final loss: 0.5723 at epoch 6
making sample. (inference: True, date:2018-12-31 00:00:00, date_number: 384)
making sample. (inference: False, date:2018-11-30 00:00:00, date_number: 383)
[AttentionModel/2018-12-31 | get_logger | INFO]: ====================AttentionModel/2018-12-31 start====================
[strategy_integration.py] set_seed(2018-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5885)training: 2/10 (0.5885)validation : 2/10 (0.5866)training: 3/10 (0.5866)validation : 3/10 (0.5866)training: 4/10 (0.5866)validation : 4/10 (0.5817)training: 5/10 (0.5817)validation : 5/10 (0.5817)training: 6/10 (0.5817)validation : 6/10 (0.5817)training: 7/10 (0.5817)validation : 7/10 (0.5817)early stopping at 7 with loss 0.5817
AttentionModel-training is done: 7/10
2018-11-30 | reset count: 0 | final loss: 0.5817 at epoch 4
[strategy_integration.py] set_seed(2018-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5818)training: 2/10 (0.5818)validation : 2/10 (0.5775)training: 3/10 (0.5775)validation : 3/10 (0.5751)training: 4/10 (0.5751)validation : 4/10 (0.5751)training: 5/10 (0.5751)validation : 5/10 (0.5751)early stopping at 5 with loss 0.5751
AttentionModel-training is done: 5/10
2018-12-31 | reset count: 0 | final loss: 0.5751 at epoch 3
making sample. (inference: True, date:2019-01-31 00:00:00, date_number: 385)
making sample. (inference: False, date:2018-12-31 00:00:00, date_number: 384)
[AttentionModel/2019-01-31 | get_logger | INFO]: ====================AttentionModel/2019-01-31 start====================
making sample. (inference: True, date:2019-02-28 00:00:00, date_number: 386)
making sample. (inference: False, date:2019-01-31 00:00:00, date_number: 385)
[AttentionModel/2019-02-28 | get_logger | INFO]: ====================AttentionModel/2019-02-28 start====================
[strategy_integration.py] set_seed(2019-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5912)training: 2/10 (0.5912)validation : 2/10 (0.5856)training: 3/10 (0.5856)validation : 3/10 (0.5847)training: 4/10 (0.5847)validation : 4/10 (0.5847)training: 5/10 (0.5847)validation : 5/10 (0.5847)training: 6/10 (0.5847)validation : 6/10 (0.5847)early stopping at 6 with loss 0.5847
AttentionModel-training is done: 6/10
2019-01-31 | reset count: 0 | final loss: 0.5847 at epoch 3
[strategy_integration.py] set_seed(2019-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5931)training: 2/10 (0.5931)validation : 2/10 (0.5876)training: 3/10 (0.5876)validation : 3/10 (0.5862)training: 4/10 (0.5862)validation : 4/10 (0.5862)training: 5/10 (0.5862)validation : 5/10 (0.5862)training: 6/10 (0.5862)validation : 6/10 (0.5862)early stopping at 6 with loss 0.5862
AttentionModel-training is done: 6/10
2019-02-28 | reset count: 0 | final loss: 0.5862 at epoch 3
making sample. (inference: True, date:2019-03-31 00:00:00, date_number: 387)
making sample. (inference: False, date:2019-02-28 00:00:00, date_number: 386)
[AttentionModel/2019-03-31 | get_logger | INFO]: ====================AttentionModel/2019-03-31 start====================
making sample. (inference: True, date:2019-04-30 00:00:00, date_number: 388)
making sample. (inference: False, date:2019-03-31 00:00:00, date_number: 387)
[AttentionModel/2019-04-30 | get_logger | INFO]: ====================AttentionModel/2019-04-30 start====================
[strategy_integration.py] set_seed(2019-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5914)training: 2/10 (0.5914)validation : 2/10 (0.5819)training: 3/10 (0.5819)validation : 3/10 (0.5819)training: 4/10 (0.5819)validation : 4/10 (0.5819)training: 5/10 (0.5819)validation : 5/10 (0.5819)training: 6/10 (0.5819)validation : 6/10 (0.5819)early stopping at 6 with loss 0.5819
AttentionModel-training is done: 6/10
2019-03-31 | reset count: 0 | final loss: 0.5819 at epoch 2
making sample. (inference: True, date:2019-05-31 00:00:00, date_number: 389)
making sample. (inference: False, date:2019-04-30 00:00:00, date_number: 388)
[AttentionModel/2019-05-31 | get_logger | INFO]: ====================AttentionModel/2019-05-31 start====================
[strategy_integration.py] set_seed(2019-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5895)training: 2/10 (0.5895)validation : 2/10 (0.5832)training: 3/10 (0.5832)validation : 3/10 (0.5832)training: 4/10 (0.5832)validation : 4/10 (0.5832)training: 5/10 (0.5832)validation : 5/10 (0.5832)training: 6/10 (0.5832)validation : 6/10 (0.5832)early stopping at 6 with loss 0.5832
AttentionModel-training is done: 6/10
2019-04-30 | reset count: 0 | final loss: 0.5832 at epoch 2
making sample. (inference: True, date:2019-06-30 00:00:00, date_number: 390)
making sample. (inference: False, date:2019-05-31 00:00:00, date_number: 389)
[AttentionModel/2019-06-30 | get_logger | INFO]: ====================AttentionModel/2019-06-30 start====================
[strategy_integration.py] set_seed(2019-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5808)training: 2/10 (0.5808)validation : 2/10 (0.5780)training: 3/10 (0.5780)validation : 3/10 (0.5780)training: 4/10 (0.5780)validation : 4/10 (0.5780)training: 5/10 (0.5780)validation : 5/10 (0.5779)training: 6/10 (0.5779)validation : 6/10 (0.5759)training: 7/10 (0.5759)validation : 7/10 (0.5759)early stopping at 7 with loss 0.5759
AttentionModel-training is done: 7/10
2019-05-31 | reset count: 0 | final loss: 0.5759 at epoch 6
[strategy_integration.py] set_seed(2019-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5873)training: 2/10 (0.5873)validation : 2/10 (0.5822)training: 3/10 (0.5822)validation : 3/10 (0.5812)training: 4/10 (0.5812)validation : 4/10 (0.5788)training: 5/10 (0.5788)validation : 5/10 (0.5788)training: 6/10 (0.5788)validation : 6/10 (0.5788)early stopping at 6 with loss 0.5788
AttentionModel-training is done: 6/10
2019-06-30 | reset count: 0 | final loss: 0.5788 at epoch 4
making sample. (inference: True, date:2019-07-31 00:00:00, date_number: 391)
making sample. (inference: False, date:2019-06-30 00:00:00, date_number: 390)
[AttentionModel/2019-07-31 | get_logger | INFO]: ====================AttentionModel/2019-07-31 start====================
making sample. (inference: True, date:2019-08-31 00:00:00, date_number: 392)
making sample. (inference: False, date:2019-07-31 00:00:00, date_number: 391)
[AttentionModel/2019-08-31 | get_logger | INFO]: ====================AttentionModel/2019-08-31 start====================
[strategy_integration.py] set_seed(2019-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5819)training: 2/10 (0.5819)validation : 2/10 (0.5819)training: 3/10 (0.5819)validation : 3/10 (0.5802)training: 4/10 (0.5802)validation : 4/10 (0.5802)training: 5/10 (0.5802)validation : 5/10 (0.5802)early stopping at 5 with loss 0.5802
AttentionModel-training is done: 5/10
2019-08-31 | reset count: 0 | final loss: 0.5802 at epoch 3
[strategy_integration.py] set_seed(2019-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5803)training: 2/10 (0.5803)validation : 2/10 (0.5803)training: 3/10 (0.5803)validation : 3/10 (0.5803)training: 4/10 (0.5803)validation : 4/10 (0.5792)training: 5/10 (0.5792)validation : 5/10 (0.5792)training: 6/10 (0.5792)validation : 6/10 (0.5792)training: 7/10 (0.5792)validation : 7/10 (0.5784)training: 8/10 (0.5784)validation : 8/10 (0.5783)training: 9/10 (0.5783)validation : 9/10 (0.5769)training: 10/10 (0.5769)validation : 10/10 (0.5769)early stopping at 10 with loss 0.5769
AttentionModel-training is done: 10/10
2019-07-31 | reset count: 0 | final loss: 0.5769 at epoch 9
making sample. (inference: True, date:2019-09-30 00:00:00, date_number: 393)
making sample. (inference: False, date:2019-08-31 00:00:00, date_number: 392)
[AttentionModel/2019-09-30 | get_logger | INFO]: ====================AttentionModel/2019-09-30 start====================
making sample. (inference: True, date:2019-10-31 00:00:00, date_number: 394)
making sample. (inference: False, date:2019-09-30 00:00:00, date_number: 393)
[AttentionModel/2019-10-31 | get_logger | INFO]: ====================AttentionModel/2019-10-31 start====================
[strategy_integration.py] set_seed(2019-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5952)training: 2/10 (0.5952)validation : 2/10 (0.5924)training: 3/10 (0.5924)validation : 3/10 (0.5898)training: 4/10 (0.5898)validation : 4/10 (0.5864)training: 5/10 (0.5864)validation : 5/10 (0.5864)training: 6/10 (0.5864)validation : 6/10 (0.5864)early stopping at 6 with loss 0.5864
AttentionModel-training is done: 6/10
2019-09-30 | reset count: 0 | final loss: 0.5864 at epoch 4
[strategy_integration.py] set_seed(2019-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5920)training: 2/10 (0.5920)validation : 2/10 (0.5901)training: 3/10 (0.5901)validation : 3/10 (0.5901)training: 4/10 (0.5901)validation : 4/10 (0.5901)training: 5/10 (0.5901)validation : 5/10 (0.5901)early stopping at 5 with loss 0.5901
AttentionModel-training is done: 5/10
2019-10-31 | reset count: 0 | final loss: 0.5901 at epoch 2
making sample. (inference: True, date:2019-11-30 00:00:00, date_number: 395)
making sample. (inference: False, date:2019-10-31 00:00:00, date_number: 394)
[AttentionModel/2019-11-30 | get_logger | INFO]: ====================AttentionModel/2019-11-30 start====================
making sample. (inference: True, date:2019-12-31 00:00:00, date_number: 396)
making sample. (inference: False, date:2019-11-30 00:00:00, date_number: 395)
[AttentionModel/2019-12-31 | get_logger | INFO]: ====================AttentionModel/2019-12-31 start====================
[strategy_integration.py] set_seed(2019-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5813)training: 2/10 (0.5813)validation : 2/10 (0.5813)training: 3/10 (0.5813)validation : 3/10 (0.5813)training: 4/10 (0.5813)validation : 4/10 (0.5813)training: 5/10 (0.5813)validation : 5/10 (0.5796)training: 6/10 (0.5796)validation : 6/10 (0.5796)training: 7/10 (0.5796)validation : 7/10 (0.5796)early stopping at 7 with loss 0.5796
AttentionModel-training is done: 7/10
2019-11-30 | reset count: 0 | final loss: 0.5796 at epoch 5
making sample. (inference: True, date:2020-01-31 00:00:00, date_number: 397)
making sample. (inference: False, date:2019-12-31 00:00:00, date_number: 396)
[strategy_integration.py] set_seed(2019-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5819)training: 2/10 (0.5819)validation : 2/10 (0.5813)training: 3/10 (0.5813)validation : 3/10 (0.5761)training: 4/10 (0.5761)validation : 4/10 (0.5761)training: 5/10 (0.5761)validation : 5/10 (0.5761)training: 6/10 (0.5761)validation : 6/10 (0.5756)training: 7/10 (0.5756)validation : 7/10 (0.5756)training: 8/10 (0.5756)validation : 8/10 (0.5756)early stopping at 8 with loss 0.5756
AttentionModel-training is done: 8/10
2019-12-31 | reset count: 0 | final loss: 0.5756 at epoch 6
[AttentionModel/2020-01-31 | get_logger | INFO]: ====================AttentionModel/2020-01-31 start====================
making sample. (inference: True, date:2020-02-29 00:00:00, date_number: 398)
making sample. (inference: False, date:2020-01-31 00:00:00, date_number: 397)
[AttentionModel/2020-02-29 | get_logger | INFO]: ====================AttentionModel/2020-02-29 start====================
[strategy_integration.py] set_seed(2020-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5812)training: 2/10 (0.5812)validation : 2/10 (0.5777)training: 3/10 (0.5777)validation : 3/10 (0.5777)training: 4/10 (0.5777)validation : 4/10 (0.5777)training: 5/10 (0.5777)validation : 5/10 (0.5777)early stopping at 5 with loss 0.5777
AttentionModel-training is done: 5/10
2020-01-31 | reset count: 0 | final loss: 0.5777 at epoch 2
making sample. (inference: True, date:2020-03-31 00:00:00, date_number: 399)
making sample. (inference: False, date:2020-02-29 00:00:00, date_number: 398)
[strategy_integration.py] set_seed(2020-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5920)training: 2/10 (0.5920)validation : 2/10 (0.5871)training: 3/10 (0.5871)validation : 3/10 (0.5870)training: 4/10 (0.5870)validation : 4/10 (0.5870)training: 5/10 (0.5870)validation : 5/10 (0.5870)early stopping at 5 with loss 0.5870
AttentionModel-training is done: 5/10
2020-02-29 | reset count: 0 | final loss: 0.5870 at epoch 3
[AttentionModel/2020-03-31 | get_logger | INFO]: ====================AttentionModel/2020-03-31 start====================
making sample. (inference: True, date:2020-04-30 00:00:00, date_number: 400)
making sample. (inference: False, date:2020-03-31 00:00:00, date_number: 399)
[AttentionModel/2020-04-30 | get_logger | INFO]: ====================AttentionModel/2020-04-30 start====================
[strategy_integration.py] set_seed(2020-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5735)training: 2/10 (0.5735)validation : 2/10 (0.5731)training: 3/10 (0.5731)validation : 3/10 (0.5731)training: 4/10 (0.5731)validation : 4/10 (0.5731)training: 5/10 (0.5731)validation : 5/10 (0.5731)early stopping at 5 with loss 0.5731
AttentionModel-training is done: 5/10
2020-03-31 | reset count: 0 | final loss: 0.5731 at epoch 2
making sample. (inference: True, date:2020-05-31 00:00:00, date_number: 401)
making sample. (inference: False, date:2020-04-30 00:00:00, date_number: 400)
[AttentionModel/2020-05-31 | get_logger | INFO]: ====================AttentionModel/2020-05-31 start====================
[strategy_integration.py] set_seed(2020-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5821)training: 2/10 (0.5821)validation : 2/10 (0.5791)training: 3/10 (0.5791)validation : 3/10 (0.5767)training: 4/10 (0.5767)validation : 4/10 (0.5767)training: 5/10 (0.5767)validation : 5/10 (0.5767)training: 6/10 (0.5767)validation : 6/10 (0.5767)early stopping at 6 with loss 0.5767
AttentionModel-training is done: 6/10
2020-04-30 | reset count: 0 | final loss: 0.5767 at epoch 3
making sample. (inference: True, date:2020-06-30 00:00:00, date_number: 402)
making sample. (inference: False, date:2020-05-31 00:00:00, date_number: 401)
[strategy_integration.py] set_seed(2020-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5783)training: 2/10 (0.5783)validation : 2/10 (0.5753)training: 3/10 (0.5753)validation : 3/10 (0.5753)training: 4/10 (0.5753)validation : 4/10 (0.5753)training: 5/10 (0.5753)validation : 5/10 (0.5752)training: 6/10 (0.5752)validation : 6/10 (0.5752)early stopping at 6 with loss 0.5752
AttentionModel-training is done: 6/10
2020-05-31 | reset count: 0 | final loss: 0.5752 at epoch 5
[AttentionModel/2020-06-30 | get_logger | INFO]: ====================AttentionModel/2020-06-30 start====================
making sample. (inference: True, date:2020-07-31 00:00:00, date_number: 403)
making sample. (inference: False, date:2020-06-30 00:00:00, date_number: 402)
[AttentionModel/2020-07-31 | get_logger | INFO]: ====================AttentionModel/2020-07-31 start====================
[strategy_integration.py] set_seed(2020-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5889)training: 2/10 (0.5889)validation : 2/10 (0.5841)training: 3/10 (0.5841)validation : 3/10 (0.5825)training: 4/10 (0.5825)validation : 4/10 (0.5809)training: 5/10 (0.5809)validation : 5/10 (0.5809)training: 6/10 (0.5809)validation : 6/10 (0.5777)training: 7/10 (0.5777)validation : 7/10 (0.5777)early stopping at 7 with loss 0.5777
AttentionModel-training is done: 7/10
2020-06-30 | reset count: 0 | final loss: 0.5777 at epoch 6
[strategy_integration.py] set_seed(2020-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5788)training: 2/10 (0.5788)validation : 2/10 (0.5788)training: 3/10 (0.5788)validation : 3/10 (0.5755)training: 4/10 (0.5755)validation : 4/10 (0.5752)training: 5/10 (0.5752)validation : 5/10 (0.5752)training: 6/10 (0.5752)validation : 6/10 (0.5752)early stopping at 6 with loss 0.5752
AttentionModel-training is done: 6/10
2020-07-31 | reset count: 0 | final loss: 0.5752 at epoch 4
making sample. (inference: True, date:2020-08-31 00:00:00, date_number: 404)
making sample. (inference: False, date:2020-07-31 00:00:00, date_number: 403)
[AttentionModel/2020-08-31 | get_logger | INFO]: ====================AttentionModel/2020-08-31 start====================
making sample. (inference: True, date:2020-09-30 00:00:00, date_number: 405)
making sample. (inference: False, date:2020-08-31 00:00:00, date_number: 404)
[AttentionModel/2020-09-30 | get_logger | INFO]: ====================AttentionModel/2020-09-30 start====================
[strategy_integration.py] set_seed(2020-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5661)training: 2/10 (0.5661)validation : 2/10 (0.5651)training: 3/10 (0.5651)validation : 3/10 (0.5632)training: 4/10 (0.5632)validation : 4/10 (0.5632)training: 5/10 (0.5632)validation : 5/10 (0.5632)training: 6/10 (0.5632)validation : 6/10 (0.5632)early stopping at 6 with loss 0.5632
AttentionModel-training is done: 6/10
2020-08-31 | reset count: 0 | final loss: 0.5632 at epoch 3
making sample. (inference: True, date:2020-10-31 00:00:00, date_number: 406)
making sample. (inference: False, date:2020-09-30 00:00:00, date_number: 405)
[AttentionModel/2020-10-31 | get_logger | INFO]: ====================AttentionModel/2020-10-31 start====================
[strategy_integration.py] set_seed(2020-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5701)training: 2/10 (0.5701)validation : 2/10 (0.5699)training: 3/10 (0.5699)validation : 3/10 (0.5699)training: 4/10 (0.5699)validation : 4/10 (0.5699)training: 5/10 (0.5699)validation : 5/10 (0.5652)training: 6/10 (0.5652)validation : 6/10 (0.5652)training: 7/10 (0.5652)validation : 7/10 (0.5652)early stopping at 7 with loss 0.5652
AttentionModel-training is done: 7/10
2020-09-30 | reset count: 0 | final loss: 0.5652 at epoch 5
making sample. (inference: True, date:2020-11-30 00:00:00, date_number: 407)
making sample. (inference: False, date:2020-10-31 00:00:00, date_number: 406)
[AttentionModel/2020-11-30 | get_logger | INFO]: ====================AttentionModel/2020-11-30 start====================
[strategy_integration.py] set_seed(2020-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5770)training: 2/10 (0.5770)validation : 2/10 (0.5700)training: 3/10 (0.5700)validation : 3/10 (0.5700)training: 4/10 (0.5700)validation : 4/10 (0.5700)training: 5/10 (0.5700)validation : 5/10 (0.5700)training: 6/10 (0.5700)validation : 6/10 (0.5700)early stopping at 6 with loss 0.5700
AttentionModel-training is done: 6/10
2020-10-31 | reset count: 0 | final loss: 0.5700 at epoch 2
making sample. (inference: True, date:2020-12-31 00:00:00, date_number: 408)
making sample. (inference: False, date:2020-11-30 00:00:00, date_number: 407)
[AttentionModel/2020-12-31 | get_logger | INFO]: ====================AttentionModel/2020-12-31 start====================
[strategy_integration.py] set_seed(2020-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5729)training: 2/10 (0.5729)validation : 2/10 (0.5729)training: 3/10 (0.5729)validation : 3/10 (0.5668)training: 4/10 (0.5668)validation : 4/10 (0.5668)training: 5/10 (0.5668)validation : 5/10 (0.5668)training: 6/10 (0.5668)validation : 6/10 (0.5668)early stopping at 6 with loss 0.5668
AttentionModel-training is done: 6/10
2020-11-30 | reset count: 0 | final loss: 0.5668 at epoch 3
[strategy_integration.py] set_seed(2020-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5697)training: 2/10 (0.5697)validation : 2/10 (0.5697)training: 3/10 (0.5697)validation : 3/10 (0.5686)training: 4/10 (0.5686)validation : 4/10 (0.5686)training: 5/10 (0.5686)validation : 5/10 (0.5686)early stopping at 5 with loss 0.5686
AttentionModel-training is done: 5/10
2020-12-31 | reset count: 0 | final loss: 0.5686 at epoch 3
making sample. (inference: True, date:2021-01-31 00:00:00, date_number: 409)
making sample. (inference: False, date:2020-12-31 00:00:00, date_number: 408)
[AttentionModel/2021-01-31 | get_logger | INFO]: ====================AttentionModel/2021-01-31 start====================
making sample. (inference: True, date:2021-02-28 00:00:00, date_number: 410)
making sample. (inference: False, date:2021-01-31 00:00:00, date_number: 409)
[AttentionModel/2021-02-28 | get_logger | INFO]: ====================AttentionModel/2021-02-28 start====================
[strategy_integration.py] set_seed(2021-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5535)training: 2/10 (0.5535)validation : 2/10 (0.5532)training: 3/10 (0.5532)validation : 3/10 (0.5491)training: 4/10 (0.5491)validation : 4/10 (0.5484)training: 5/10 (0.5484)validation : 5/10 (0.5484)training: 6/10 (0.5484)validation : 6/10 (0.5482)training: 7/10 (0.5482)validation : 7/10 (0.5482)early stopping at 7 with loss 0.5482
AttentionModel-training is done: 7/10
2021-01-31 | reset count: 0 | final loss: 0.5482 at epoch 6
making sample. (inference: True, date:2021-03-31 00:00:00, date_number: 411)
making sample. (inference: False, date:2021-02-28 00:00:00, date_number: 410)
[strategy_integration.py] set_seed(2021-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5722)training: 2/10 (0.5722)validation : 2/10 (0.5703)training: 3/10 (0.5703)validation : 3/10 (0.5703)training: 4/10 (0.5703)validation : 4/10 (0.5703)training: 5/10 (0.5703)validation : 5/10 (0.5703)training: 6/10 (0.5703)validation : 6/10 (0.5701)training: 7/10 (0.5701)validation : 7/10 (0.5697)training: 8/10 (0.5697)validation : 8/10 (0.5697)early stopping at 8 with loss 0.5697
AttentionModel-training is done: 8/10
2021-02-28 | reset count: 0 | final loss: 0.5697 at epoch 7
[AttentionModel/2021-03-31 | get_logger | INFO]: ====================AttentionModel/2021-03-31 start====================
making sample. (inference: True, date:2021-04-30 00:00:00, date_number: 412)
making sample. (inference: False, date:2021-03-31 00:00:00, date_number: 411)
[AttentionModel/2021-04-30 | get_logger | INFO]: ====================AttentionModel/2021-04-30 start====================
[strategy_integration.py] set_seed(2021-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5672)training: 2/10 (0.5672)validation : 2/10 (0.5672)training: 3/10 (0.5672)validation : 3/10 (0.5652)training: 4/10 (0.5652)validation : 4/10 (0.5652)training: 5/10 (0.5652)validation : 5/10 (0.5652)training: 6/10 (0.5652)validation : 6/10 (0.5652)early stopping at 6 with loss 0.5652
AttentionModel-training is done: 6/10
2021-03-31 | reset count: 0 | final loss: 0.5652 at epoch 3
[strategy_integration.py] set_seed(2021-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5639)training: 2/10 (0.5639)validation : 2/10 (0.5639)training: 3/10 (0.5639)validation : 3/10 (0.5609)training: 4/10 (0.5609)validation : 4/10 (0.5609)training: 5/10 (0.5609)validation : 5/10 (0.5570)training: 6/10 (0.5570)validation : 6/10 (0.5570)training: 7/10 (0.5570)validation : 7/10 (0.5570)early stopping at 7 with loss 0.5570
AttentionModel-training is done: 7/10
2021-04-30 | reset count: 0 | final loss: 0.5570 at epoch 5
making sample. (inference: True, date:2021-05-31 00:00:00, date_number: 413)
making sample. (inference: False, date:2021-04-30 00:00:00, date_number: 412)
[AttentionModel/2021-05-31 | get_logger | INFO]: ====================AttentionModel/2021-05-31 start====================
making sample. (inference: True, date:2021-06-30 00:00:00, date_number: 414)
making sample. (inference: False, date:2021-05-31 00:00:00, date_number: 413)
[AttentionModel/2021-06-30 | get_logger | INFO]: ====================AttentionModel/2021-06-30 start====================
[strategy_integration.py] set_seed(2021-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5624)training: 2/10 (0.5624)validation : 2/10 (0.5624)training: 3/10 (0.5624)validation : 3/10 (0.5624)training: 4/10 (0.5624)validation : 4/10 (0.5624)training: 5/10 (0.5624)validation : 5/10 (0.5624)early stopping at 5 with loss 0.5624
AttentionModel-training is done: 5/10
2021-05-31 | reset count: 0 | final loss: 0.5624 at epoch 1
making sample. (inference: True, date:2021-07-31 00:00:00, date_number: 415)
making sample. (inference: False, date:2021-06-30 00:00:00, date_number: 414)
[AttentionModel/2021-07-31 | get_logger | INFO]: ====================AttentionModel/2021-07-31 start====================
[strategy_integration.py] set_seed(2021-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5738)training: 2/10 (0.5738)validation : 2/10 (0.5735)training: 3/10 (0.5735)validation : 3/10 (0.5711)training: 4/10 (0.5711)validation : 4/10 (0.5711)training: 5/10 (0.5711)validation : 5/10 (0.5698)training: 6/10 (0.5698)validation : 6/10 (0.5698)early stopping at 6 with loss 0.5698
AttentionModel-training is done: 6/10
2021-06-30 | reset count: 0 | final loss: 0.5698 at epoch 5
[strategy_integration.py] set_seed(2021-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5755)training: 2/10 (0.5755)validation : 2/10 (0.5700)training: 3/10 (0.5700)validation : 3/10 (0.5700)training: 4/10 (0.5700)validation : 4/10 (0.5700)training: 5/10 (0.5700)validation : 5/10 (0.5700)early stopping at 5 with loss 0.5700
AttentionModel-training is done: 5/10
2021-07-31 | reset count: 0 | final loss: 0.5700 at epoch 2
making sample. (inference: True, date:2021-08-31 00:00:00, date_number: 416)
making sample. (inference: False, date:2021-07-31 00:00:00, date_number: 415)
[AttentionModel/2021-08-31 | get_logger | INFO]: ====================AttentionModel/2021-08-31 start====================
making sample. (inference: True, date:2021-09-30 00:00:00, date_number: 417)
making sample. (inference: False, date:2021-08-31 00:00:00, date_number: 416)
[AttentionModel/2021-09-30 | get_logger | INFO]: ====================AttentionModel/2021-09-30 start====================
[strategy_integration.py] set_seed(2021-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5763)training: 2/10 (0.5763)validation : 2/10 (0.5752)training: 3/10 (0.5752)validation : 3/10 (0.5752)training: 4/10 (0.5752)validation : 4/10 (0.5729)training: 5/10 (0.5729)validation : 5/10 (0.5725)training: 6/10 (0.5725)validation : 6/10 (0.5725)training: 7/10 (0.5725)validation : 7/10 (0.5718)training: 8/10 (0.5718)validation : 8/10 (0.5718)early stopping at 8 with loss 0.5718
AttentionModel-training is done: 8/10
2021-08-31 | reset count: 0 | final loss: 0.5718 at epoch 7
[strategy_integration.py] set_seed(2021-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5739)training: 2/10 (0.5739)validation : 2/10 (0.5729)training: 3/10 (0.5729)validation : 3/10 (0.5729)training: 4/10 (0.5729)validation : 4/10 (0.5729)training: 5/10 (0.5729)validation : 5/10 (0.5729)early stopping at 5 with loss 0.5729
AttentionModel-training is done: 5/10
2021-09-30 | reset count: 0 | final loss: 0.5729 at epoch 2
making sample. (inference: True, date:2021-10-31 00:00:00, date_number: 418)
making sample. (inference: False, date:2021-09-30 00:00:00, date_number: 417)
[AttentionModel/2021-10-31 | get_logger | INFO]: ====================AttentionModel/2021-10-31 start====================
[strategy_integration.py] set_seed(2021-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5885)training: 2/10 (0.5885)validation : 2/10 (0.5858)training: 3/10 (0.5858)validation : 3/10 (0.5837)training: 4/10 (0.5837)validation : 4/10 (0.5804)training: 5/10 (0.5804)validation : 5/10 (0.5804)training: 6/10 (0.5804)validation : 6/10 (0.5804)early stopping at 6 with loss 0.5804
AttentionModel-training is done: 6/10
2021-10-31 | reset count: 0 | final loss: 0.5804 at epoch 4
[strategy | get_logger | INFO]: ====================strategy start====================
[strategy | _get_infer_dirs | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick88_test02/infer 이 strategy 대상에 추가됨
[strategy | _backtest | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick88_test02/infer is starting
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.149 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading masking data⠦ [Compustat API] : Loading masking data⠦ [Compustat API] : Loading get_historic_universe from cache✓ [Compustat API] : Loading get_historic_universe from cache done in 1.941 secs
⠧ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.243 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.063 secs
⠋ [Compustat API] : Loading get_monthly_total_return from cache✓ [Compustat API] : Loading get_monthly_total_return from cache done in 0.062 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.076 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.072 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.07 secs
⠋ [Compustat API] : Loading get_monthly_volume_data from cache✓ [Compustat API] : Loading get_monthly_volume_data from cache done in 0.08 secs
[strategy | _allocating | INFO]: [STRATEGY] allocating start
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_weight_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] performance.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] combined.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] weight.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] universe.csv is saved.
[comparison | get_logger | INFO]: ====================comparison start====================
[comparison | compare | INFO]: [5. COMPARISON] comparison is saved.
[comparison | compare | INFO]:                        Harvest_Green_concept001_pick88_test02_price_return  ...  SPCOMP_total_win
2016-01-31_2021-04-30                                           0.238954    ...          0.765625

[1 rows x 27 columns]
2015-12-31
2016-01-31
2016-02-29
2016-03-31
2016-04-30
2016-05-31
2016-06-30
2016-07-31
2016-08-31
2016-09-30
2016-10-31
2016-11-30
2016-12-31
2017-01-31
2017-02-28
2017-03-31
2017-04-30
2017-05-31
2017-06-30
2017-07-31
2017-08-31
2017-09-30
2017-10-31
2017-11-30
2017-12-31
2018-01-31
2018-02-28
2018-03-31
2018-04-30
2018-05-31
2018-06-30
2018-07-31
2018-08-31
2018-09-30
2018-10-31
2018-11-30
2018-12-31
2019-01-31
2019-02-28
2019-03-31
2019-04-30
2019-05-31
2019-06-30
2019-07-31
2019-08-31
2019-09-30
2019-10-31
2019-11-30
2019-12-31
2020-01-31
2020-02-29
2020-03-31
2020-04-30
2020-05-31
2020-06-30
2020-07-31
2020-08-31
2020-09-30
2020-10-31
2020-11-30
2020-12-31
2021-01-31
2021-02-28
2021-03-31
2021-04-30
2021-05-31
2021-06-30
2021-07-31
2021-08-31
2021-09-30
2021-10-31
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.051 secs
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.049 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.07 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.084 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.077 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.663 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.674 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.682 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.679 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.68 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.075 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.671 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.67 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.671 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.664 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.663 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.077 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.686 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.075 secs
⠋ [Compustat API] : Cleaning past days cache✓ [Compustat API] : Cleaning past days cache done in 0.0 secs
auto caching이 활성화 되었습니다. /home/sronly/sr-storage/kirin_cache/kirin_api_cache/2.19.17_2021-11-23에 캐쉬를 저장합니다.
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.147 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache✓ [Compustat API] : Loading get_historic_universe from cache done in 1.991 secs
⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.292 secs
setting tensorflow random seed failed
load_data: pr_1m_0m
load_data: mv
load_data: btm
load_data: snp500_pr
load_data: wilshire500_pr
load_data: ted
load_data: retail_mfr
load_data: m1
load_data: m2
load_data: real_pig
load_data: federal_tg
load_data: real_gdp
load_data: res_mom_12m_1m_0m
load_data: res_vol_6m_3m_0m
load_data: at
load_data: gpa
load_data: rev_surp
load_data: cash_at
load_data: aaa_t10y
load_data: baa_t10y
load_data: aaa_ff
load_data: op_lev
load_data: roe
load_data: std_u_e
load_data: ppi
load_data: trimmed_pce
load_data: unemploy
load_data: snp500_vol
load_data: ae_m
load_data: ia_ta
load_data: rc_a
load_data: t1y_ff
load_data: t5y_ff
load_data: t10y_t2y
load_data: export_growth
load_data: import_growth
load_data: real_gig
load_data: ret_noa
load_data: etm
load_data: ia_mv
load_data: mom_12m_1m
load_data: ram_12m_0m
load_data: vol_3m
load_data: t3y
load_data: t5y
load_data: t7y
load_data: baa_ff
load_data: core_cpi
load_data: core_pce
load_data: corporate_tg
load_data: industrial_prod
load_data: home_pr
load_data: r_s
load_data: r_a
load_data: fred_ff
load_data: t3m
load_data: t6m
load_data: t2y
load_data: t10y
load_data: aaa
load_data: baa
load_data: core_ppi
load_data: cpi
load_data: pce
load_data: wti
load_data: capa_util
load_data: snp500_pe
load_data: sector_values
load_data: mv
load_data: pr_1m_0m
load_data: mv
making sample. (inference: True, date:2015-12-31 00:00:00, date_number: 348)
making sample. (inference: False, date:2012-12-31 00:00:00, date_number: 312)
making sample. (inference: False, date:2013-01-31 00:00:00, date_number: 313)
making sample. (inference: False, date:2013-02-28 00:00:00, date_number: 314)
making sample. (inference: False, date:2013-03-31 00:00:00, date_number: 315)
making sample. (inference: False, date:2013-04-30 00:00:00, date_number: 316)
making sample. (inference: False, date:2013-05-31 00:00:00, date_number: 317)
making sample. (inference: False, date:2013-06-30 00:00:00, date_number: 318)
making sample. (inference: False, date:2013-07-31 00:00:00, date_number: 319)
making sample. (inference: False, date:2013-08-31 00:00:00, date_number: 320)
making sample. (inference: False, date:2013-09-30 00:00:00, date_number: 321)
making sample. (inference: False, date:2013-10-31 00:00:00, date_number: 322)
making sample. (inference: False, date:2013-11-30 00:00:00, date_number: 323)
making sample. (inference: False, date:2013-12-31 00:00:00, date_number: 324)
making sample. (inference: False, date:2014-01-31 00:00:00, date_number: 325)
making sample. (inference: False, date:2014-02-28 00:00:00, date_number: 326)
making sample. (inference: False, date:2014-03-31 00:00:00, date_number: 327)
making sample. (inference: False, date:2014-04-30 00:00:00, date_number: 328)
making sample. (inference: False, date:2014-05-31 00:00:00, date_number: 329)
making sample. (inference: False, date:2014-06-30 00:00:00, date_number: 330)
making sample. (inference: False, date:2014-07-31 00:00:00, date_number: 331)
making sample. (inference: False, date:2014-08-31 00:00:00, date_number: 332)
making sample. (inference: False, date:2014-09-30 00:00:00, date_number: 333)
making sample. (inference: False, date:2014-10-31 00:00:00, date_number: 334)
making sample. (inference: False, date:2014-11-30 00:00:00, date_number: 335)
making sample. (inference: False, date:2014-12-31 00:00:00, date_number: 336)
making sample. (inference: False, date:2015-01-31 00:00:00, date_number: 337)
making sample. (inference: False, date:2015-02-28 00:00:00, date_number: 338)
making sample. (inference: False, date:2015-03-31 00:00:00, date_number: 339)
making sample. (inference: False, date:2015-04-30 00:00:00, date_number: 340)
making sample. (inference: False, date:2015-05-31 00:00:00, date_number: 341)
making sample. (inference: False, date:2015-06-30 00:00:00, date_number: 342)
making sample. (inference: False, date:2015-07-31 00:00:00, date_number: 343)
making sample. (inference: False, date:2015-08-31 00:00:00, date_number: 344)
making sample. (inference: False, date:2015-09-30 00:00:00, date_number: 345)
making sample. (inference: False, date:2015-10-31 00:00:00, date_number: 346)
making sample. (inference: False, date:2015-11-30 00:00:00, date_number: 347)
[AttentionModel/2015-12-31 | get_logger | INFO]: ====================AttentionModel/2015-12-31 start====================
making sample. (inference: True, date:2016-01-31 00:00:00, date_number: 349)
making sample. (inference: False, date:2015-12-31 00:00:00, date_number: 348)
[AttentionModel/2016-01-31 | get_logger | INFO]: ====================AttentionModel/2016-01-31 start====================
[strategy_integration.py] set_seed(2015-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5735)training: 2/10 (0.5735)validation : 2/10 (0.5735)training: 3/10 (0.5735)validation : 3/10 (0.5735)training: 4/10 (0.5735)validation : 4/10 (0.5727)training: 5/10 (0.5727)validation : 5/10 (0.5727)training: 6/10 (0.5727)validation : 6/10 (0.5727)early stopping at 6 with loss 0.5727
AttentionModel-training is done: 6/10
2015-12-31 | reset count: 0 | final loss: 0.5727 at epoch 5
[strategy_integration.py] set_seed(2016-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5877)training: 2/10 (0.5877)validation : 2/10 (0.5855)training: 3/10 (0.5855)validation : 3/10 (0.5844)training: 4/10 (0.5844)validation : 4/10 (0.5844)training: 5/10 (0.5844)validation : 5/10 (0.5844)early stopping at 5 with loss 0.5844
AttentionModel-training is done: 5/10
2016-01-31 | reset count: 0 | final loss: 0.5844 at epoch 3
making sample. (inference: True, date:2016-02-29 00:00:00, date_number: 350)
making sample. (inference: False, date:2016-01-31 00:00:00, date_number: 349)
[AttentionModel/2016-02-29 | get_logger | INFO]: ====================AttentionModel/2016-02-29 start====================
making sample. (inference: True, date:2016-03-31 00:00:00, date_number: 351)
making sample. (inference: False, date:2016-02-29 00:00:00, date_number: 350)
[AttentionModel/2016-03-31 | get_logger | INFO]: ====================AttentionModel/2016-03-31 start====================
[strategy_integration.py] set_seed(2016-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5683)training: 2/10 (0.5683)validation : 2/10 (0.5683)training: 3/10 (0.5683)validation : 3/10 (0.5683)training: 4/10 (0.5683)validation : 4/10 (0.5683)training: 5/10 (0.5683)validation : 5/10 (0.5683)early stopping at 5 with loss 0.5683
AttentionModel-training is done: 5/10
2016-02-29 | reset count: 0 | final loss: 0.5683 at epoch 1
[strategy_integration.py] set_seed(2016-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5682)training: 2/10 (0.5682)validation : 2/10 (0.5671)training: 3/10 (0.5671)validation : 3/10 (0.5641)training: 4/10 (0.5641)validation : 4/10 (0.5641)training: 5/10 (0.5641)validation : 5/10 (0.5641)early stopping at 5 with loss 0.5641
AttentionModel-training is done: 5/10
2016-03-31 | reset count: 0 | final loss: 0.5641 at epoch 3
making sample. (inference: True, date:2016-04-30 00:00:00, date_number: 352)
making sample. (inference: False, date:2016-03-31 00:00:00, date_number: 351)
[AttentionModel/2016-04-30 | get_logger | INFO]: ====================AttentionModel/2016-04-30 start====================
making sample. (inference: True, date:2016-05-31 00:00:00, date_number: 353)
making sample. (inference: False, date:2016-04-30 00:00:00, date_number: 352)
[AttentionModel/2016-05-31 | get_logger | INFO]: ====================AttentionModel/2016-05-31 start====================
[strategy_integration.py] set_seed(2016-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5809)training: 2/10 (0.5809)validation : 2/10 (0.5809)training: 3/10 (0.5809)validation : 3/10 (0.5809)training: 4/10 (0.5809)validation : 4/10 (0.5809)training: 5/10 (0.5809)validation : 5/10 (0.5809)training: 6/10 (0.5809)validation : 6/10 (0.5799)training: 7/10 (0.5799)validation : 7/10 (0.5774)training: 8/10 (0.5774)validation : 8/10 (0.5774)training: 9/10 (0.5774)validation : 9/10 (0.5774)early stopping at 9 with loss 0.5774
AttentionModel-training is done: 9/10
2016-04-30 | reset count: 0 | final loss: 0.5774 at epoch 7
[strategy_integration.py] set_seed(2016-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5779)training: 2/10 (0.5779)validation : 2/10 (0.5779)training: 3/10 (0.5779)validation : 3/10 (0.5740)training: 4/10 (0.5740)validation : 4/10 (0.5738)training: 5/10 (0.5738)validation : 5/10 (0.5738)training: 6/10 (0.5738)validation : 6/10 (0.5738)early stopping at 6 with loss 0.5738
AttentionModel-training is done: 6/10
2016-05-31 | reset count: 0 | final loss: 0.5738 at epoch 4
making sample. (inference: True, date:2016-06-30 00:00:00, date_number: 354)
making sample. (inference: False, date:2016-05-31 00:00:00, date_number: 353)
[AttentionModel/2016-06-30 | get_logger | INFO]: ====================AttentionModel/2016-06-30 start====================
making sample. (inference: True, date:2016-07-31 00:00:00, date_number: 355)
making sample. (inference: False, date:2016-06-30 00:00:00, date_number: 354)
[AttentionModel/2016-07-31 | get_logger | INFO]: ====================AttentionModel/2016-07-31 start====================
[strategy_integration.py] set_seed(2016-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5651)training: 2/10 (0.5651)validation : 2/10 (0.5651)training: 3/10 (0.5651)validation : 3/10 (0.5651)training: 4/10 (0.5651)validation : 4/10 (0.5635)training: 5/10 (0.5635)validation : 5/10 (0.5635)training: 6/10 (0.5635)validation : 6/10 (0.5627)training: 7/10 (0.5627)validation : 7/10 (0.5627)early stopping at 7 with loss 0.5627
AttentionModel-training is done: 7/10
2016-06-30 | reset count: 0 | final loss: 0.5627 at epoch 6
[strategy_integration.py] set_seed(2016-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5741)training: 2/10 (0.5741)validation : 2/10 (0.5741)training: 3/10 (0.5741)validation : 3/10 (0.5739)training: 4/10 (0.5739)validation : 4/10 (0.5700)training: 5/10 (0.5700)validation : 5/10 (0.5700)training: 6/10 (0.5700)validation : 6/10 (0.5700)early stopping at 6 with loss 0.5700
AttentionModel-training is done: 6/10
2016-07-31 | reset count: 0 | final loss: 0.5700 at epoch 4
making sample. (inference: True, date:2016-08-31 00:00:00, date_number: 356)
making sample. (inference: False, date:2016-07-31 00:00:00, date_number: 355)
[AttentionModel/2016-08-31 | get_logger | INFO]: ====================AttentionModel/2016-08-31 start====================
making sample. (inference: True, date:2016-09-30 00:00:00, date_number: 357)
making sample. (inference: False, date:2016-08-31 00:00:00, date_number: 356)
[AttentionModel/2016-09-30 | get_logger | INFO]: ====================AttentionModel/2016-09-30 start====================
[strategy_integration.py] set_seed(2016-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5793)training: 2/10 (0.5793)validation : 2/10 (0.5753)training: 3/10 (0.5753)validation : 3/10 (0.5753)training: 4/10 (0.5753)validation : 4/10 (0.5753)training: 5/10 (0.5753)validation : 5/10 (0.5743)training: 6/10 (0.5743)validation : 6/10 (0.5743)training: 7/10 (0.5743)validation : 7/10 (0.5743)early stopping at 7 with loss 0.5743
AttentionModel-training is done: 7/10
2016-08-31 | reset count: 0 | final loss: 0.5743 at epoch 5
[strategy_integration.py] set_seed(2016-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5790)training: 2/10 (0.5790)validation : 2/10 (0.5750)training: 3/10 (0.5750)validation : 3/10 (0.5750)training: 4/10 (0.5750)validation : 4/10 (0.5725)training: 5/10 (0.5725)validation : 5/10 (0.5725)training: 6/10 (0.5725)validation : 6/10 (0.5725)training: 7/10 (0.5725)validation : 7/10 (0.5725)early stopping at 7 with loss 0.5725
AttentionModel-training is done: 7/10
2016-09-30 | reset count: 0 | final loss: 0.5725 at epoch 4
making sample. (inference: True, date:2016-10-31 00:00:00, date_number: 358)
making sample. (inference: False, date:2016-09-30 00:00:00, date_number: 357)
[AttentionModel/2016-10-31 | get_logger | INFO]: ====================AttentionModel/2016-10-31 start====================
making sample. (inference: True, date:2016-11-30 00:00:00, date_number: 359)
making sample. (inference: False, date:2016-10-31 00:00:00, date_number: 358)
[AttentionModel/2016-11-30 | get_logger | INFO]: ====================AttentionModel/2016-11-30 start====================
[strategy_integration.py] set_seed(2016-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5703)training: 2/10 (0.5703)validation : 2/10 (0.5687)training: 3/10 (0.5687)validation : 3/10 (0.5684)training: 4/10 (0.5684)validation : 4/10 (0.5684)training: 5/10 (0.5684)validation : 5/10 (0.5684)training: 6/10 (0.5684)validation : 6/10 (0.5684)early stopping at 6 with loss 0.5684
AttentionModel-training is done: 6/10
2016-10-31 | reset count: 0 | final loss: 0.5684 at epoch 3
making sample. (inference: True, date:2016-12-31 00:00:00, date_number: 360)
making sample. (inference: False, date:2016-11-30 00:00:00, date_number: 359)
[AttentionModel/2016-12-31 | get_logger | INFO]: ====================AttentionModel/2016-12-31 start====================
[strategy_integration.py] set_seed(2016-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5590)training: 2/10 (0.5590)validation : 2/10 (0.5558)training: 3/10 (0.5558)validation : 3/10 (0.5558)training: 4/10 (0.5558)validation : 4/10 (0.5558)training: 5/10 (0.5558)validation : 5/10 (0.5558)training: 6/10 (0.5558)validation : 6/10 (0.5558)training: 7/10 (0.5558)validation : 7/10 (0.5546)training: 8/10 (0.5546)validation : 8/10 (0.5546)training: 9/10 (0.5546)validation : 9/10 (0.5546)training: 10/10 (0.5546)validation : 10/10 (0.5546)early stopping at 10 with loss 0.5546
AttentionModel-training is done: 10/10
2016-11-30 | reset count: 0 | final loss: 0.5546 at epoch 7
making sample. (inference: True, date:2017-01-31 00:00:00, date_number: 361)
making sample. (inference: False, date:2016-12-31 00:00:00, date_number: 360)
[AttentionModel/2017-01-31 | get_logger | INFO]: ====================AttentionModel/2017-01-31 start====================
[strategy_integration.py] set_seed(2016-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5683)training: 2/10 (0.5683)validation : 2/10 (0.5622)training: 3/10 (0.5622)validation : 3/10 (0.5622)training: 4/10 (0.5622)validation : 4/10 (0.5610)training: 5/10 (0.5610)validation : 5/10 (0.5605)training: 6/10 (0.5605)validation : 6/10 (0.5566)training: 7/10 (0.5566)validation : 7/10 (0.5566)training: 8/10 (0.5566)validation : 8/10 (0.5566)early stopping at 8 with loss 0.5566
AttentionModel-training is done: 8/10
2016-12-31 | reset count: 0 | final loss: 0.5566 at epoch 6
making sample. (inference: True, date:2017-02-28 00:00:00, date_number: 362)
making sample. (inference: False, date:2017-01-31 00:00:00, date_number: 361)
[AttentionModel/2017-02-28 | get_logger | INFO]: ====================AttentionModel/2017-02-28 start====================
[strategy_integration.py] set_seed(2017-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5727)training: 2/10 (0.5727)validation : 2/10 (0.5699)training: 3/10 (0.5699)validation : 3/10 (0.5674)training: 4/10 (0.5674)validation : 4/10 (0.5674)training: 5/10 (0.5674)validation : 5/10 (0.5674)training: 6/10 (0.5674)validation : 6/10 (0.5674)early stopping at 6 with loss 0.5674
AttentionModel-training is done: 6/10
2017-01-31 | reset count: 0 | final loss: 0.5674 at epoch 3
making sample. (inference: True, date:2017-03-31 00:00:00, date_number: 363)
making sample. (inference: False, date:2017-02-28 00:00:00, date_number: 362)
[AttentionModel/2017-03-31 | get_logger | INFO]: ====================AttentionModel/2017-03-31 start====================
[strategy_integration.py] set_seed(2017-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5800)training: 2/10 (0.5800)validation : 2/10 (0.5737)training: 3/10 (0.5737)validation : 3/10 (0.5737)training: 4/10 (0.5737)validation : 4/10 (0.5737)training: 5/10 (0.5737)validation : 5/10 (0.5737)early stopping at 5 with loss 0.5737
AttentionModel-training is done: 5/10
2017-02-28 | reset count: 0 | final loss: 0.5737 at epoch 2
making sample. (inference: True, date:2017-04-30 00:00:00, date_number: 364)
making sample. (inference: False, date:2017-03-31 00:00:00, date_number: 363)
[strategy_integration.py] set_seed(2017-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5683)training: 2/10 (0.5683)validation : 2/10 (0.5657)training: 3/10 (0.5657)validation : 3/10 (0.5657)training: 4/10 (0.5657)validation : 4/10 (0.5657)training: 5/10 (0.5657)validation : 5/10 (0.5657)early stopping at 5 with loss 0.5657
AttentionModel-training is done: 5/10
2017-03-31 | reset count: 0 | final loss: 0.5657 at epoch 2
[AttentionModel/2017-04-30 | get_logger | INFO]: ====================AttentionModel/2017-04-30 start====================
making sample. (inference: True, date:2017-05-31 00:00:00, date_number: 365)
making sample. (inference: False, date:2017-04-30 00:00:00, date_number: 364)
[AttentionModel/2017-05-31 | get_logger | INFO]: ====================AttentionModel/2017-05-31 start====================
[strategy_integration.py] set_seed(2017-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5659)training: 2/10 (0.5659)validation : 2/10 (0.5659)training: 3/10 (0.5659)validation : 3/10 (0.5659)training: 4/10 (0.5659)validation : 4/10 (0.5656)training: 5/10 (0.5656)validation : 5/10 (0.5656)early stopping at 5 with loss 0.5656
AttentionModel-training is done: 5/10
2017-04-30 | reset count: 0 | final loss: 0.5656 at epoch 4
[strategy_integration.py] set_seed(2017-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5688)training: 2/10 (0.5688)validation : 2/10 (0.5682)training: 3/10 (0.5682)validation : 3/10 (0.5673)training: 4/10 (0.5673)validation : 4/10 (0.5673)training: 5/10 (0.5673)validation : 5/10 (0.5673)early stopping at 5 with loss 0.5673
AttentionModel-training is done: 5/10
2017-05-31 | reset count: 0 | final loss: 0.5673 at epoch 3
making sample. (inference: True, date:2017-06-30 00:00:00, date_number: 366)
making sample. (inference: False, date:2017-05-31 00:00:00, date_number: 365)
[AttentionModel/2017-06-30 | get_logger | INFO]: ====================AttentionModel/2017-06-30 start====================
making sample. (inference: True, date:2017-07-31 00:00:00, date_number: 367)
making sample. (inference: False, date:2017-06-30 00:00:00, date_number: 366)
[AttentionModel/2017-07-31 | get_logger | INFO]: ====================AttentionModel/2017-07-31 start====================
[strategy_integration.py] set_seed(2017-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5597)training: 2/10 (0.5597)validation : 2/10 (0.5550)training: 3/10 (0.5550)validation : 3/10 (0.5550)training: 4/10 (0.5550)validation : 4/10 (0.5550)training: 5/10 (0.5550)validation : 5/10 (0.5550)early stopping at 5 with loss 0.5550
AttentionModel-training is done: 5/10
2017-06-30 | reset count: 0 | final loss: 0.5550 at epoch 2
making sample. (inference: True, date:2017-08-31 00:00:00, date_number: 368)
making sample. (inference: False, date:2017-07-31 00:00:00, date_number: 367)
[AttentionModel/2017-08-31 | get_logger | INFO]: ====================AttentionModel/2017-08-31 start====================
[strategy_integration.py] set_seed(2017-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5633)training: 2/10 (0.5633)validation : 2/10 (0.5633)training: 3/10 (0.5633)validation : 3/10 (0.5632)training: 4/10 (0.5632)validation : 4/10 (0.5568)training: 5/10 (0.5568)validation : 5/10 (0.5568)training: 6/10 (0.5568)validation : 6/10 (0.5568)training: 7/10 (0.5568)validation : 7/10 (0.5568)early stopping at 7 with loss 0.5568
AttentionModel-training is done: 7/10
2017-07-31 | reset count: 0 | final loss: 0.5568 at epoch 4
making sample. (inference: True, date:2017-09-30 00:00:00, date_number: 369)
making sample. (inference: False, date:2017-08-31 00:00:00, date_number: 368)
[AttentionModel/2017-09-30 | get_logger | INFO]: ====================AttentionModel/2017-09-30 start====================
[strategy_integration.py] set_seed(2017-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5740)training: 2/10 (0.5740)validation : 2/10 (0.5740)training: 3/10 (0.5740)validation : 3/10 (0.5731)training: 4/10 (0.5731)validation : 4/10 (0.5685)training: 5/10 (0.5685)validation : 5/10 (0.5685)training: 6/10 (0.5685)validation : 6/10 (0.5685)training: 7/10 (0.5685)validation : 7/10 (0.5685)early stopping at 7 with loss 0.5685
AttentionModel-training is done: 7/10
2017-08-31 | reset count: 0 | final loss: 0.5685 at epoch 4
making sample. (inference: True, date:2017-10-31 00:00:00, date_number: 370)
making sample. (inference: False, date:2017-09-30 00:00:00, date_number: 369)
[AttentionModel/2017-10-31 | get_logger | INFO]: ====================AttentionModel/2017-10-31 start====================
[strategy_integration.py] set_seed(2017-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5606)training: 2/10 (0.5606)validation : 2/10 (0.5581)training: 3/10 (0.5581)validation : 3/10 (0.5581)training: 4/10 (0.5581)validation : 4/10 (0.5581)training: 5/10 (0.5581)validation : 5/10 (0.5549)training: 6/10 (0.5549)validation : 6/10 (0.5549)training: 7/10 (0.5549)validation : 7/10 (0.5549)training: 8/10 (0.5549)validation : 8/10 (0.5549)early stopping at 8 with loss 0.5549
AttentionModel-training is done: 8/10
2017-09-30 | reset count: 0 | final loss: 0.5549 at epoch 5
making sample. (inference: True, date:2017-11-30 00:00:00, date_number: 371)
making sample. (inference: False, date:2017-10-31 00:00:00, date_number: 370)
[AttentionModel/2017-11-30 | get_logger | INFO]: ====================AttentionModel/2017-11-30 start====================
[strategy_integration.py] set_seed(2017-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5734)training: 2/10 (0.5734)validation : 2/10 (0.5734)training: 3/10 (0.5734)validation : 3/10 (0.5703)training: 4/10 (0.5703)validation : 4/10 (0.5703)training: 5/10 (0.5703)validation : 5/10 (0.5691)training: 6/10 (0.5691)validation : 6/10 (0.5691)training: 7/10 (0.5691)validation : 7/10 (0.5691)training: 8/10 (0.5691)validation : 8/10 (0.5691)early stopping at 8 with loss 0.5691
AttentionModel-training is done: 8/10
2017-10-31 | reset count: 0 | final loss: 0.5691 at epoch 5
making sample. (inference: True, date:2017-12-31 00:00:00, date_number: 372)
making sample. (inference: False, date:2017-11-30 00:00:00, date_number: 371)
[AttentionModel/2017-12-31 | get_logger | INFO]: ====================AttentionModel/2017-12-31 start====================
[strategy_integration.py] set_seed(2017-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5738)training: 2/10 (0.5738)validation : 2/10 (0.5695)training: 3/10 (0.5695)validation : 3/10 (0.5695)training: 4/10 (0.5695)validation : 4/10 (0.5695)training: 5/10 (0.5695)validation : 5/10 (0.5695)training: 6/10 (0.5695)validation : 6/10 (0.5677)training: 7/10 (0.5677)validation : 7/10 (0.5677)training: 8/10 (0.5677)validation : 8/10 (0.5677)early stopping at 8 with loss 0.5677
AttentionModel-training is done: 8/10
2017-11-30 | reset count: 0 | final loss: 0.5677 at epoch 6
making sample. (inference: True, date:2018-01-31 00:00:00, date_number: 373)
making sample. (inference: False, date:2017-12-31 00:00:00, date_number: 372)
[AttentionModel/2018-01-31 | get_logger | INFO]: ====================AttentionModel/2018-01-31 start====================
[strategy_integration.py] set_seed(2017-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5692)training: 2/10 (0.5692)validation : 2/10 (0.5647)training: 3/10 (0.5647)validation : 3/10 (0.5628)training: 4/10 (0.5628)validation : 4/10 (0.5620)training: 5/10 (0.5620)validation : 5/10 (0.5620)training: 6/10 (0.5620)validation : 6/10 (0.5620)early stopping at 6 with loss 0.5620
AttentionModel-training is done: 6/10
2017-12-31 | reset count: 0 | final loss: 0.5620 at epoch 4
making sample. (inference: True, date:2018-02-28 00:00:00, date_number: 374)
making sample. (inference: False, date:2018-01-31 00:00:00, date_number: 373)
[AttentionModel/2018-02-28 | get_logger | INFO]: ====================AttentionModel/2018-02-28 start====================
[strategy_integration.py] set_seed(2018-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5666)training: 2/10 (0.5666)validation : 2/10 (0.5646)training: 3/10 (0.5646)validation : 3/10 (0.5637)training: 4/10 (0.5637)validation : 4/10 (0.5610)training: 5/10 (0.5610)validation : 5/10 (0.5610)training: 6/10 (0.5610)validation : 6/10 (0.5610)early stopping at 6 with loss 0.5610
AttentionModel-training is done: 6/10
2018-01-31 | reset count: 0 | final loss: 0.5610 at epoch 4
making sample. (inference: True, date:2018-03-31 00:00:00, date_number: 375)
making sample. (inference: False, date:2018-02-28 00:00:00, date_number: 374)
[AttentionModel/2018-03-31 | get_logger | INFO]: ====================AttentionModel/2018-03-31 start====================
[strategy_integration.py] set_seed(2018-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5652)training: 2/10 (0.5652)validation : 2/10 (0.5622)training: 3/10 (0.5622)validation : 3/10 (0.5610)training: 4/10 (0.5610)validation : 4/10 (0.5610)training: 5/10 (0.5610)validation : 5/10 (0.5610)training: 6/10 (0.5610)validation : 6/10 (0.5610)early stopping at 6 with loss 0.5610
AttentionModel-training is done: 6/10
2018-02-28 | reset count: 0 | final loss: 0.5610 at epoch 3
making sample. (inference: True, date:2018-04-30 00:00:00, date_number: 376)
making sample. (inference: False, date:2018-03-31 00:00:00, date_number: 375)
[AttentionModel/2018-04-30 | get_logger | INFO]: ====================AttentionModel/2018-04-30 start====================
[strategy_integration.py] set_seed(2018-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5553)training: 2/10 (0.5553)validation : 2/10 (0.5538)training: 3/10 (0.5538)validation : 3/10 (0.5521)training: 4/10 (0.5521)validation : 4/10 (0.5521)training: 5/10 (0.5521)validation : 5/10 (0.5521)training: 6/10 (0.5521)validation : 6/10 (0.5521)early stopping at 6 with loss 0.5521
AttentionModel-training is done: 6/10
2018-03-31 | reset count: 0 | final loss: 0.5521 at epoch 3
making sample. (inference: True, date:2018-05-31 00:00:00, date_number: 377)
making sample. (inference: False, date:2018-04-30 00:00:00, date_number: 376)
[AttentionModel/2018-05-31 | get_logger | INFO]: ====================AttentionModel/2018-05-31 start====================
[strategy_integration.py] set_seed(2018-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5678)training: 2/10 (0.5678)validation : 2/10 (0.5663)training: 3/10 (0.5663)validation : 3/10 (0.5663)training: 4/10 (0.5663)validation : 4/10 (0.5663)training: 5/10 (0.5663)validation : 5/10 (0.5663)training: 6/10 (0.5663)validation : 6/10 (0.5663)early stopping at 6 with loss 0.5663
AttentionModel-training is done: 6/10
2018-04-30 | reset count: 0 | final loss: 0.5663 at epoch 2
making sample. (inference: True, date:2018-06-30 00:00:00, date_number: 378)
making sample. (inference: False, date:2018-05-31 00:00:00, date_number: 377)
[AttentionModel/2018-06-30 | get_logger | INFO]: ====================AttentionModel/2018-06-30 start====================
[strategy_integration.py] set_seed(2018-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5653)training: 2/10 (0.5653)validation : 2/10 (0.5653)training: 3/10 (0.5653)validation : 3/10 (0.5653)training: 4/10 (0.5653)validation : 4/10 (0.5653)training: 5/10 (0.5653)validation : 5/10 (0.5636)training: 6/10 (0.5636)validation : 6/10 (0.5636)training: 7/10 (0.5636)validation : 7/10 (0.5636)training: 8/10 (0.5636)validation : 8/10 (0.5631)training: 9/10 (0.5631)validation : 9/10 (0.5609)training: 10/10 (0.5609)validation : 10/10 (0.5609)early stopping at 10 with loss 0.5609
AttentionModel-training is done: 10/10
2018-05-31 | reset count: 0 | final loss: 0.5609 at epoch 9
[strategy_integration.py] set_seed(2018-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5693)training: 2/10 (0.5693)validation : 2/10 (0.5654)training: 3/10 (0.5654)validation : 3/10 (0.5654)training: 4/10 (0.5654)validation : 4/10 (0.5654)training: 5/10 (0.5654)validation : 5/10 (0.5654)early stopping at 5 with loss 0.5654
AttentionModel-training is done: 5/10
2018-06-30 | reset count: 0 | final loss: 0.5654 at epoch 2
making sample. (inference: True, date:2018-07-31 00:00:00, date_number: 379)
making sample. (inference: False, date:2018-06-30 00:00:00, date_number: 378)
[AttentionModel/2018-07-31 | get_logger | INFO]: ====================AttentionModel/2018-07-31 start====================
making sample. (inference: True, date:2018-08-31 00:00:00, date_number: 380)
making sample. (inference: False, date:2018-07-31 00:00:00, date_number: 379)
[AttentionModel/2018-08-31 | get_logger | INFO]: ====================AttentionModel/2018-08-31 start====================
[strategy_integration.py] set_seed(2018-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5646)training: 2/10 (0.5646)validation : 2/10 (0.5637)training: 3/10 (0.5637)validation : 3/10 (0.5616)training: 4/10 (0.5616)validation : 4/10 (0.5616)training: 5/10 (0.5616)validation : 5/10 (0.5616)training: 6/10 (0.5616)validation : 6/10 (0.5606)training: 7/10 (0.5606)validation : 7/10 (0.5606)early stopping at 7 with loss 0.5606
AttentionModel-training is done: 7/10
2018-07-31 | reset count: 0 | final loss: 0.5606 at epoch 6
[strategy_integration.py] set_seed(2018-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5742)training: 2/10 (0.5742)validation : 2/10 (0.5740)training: 3/10 (0.5740)validation : 3/10 (0.5736)training: 4/10 (0.5736)validation : 4/10 (0.5736)training: 5/10 (0.5736)validation : 5/10 (0.5721)early stopping at 5 with loss 0.5721
AttentionModel-training is done: 5/10
2018-08-31 | reset count: 0 | final loss: 0.5721 at epoch 5
making sample. (inference: True, date:2018-09-30 00:00:00, date_number: 381)
making sample. (inference: False, date:2018-08-31 00:00:00, date_number: 380)
[AttentionModel/2018-09-30 | get_logger | INFO]: ====================AttentionModel/2018-09-30 start====================
making sample. (inference: True, date:2018-10-31 00:00:00, date_number: 382)
making sample. (inference: False, date:2018-09-30 00:00:00, date_number: 381)
[AttentionModel/2018-10-31 | get_logger | INFO]: ====================AttentionModel/2018-10-31 start====================
[strategy_integration.py] set_seed(2018-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5714)training: 2/10 (0.5714)validation : 2/10 (0.5714)training: 3/10 (0.5714)validation : 3/10 (0.5714)training: 4/10 (0.5714)validation : 4/10 (0.5714)training: 5/10 (0.5714)validation : 5/10 (0.5701)training: 6/10 (0.5701)validation : 6/10 (0.5690)training: 7/10 (0.5690)validation : 7/10 (0.5690)training: 8/10 (0.5690)validation : 8/10 (0.5690)early stopping at 8 with loss 0.5690
AttentionModel-training is done: 8/10
2018-09-30 | reset count: 0 | final loss: 0.5690 at epoch 6
[strategy_integration.py] set_seed(2018-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5874)training: 2/10 (0.5874)validation : 2/10 (0.5821)training: 3/10 (0.5821)validation : 3/10 (0.5821)training: 4/10 (0.5821)validation : 4/10 (0.5821)training: 5/10 (0.5821)validation : 5/10 (0.5821)training: 6/10 (0.5821)validation : 6/10 (0.5821)training: 7/10 (0.5821)validation : 7/10 (0.5821)training: 8/10 (0.5821)validation : 8/10 (0.5821)early stopping at 8 with loss 0.5821
AttentionModel-training is done: 8/10
2018-10-31 | reset count: 0 | final loss: 0.5821 at epoch 2
making sample. (inference: True, date:2018-11-30 00:00:00, date_number: 383)
making sample. (inference: False, date:2018-10-31 00:00:00, date_number: 382)
[AttentionModel/2018-11-30 | get_logger | INFO]: ====================AttentionModel/2018-11-30 start====================
making sample. (inference: True, date:2018-12-31 00:00:00, date_number: 384)
making sample. (inference: False, date:2018-11-30 00:00:00, date_number: 383)
[AttentionModel/2018-12-31 | get_logger | INFO]: ====================AttentionModel/2018-12-31 start====================
[strategy_integration.py] set_seed(2018-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5776)training: 2/10 (0.5776)validation : 2/10 (0.5727)training: 3/10 (0.5727)validation : 3/10 (0.5727)training: 4/10 (0.5727)validation : 4/10 (0.5727)training: 5/10 (0.5727)validation : 5/10 (0.5727)early stopping at 5 with loss 0.5727
AttentionModel-training is done: 5/10
2018-11-30 | reset count: 0 | final loss: 0.5727 at epoch 2
making sample. (inference: True, date:2019-01-31 00:00:00, date_number: 385)
making sample. (inference: False, date:2018-12-31 00:00:00, date_number: 384)
[AttentionModel/2019-01-31 | get_logger | INFO]: ====================AttentionModel/2019-01-31 start====================
[strategy_integration.py] set_seed(2018-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5818)training: 2/10 (0.5818)validation : 2/10 (0.5818)training: 3/10 (0.5818)validation : 3/10 (0.5800)training: 4/10 (0.5800)validation : 4/10 (0.5800)training: 5/10 (0.5800)validation : 5/10 (0.5799)training: 6/10 (0.5799)validation : 6/10 (0.5799)early stopping at 6 with loss 0.5799
AttentionModel-training is done: 6/10
2018-12-31 | reset count: 0 | final loss: 0.5799 at epoch 5
making sample. (inference: True, date:2019-02-28 00:00:00, date_number: 386)
making sample. (inference: False, date:2019-01-31 00:00:00, date_number: 385)
[AttentionModel/2019-02-28 | get_logger | INFO]: ====================AttentionModel/2019-02-28 start====================
[strategy_integration.py] set_seed(2019-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5826)training: 2/10 (0.5826)validation : 2/10 (0.5826)training: 3/10 (0.5826)validation : 3/10 (0.5826)training: 4/10 (0.5826)validation : 4/10 (0.5816)training: 5/10 (0.5816)validation : 5/10 (0.5801)training: 6/10 (0.5801)validation : 6/10 (0.5801)early stopping at 6 with loss 0.5801
AttentionModel-training is done: 6/10
2019-01-31 | reset count: 0 | final loss: 0.5801 at epoch 5
making sample. (inference: True, date:2019-03-31 00:00:00, date_number: 387)
making sample. (inference: False, date:2019-02-28 00:00:00, date_number: 386)
[AttentionModel/2019-03-31 | get_logger | INFO]: ====================AttentionModel/2019-03-31 start====================
[strategy_integration.py] set_seed(2019-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5870)training: 2/10 (0.5870)validation : 2/10 (0.5835)training: 3/10 (0.5835)validation : 3/10 (0.5801)training: 4/10 (0.5801)validation : 4/10 (0.5801)training: 5/10 (0.5801)validation : 5/10 (0.5801)training: 6/10 (0.5801)validation : 6/10 (0.5801)early stopping at 6 with loss 0.5801
AttentionModel-training is done: 6/10
2019-02-28 | reset count: 0 | final loss: 0.5801 at epoch 3
making sample. (inference: True, date:2019-04-30 00:00:00, date_number: 388)
making sample. (inference: False, date:2019-03-31 00:00:00, date_number: 387)
[strategy_integration.py] set_seed(2019-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5905)training: 2/10 (0.5905)validation : 2/10 (0.5905)training: 3/10 (0.5905)validation : 3/10 (0.5882)training: 4/10 (0.5882)validation : 4/10 (0.5882)training: 5/10 (0.5882)validation : 5/10 (0.5872)training: 6/10 (0.5872)validation : 6/10 (0.5872)early stopping at 6 with loss 0.5872
AttentionModel-training is done: 6/10
2019-03-31 | reset count: 0 | final loss: 0.5872 at epoch 5
[AttentionModel/2019-04-30 | get_logger | INFO]: ====================AttentionModel/2019-04-30 start====================
making sample. (inference: True, date:2019-05-31 00:00:00, date_number: 389)
making sample. (inference: False, date:2019-04-30 00:00:00, date_number: 388)
[AttentionModel/2019-05-31 | get_logger | INFO]: ====================AttentionModel/2019-05-31 start====================
[strategy_integration.py] set_seed(2019-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5829)training: 2/10 (0.5829)validation : 2/10 (0.5794)training: 3/10 (0.5794)validation : 3/10 (0.5794)training: 4/10 (0.5794)validation : 4/10 (0.5794)training: 5/10 (0.5794)validation : 5/10 (0.5794)early stopping at 5 with loss 0.5794
AttentionModel-training is done: 5/10
2019-04-30 | reset count: 0 | final loss: 0.5794 at epoch 2
[strategy_integration.py] set_seed(2019-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5910)training: 2/10 (0.5910)validation : 2/10 (0.5870)training: 3/10 (0.5870)validation : 3/10 (0.5866)training: 4/10 (0.5866)validation : 4/10 (0.5866)training: 5/10 (0.5866)validation : 5/10 (0.5866)early stopping at 5 with loss 0.5866
AttentionModel-training is done: 5/10
2019-05-31 | reset count: 0 | final loss: 0.5866 at epoch 3
making sample. (inference: True, date:2019-06-30 00:00:00, date_number: 390)
making sample. (inference: False, date:2019-05-31 00:00:00, date_number: 389)
[AttentionModel/2019-06-30 | get_logger | INFO]: ====================AttentionModel/2019-06-30 start====================
making sample. (inference: True, date:2019-07-31 00:00:00, date_number: 391)
making sample. (inference: False, date:2019-06-30 00:00:00, date_number: 390)
[AttentionModel/2019-07-31 | get_logger | INFO]: ====================AttentionModel/2019-07-31 start====================
[strategy_integration.py] set_seed(2019-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5823)training: 2/10 (0.5823)validation : 2/10 (0.5777)training: 3/10 (0.5777)validation : 3/10 (0.5777)training: 4/10 (0.5777)validation : 4/10 (0.5777)training: 5/10 (0.5777)validation : 5/10 (0.5777)training: 6/10 (0.5777)validation : 6/10 (0.5777)early stopping at 6 with loss 0.5777
AttentionModel-training is done: 6/10
2019-06-30 | reset count: 0 | final loss: 0.5777 at epoch 2
making sample. (inference: True, date:2019-08-31 00:00:00, date_number: 392)
making sample. (inference: False, date:2019-07-31 00:00:00, date_number: 391)
[strategy_integration.py] set_seed(2019-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5831)training: 2/10 (0.5831)validation : 2/10 (0.5817)training: 3/10 (0.5817)validation : 3/10 (0.5803)training: 4/10 (0.5803)validation : 4/10 (0.5772)training: 5/10 (0.5772)validation : 5/10 (0.5772)training: 6/10 (0.5772)validation : 6/10 (0.5772)training: 7/10 (0.5772)validation : 7/10 (0.5772)early stopping at 7 with loss 0.5772
AttentionModel-training is done: 7/10
2019-07-31 | reset count: 0 | final loss: 0.5772 at epoch 4
[AttentionModel/2019-08-31 | get_logger | INFO]: ====================AttentionModel/2019-08-31 start====================
making sample. (inference: True, date:2019-09-30 00:00:00, date_number: 393)
making sample. (inference: False, date:2019-08-31 00:00:00, date_number: 392)
[AttentionModel/2019-09-30 | get_logger | INFO]: ====================AttentionModel/2019-09-30 start====================
[strategy_integration.py] set_seed(2019-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5804)training: 2/10 (0.5804)validation : 2/10 (0.5756)training: 3/10 (0.5756)validation : 3/10 (0.5734)training: 4/10 (0.5734)validation : 4/10 (0.5733)training: 5/10 (0.5733)validation : 5/10 (0.5733)training: 6/10 (0.5733)validation : 6/10 (0.5733)early stopping at 6 with loss 0.5733
AttentionModel-training is done: 6/10
2019-08-31 | reset count: 0 | final loss: 0.5733 at epoch 4
[strategy_integration.py] set_seed(2019-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5843)training: 2/10 (0.5843)validation : 2/10 (0.5797)training: 3/10 (0.5797)validation : 3/10 (0.5794)training: 4/10 (0.5794)validation : 4/10 (0.5794)training: 5/10 (0.5794)validation : 5/10 (0.5794)early stopping at 5 with loss 0.5794
AttentionModel-training is done: 5/10
2019-09-30 | reset count: 0 | final loss: 0.5794 at epoch 3
making sample. (inference: True, date:2019-10-31 00:00:00, date_number: 394)
making sample. (inference: False, date:2019-09-30 00:00:00, date_number: 393)
[AttentionModel/2019-10-31 | get_logger | INFO]: ====================AttentionModel/2019-10-31 start====================
making sample. (inference: True, date:2019-11-30 00:00:00, date_number: 395)
making sample. (inference: False, date:2019-10-31 00:00:00, date_number: 394)
[AttentionModel/2019-11-30 | get_logger | INFO]: ====================AttentionModel/2019-11-30 start====================
[strategy_integration.py] set_seed(2019-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5776)training: 2/10 (0.5776)validation : 2/10 (0.5740)training: 3/10 (0.5740)validation : 3/10 (0.5740)training: 4/10 (0.5740)validation : 4/10 (0.5737)training: 5/10 (0.5737)validation : 5/10 (0.5737)training: 6/10 (0.5737)validation : 6/10 (0.5737)training: 7/10 (0.5737)validation : 7/10 (0.5737)early stopping at 7 with loss 0.5737
AttentionModel-training is done: 7/10
2019-10-31 | reset count: 0 | final loss: 0.5737 at epoch 4
making sample. (inference: True, date:2019-12-31 00:00:00, date_number: 396)
making sample. (inference: False, date:2019-11-30 00:00:00, date_number: 395)
[AttentionModel/2019-12-31 | get_logger | INFO]: ====================AttentionModel/2019-12-31 start====================
[strategy_integration.py] set_seed(2019-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5970)training: 2/10 (0.5970)validation : 2/10 (0.5865)training: 3/10 (0.5865)validation : 3/10 (0.5865)training: 4/10 (0.5865)validation : 4/10 (0.5865)training: 5/10 (0.5865)validation : 5/10 (0.5865)training: 6/10 (0.5865)validation : 6/10 (0.5864)training: 7/10 (0.5864)validation : 7/10 (0.5850)training: 8/10 (0.5850)validation : 8/10 (0.5850)training: 9/10 (0.5850)validation : 9/10 (0.5838)training: 10/10 (0.5838)validation : 10/10 (0.5838)early stopping at 10 with loss 0.5838
AttentionModel-training is done: 10/10
2019-11-30 | reset count: 0 | final loss: 0.5838 at epoch 9
making sample. (inference: True, date:2020-01-31 00:00:00, date_number: 397)
making sample. (inference: False, date:2019-12-31 00:00:00, date_number: 396)
[AttentionModel/2020-01-31 | get_logger | INFO]: ====================AttentionModel/2020-01-31 start====================
[strategy_integration.py] set_seed(2019-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5846)training: 2/10 (0.5846)validation : 2/10 (0.5846)training: 3/10 (0.5846)validation : 3/10 (0.5825)training: 4/10 (0.5825)validation : 4/10 (0.5824)training: 5/10 (0.5824)validation : 5/10 (0.5824)training: 6/10 (0.5824)validation : 6/10 (0.5815)training: 7/10 (0.5815)validation : 7/10 (0.5815)early stopping at 7 with loss 0.5815
AttentionModel-training is done: 7/10
2019-12-31 | reset count: 0 | final loss: 0.5815 at epoch 6
making sample. (inference: True, date:2020-02-29 00:00:00, date_number: 398)
making sample. (inference: False, date:2020-01-31 00:00:00, date_number: 397)
[AttentionModel/2020-02-29 | get_logger | INFO]: ====================AttentionModel/2020-02-29 start====================
[strategy_integration.py] set_seed(2020-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5882)training: 2/10 (0.5882)validation : 2/10 (0.5840)training: 3/10 (0.5840)validation : 3/10 (0.5814)training: 4/10 (0.5814)validation : 4/10 (0.5814)training: 5/10 (0.5814)validation : 5/10 (0.5794)training: 6/10 (0.5794)validation : 6/10 (0.5794)early stopping at 6 with loss 0.5794
AttentionModel-training is done: 6/10
2020-01-31 | reset count: 0 | final loss: 0.5794 at epoch 5
making sample. (inference: True, date:2020-03-31 00:00:00, date_number: 399)
making sample. (inference: False, date:2020-02-29 00:00:00, date_number: 398)
[AttentionModel/2020-03-31 | get_logger | INFO]: ====================AttentionModel/2020-03-31 start====================
[strategy_integration.py] set_seed(2020-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5829)training: 2/10 (0.5829)validation : 2/10 (0.5800)training: 3/10 (0.5800)validation : 3/10 (0.5800)training: 4/10 (0.5800)validation : 4/10 (0.5781)training: 5/10 (0.5781)validation : 5/10 (0.5781)training: 6/10 (0.5781)validation : 6/10 (0.5781)training: 7/10 (0.5781)validation : 7/10 (0.5780)training: 8/10 (0.5780)validation : 8/10 (0.5780)early stopping at 8 with loss 0.5780
AttentionModel-training is done: 8/10
2020-02-29 | reset count: 0 | final loss: 0.5780 at epoch 7
[strategy_integration.py] set_seed(2020-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5783)training: 2/10 (0.5783)validation : 2/10 (0.5689)training: 3/10 (0.5689)validation : 3/10 (0.5689)training: 4/10 (0.5689)validation : 4/10 (0.5689)training: 5/10 (0.5689)validation : 5/10 (0.5680)training: 6/10 (0.5680)validation : 6/10 (0.5680)early stopping at 6 with loss 0.5680
AttentionModel-training is done: 6/10
2020-03-31 | reset count: 0 | final loss: 0.5680 at epoch 5
making sample. (inference: True, date:2020-04-30 00:00:00, date_number: 400)
making sample. (inference: False, date:2020-03-31 00:00:00, date_number: 399)
[AttentionModel/2020-04-30 | get_logger | INFO]: ====================AttentionModel/2020-04-30 start====================
making sample. (inference: True, date:2020-05-31 00:00:00, date_number: 401)
making sample. (inference: False, date:2020-04-30 00:00:00, date_number: 400)
[AttentionModel/2020-05-31 | get_logger | INFO]: ====================AttentionModel/2020-05-31 start====================
[strategy_integration.py] set_seed(2020-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5837)training: 2/10 (0.5837)validation : 2/10 (0.5837)training: 3/10 (0.5837)validation : 3/10 (0.5816)training: 4/10 (0.5816)validation : 4/10 (0.5816)training: 5/10 (0.5816)validation : 5/10 (0.5811)training: 6/10 (0.5811)validation : 6/10 (0.5811)early stopping at 6 with loss 0.5811
AttentionModel-training is done: 6/10
2020-04-30 | reset count: 0 | final loss: 0.5811 at epoch 5
making sample. (inference: True, date:2020-06-30 00:00:00, date_number: 402)
making sample. (inference: False, date:2020-05-31 00:00:00, date_number: 401)
[AttentionModel/2020-06-30 | get_logger | INFO]: ====================AttentionModel/2020-06-30 start====================
[strategy_integration.py] set_seed(2020-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5821)training: 2/10 (0.5821)validation : 2/10 (0.5764)training: 3/10 (0.5764)validation : 3/10 (0.5764)training: 4/10 (0.5764)validation : 4/10 (0.5728)training: 5/10 (0.5728)validation : 5/10 (0.5728)training: 6/10 (0.5728)validation : 6/10 (0.5728)training: 7/10 (0.5728)validation : 7/10 (0.5681)training: 8/10 (0.5681)validation : 8/10 (0.5681)training: 9/10 (0.5681)validation : 9/10 (0.5681)training: 10/10 (0.5681)validation : 10/10 (0.5681)early stopping at 10 with loss 0.5681
AttentionModel-training is done: 10/10
2020-05-31 | reset count: 0 | final loss: 0.5681 at epoch 7
[strategy_integration.py] set_seed(2020-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5789)training: 2/10 (0.5789)validation : 2/10 (0.5773)training: 3/10 (0.5773)validation : 3/10 (0.5743)training: 4/10 (0.5743)validation : 4/10 (0.5743)training: 5/10 (0.5743)validation : 5/10 (0.5743)training: 6/10 (0.5743)validation : 6/10 (0.5743)early stopping at 6 with loss 0.5743
AttentionModel-training is done: 6/10
2020-06-30 | reset count: 0 | final loss: 0.5743 at epoch 3
making sample. (inference: True, date:2020-07-31 00:00:00, date_number: 403)
making sample. (inference: False, date:2020-06-30 00:00:00, date_number: 402)
[AttentionModel/2020-07-31 | get_logger | INFO]: ====================AttentionModel/2020-07-31 start====================
making sample. (inference: True, date:2020-08-31 00:00:00, date_number: 404)
making sample. (inference: False, date:2020-07-31 00:00:00, date_number: 403)
[AttentionModel/2020-08-31 | get_logger | INFO]: ====================AttentionModel/2020-08-31 start====================
[strategy_integration.py] set_seed(2020-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5757)training: 2/10 (0.5757)validation : 2/10 (0.5741)training: 3/10 (0.5741)validation : 3/10 (0.5729)training: 4/10 (0.5729)validation : 4/10 (0.5729)training: 5/10 (0.5729)validation : 5/10 (0.5729)early stopping at 5 with loss 0.5729
AttentionModel-training is done: 5/10
2020-07-31 | reset count: 0 | final loss: 0.5729 at epoch 3
[strategy_integration.py] set_seed(2020-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5734)training: 2/10 (0.5734)validation : 2/10 (0.5722)training: 3/10 (0.5722)validation : 3/10 (0.5722)training: 4/10 (0.5722)validation : 4/10 (0.5722)training: 5/10 (0.5722)validation : 5/10 (0.5721)early stopping at 5 with loss 0.5721
AttentionModel-training is done: 5/10
2020-08-31 | reset count: 0 | final loss: 0.5721 at epoch 5
making sample. (inference: True, date:2020-09-30 00:00:00, date_number: 405)
making sample. (inference: False, date:2020-08-31 00:00:00, date_number: 404)
[AttentionModel/2020-09-30 | get_logger | INFO]: ====================AttentionModel/2020-09-30 start====================
making sample. (inference: True, date:2020-10-31 00:00:00, date_number: 406)
making sample. (inference: False, date:2020-09-30 00:00:00, date_number: 405)
[AttentionModel/2020-10-31 | get_logger | INFO]: ====================AttentionModel/2020-10-31 start====================
[strategy_integration.py] set_seed(2020-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5775)training: 2/10 (0.5775)validation : 2/10 (0.5737)training: 3/10 (0.5737)validation : 3/10 (0.5714)training: 4/10 (0.5714)validation : 4/10 (0.5714)training: 5/10 (0.5714)validation : 5/10 (0.5714)early stopping at 5 with loss 0.5714
AttentionModel-training is done: 5/10
2020-09-30 | reset count: 0 | final loss: 0.5714 at epoch 3
making sample. (inference: True, date:2020-11-30 00:00:00, date_number: 407)
making sample. (inference: False, date:2020-10-31 00:00:00, date_number: 406)
[AttentionModel/2020-11-30 | get_logger | INFO]: ====================AttentionModel/2020-11-30 start====================
[strategy_integration.py] set_seed(2020-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5734)training: 2/10 (0.5734)validation : 2/10 (0.5734)training: 3/10 (0.5734)validation : 3/10 (0.5734)training: 4/10 (0.5734)validation : 4/10 (0.5734)training: 5/10 (0.5734)validation : 5/10 (0.5733)training: 6/10 (0.5733)validation : 6/10 (0.5730)training: 7/10 (0.5730)validation : 7/10 (0.5730)training: 8/10 (0.5730)validation : 8/10 (0.5730)early stopping at 8 with loss 0.5730
AttentionModel-training is done: 8/10
2020-10-31 | reset count: 0 | final loss: 0.5730 at epoch 6
making sample. (inference: True, date:2020-12-31 00:00:00, date_number: 408)
making sample. (inference: False, date:2020-11-30 00:00:00, date_number: 407)
[AttentionModel/2020-12-31 | get_logger | INFO]: ====================AttentionModel/2020-12-31 start====================
[strategy_integration.py] set_seed(2020-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5715)training: 2/10 (0.5715)validation : 2/10 (0.5651)training: 3/10 (0.5651)validation : 3/10 (0.5651)training: 4/10 (0.5651)validation : 4/10 (0.5651)training: 5/10 (0.5651)validation : 5/10 (0.5651)early stopping at 5 with loss 0.5651
AttentionModel-training is done: 5/10
2020-11-30 | reset count: 0 | final loss: 0.5651 at epoch 2
making sample. (inference: True, date:2021-01-31 00:00:00, date_number: 409)
making sample. (inference: False, date:2020-12-31 00:00:00, date_number: 408)
[AttentionModel/2021-01-31 | get_logger | INFO]: ====================AttentionModel/2021-01-31 start====================
[strategy_integration.py] set_seed(2020-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5705)training: 2/10 (0.5705)validation : 2/10 (0.5695)training: 3/10 (0.5695)validation : 3/10 (0.5676)training: 4/10 (0.5676)validation : 4/10 (0.5676)training: 5/10 (0.5676)validation : 5/10 (0.5676)training: 6/10 (0.5676)validation : 6/10 (0.5676)early stopping at 6 with loss 0.5676
AttentionModel-training is done: 6/10
2020-12-31 | reset count: 0 | final loss: 0.5676 at epoch 3
making sample. (inference: True, date:2021-02-28 00:00:00, date_number: 410)
making sample. (inference: False, date:2021-01-31 00:00:00, date_number: 409)
[AttentionModel/2021-02-28 | get_logger | INFO]: ====================AttentionModel/2021-02-28 start====================
[strategy_integration.py] set_seed(2021-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5498)training: 2/10 (0.5498)validation : 2/10 (0.5487)training: 3/10 (0.5487)validation : 3/10 (0.5447)training: 4/10 (0.5447)validation : 4/10 (0.5447)training: 5/10 (0.5447)validation : 5/10 (0.5417)training: 6/10 (0.5417)validation : 6/10 (0.5417)training: 7/10 (0.5417)validation : 7/10 (0.5417)early stopping at 7 with loss 0.5417
AttentionModel-training is done: 7/10
2021-01-31 | reset count: 0 | final loss: 0.5417 at epoch 5
making sample. (inference: True, date:2021-03-31 00:00:00, date_number: 411)
making sample. (inference: False, date:2021-02-28 00:00:00, date_number: 410)
[AttentionModel/2021-03-31 | get_logger | INFO]: ====================AttentionModel/2021-03-31 start====================
[strategy_integration.py] set_seed(2021-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5674)training: 2/10 (0.5674)validation : 2/10 (0.5570)training: 3/10 (0.5570)validation : 3/10 (0.5570)training: 4/10 (0.5570)validation : 4/10 (0.5570)training: 5/10 (0.5570)validation : 5/10 (0.5570)training: 6/10 (0.5570)validation : 6/10 (0.5570)training: 7/10 (0.5570)validation : 7/10 (0.5570)training: 8/10 (0.5570)validation : 8/10 (0.5570)early stopping at 8 with loss 0.5570
AttentionModel-training is done: 8/10
2021-02-28 | reset count: 0 | final loss: 0.5570 at epoch 2
making sample. (inference: True, date:2021-04-30 00:00:00, date_number: 412)
making sample. (inference: False, date:2021-03-31 00:00:00, date_number: 411)
[strategy_integration.py] set_seed(2021-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5691)training: 2/10 (0.5691)validation : 2/10 (0.5638)training: 3/10 (0.5638)validation : 3/10 (0.5631)training: 4/10 (0.5631)validation : 4/10 (0.5631)training: 5/10 (0.5631)validation : 5/10 (0.5631)training: 6/10 (0.5631)validation : 6/10 (0.5631)early stopping at 6 with loss 0.5631
AttentionModel-training is done: 6/10
2021-03-31 | reset count: 0 | final loss: 0.5631 at epoch 3
[AttentionModel/2021-04-30 | get_logger | INFO]: ====================AttentionModel/2021-04-30 start====================
making sample. (inference: True, date:2021-05-31 00:00:00, date_number: 413)
making sample. (inference: False, date:2021-04-30 00:00:00, date_number: 412)
[AttentionModel/2021-05-31 | get_logger | INFO]: ====================AttentionModel/2021-05-31 start====================
[strategy_integration.py] set_seed(2021-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5673)training: 2/10 (0.5673)validation : 2/10 (0.5630)training: 3/10 (0.5630)validation : 3/10 (0.5630)training: 4/10 (0.5630)validation : 4/10 (0.5630)training: 5/10 (0.5630)validation : 5/10 (0.5630)training: 6/10 (0.5630)validation : 6/10 (0.5606)training: 7/10 (0.5606)validation : 7/10 (0.5606)training: 8/10 (0.5606)validation : 8/10 (0.5606)training: 9/10 (0.5606)validation : 9/10 (0.5606)early stopping at 9 with loss 0.5606
AttentionModel-training is done: 9/10
2021-04-30 | reset count: 0 | final loss: 0.5606 at epoch 6
[strategy_integration.py] set_seed(2021-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5723)training: 2/10 (0.5723)validation : 2/10 (0.5723)training: 3/10 (0.5723)validation : 3/10 (0.5719)training: 4/10 (0.5719)validation : 4/10 (0.5716)training: 5/10 (0.5716)validation : 5/10 (0.5690)training: 6/10 (0.5690)validation : 6/10 (0.5690)early stopping at 6 with loss 0.5690
AttentionModel-training is done: 6/10
2021-05-31 | reset count: 0 | final loss: 0.5690 at epoch 5
making sample. (inference: True, date:2021-06-30 00:00:00, date_number: 414)
making sample. (inference: False, date:2021-05-31 00:00:00, date_number: 413)
[AttentionModel/2021-06-30 | get_logger | INFO]: ====================AttentionModel/2021-06-30 start====================
making sample. (inference: True, date:2021-07-31 00:00:00, date_number: 415)
making sample. (inference: False, date:2021-06-30 00:00:00, date_number: 414)
[AttentionModel/2021-07-31 | get_logger | INFO]: ====================AttentionModel/2021-07-31 start====================
[strategy_integration.py] set_seed(2021-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5652)training: 2/10 (0.5652)validation : 2/10 (0.5635)training: 3/10 (0.5635)validation : 3/10 (0.5625)training: 4/10 (0.5625)validation : 4/10 (0.5625)training: 5/10 (0.5625)validation : 5/10 (0.5625)training: 6/10 (0.5625)validation : 6/10 (0.5625)early stopping at 6 with loss 0.5625
AttentionModel-training is done: 6/10
2021-06-30 | reset count: 0 | final loss: 0.5625 at epoch 3
making sample. (inference: True, date:2021-08-31 00:00:00, date_number: 416)
making sample. (inference: False, date:2021-07-31 00:00:00, date_number: 415)
[AttentionModel/2021-08-31 | get_logger | INFO]: ====================AttentionModel/2021-08-31 start====================
[strategy_integration.py] set_seed(2021-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5702)training: 2/10 (0.5702)validation : 2/10 (0.5702)training: 3/10 (0.5702)validation : 3/10 (0.5702)training: 4/10 (0.5702)validation : 4/10 (0.5702)training: 5/10 (0.5702)validation : 5/10 (0.5690)training: 6/10 (0.5690)validation : 6/10 (0.5690)training: 7/10 (0.5690)validation : 7/10 (0.5690)early stopping at 7 with loss 0.5690
AttentionModel-training is done: 7/10
2021-07-31 | reset count: 0 | final loss: 0.5690 at epoch 5
making sample. (inference: True, date:2021-09-30 00:00:00, date_number: 417)
making sample. (inference: False, date:2021-08-31 00:00:00, date_number: 416)
[AttentionModel/2021-09-30 | get_logger | INFO]: ====================AttentionModel/2021-09-30 start====================
[strategy_integration.py] set_seed(2021-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5736)training: 2/10 (0.5736)validation : 2/10 (0.5688)training: 3/10 (0.5688)validation : 3/10 (0.5688)training: 4/10 (0.5688)validation : 4/10 (0.5688)training: 5/10 (0.5688)validation : 5/10 (0.5688)training: 6/10 (0.5688)validation : 6/10 (0.5688)early stopping at 6 with loss 0.5688
AttentionModel-training is done: 6/10
2021-08-31 | reset count: 0 | final loss: 0.5688 at epoch 2
making sample. (inference: True, date:2021-10-31 00:00:00, date_number: 418)
making sample. (inference: False, date:2021-09-30 00:00:00, date_number: 417)
[AttentionModel/2021-10-31 | get_logger | INFO]: ====================AttentionModel/2021-10-31 start====================
[strategy_integration.py] set_seed(2021-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5877)training: 2/10 (0.5877)validation : 2/10 (0.5858)training: 3/10 (0.5858)validation : 3/10 (0.5822)training: 4/10 (0.5822)validation : 4/10 (0.5822)training: 5/10 (0.5822)validation : 5/10 (0.5822)training: 6/10 (0.5822)validation : 6/10 (0.5812)training: 7/10 (0.5812)validation : 7/10 (0.5812)early stopping at 7 with loss 0.5812
AttentionModel-training is done: 7/10
2021-09-30 | reset count: 0 | final loss: 0.5812 at epoch 6
[strategy_integration.py] set_seed(2021-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5766)training: 2/10 (0.5766)validation : 2/10 (0.5766)training: 3/10 (0.5766)validation : 3/10 (0.5766)training: 4/10 (0.5766)validation : 4/10 (0.5766)training: 5/10 (0.5766)validation : 5/10 (0.5756)training: 6/10 (0.5756)validation : 6/10 (0.5756)training: 7/10 (0.5756)validation : 7/10 (0.5756)training: 8/10 (0.5756)validation : 8/10 (0.5749)training: 9/10 (0.5749)validation : 9/10 (0.5749)training: 10/10 (0.5749)validation : 10/10 (0.5749)AttentionModel-training is done: 10/10
2021-10-31 | reset count: 0 | final loss: 0.5749 at epoch 8
[strategy | get_logger | INFO]: ====================strategy start====================
[strategy | _get_infer_dirs | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick88_test03/infer 이 strategy 대상에 추가됨
[strategy | _backtest | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick88_test03/infer is starting
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.155 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading masking data⠦ [Compustat API] : Loading get_historic_universe from cache⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading get_historic_universe from cache done in 1.945 secs
⠧ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.246 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.062 secs
⠋ [Compustat API] : Loading get_monthly_total_return from cache✓ [Compustat API] : Loading get_monthly_total_return from cache done in 0.062 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.076 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.071 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.069 secs
⠋ [Compustat API] : Loading get_monthly_volume_data from cache✓ [Compustat API] : Loading get_monthly_volume_data from cache done in 0.08 secs
[strategy | _allocating | INFO]: [STRATEGY] allocating start
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_weight_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] performance.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] combined.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] weight.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] universe.csv is saved.
[comparison | get_logger | INFO]: ====================comparison start====================
[comparison | compare | INFO]: [5. COMPARISON] comparison is saved.
[comparison | compare | INFO]:                        Harvest_Green_concept001_pick88_test03_price_return  ...  SPCOMP_total_win
2016-01-31_2021-04-30                                           0.248875    ...          0.765625

[1 rows x 27 columns]
2015-12-31
2016-01-31
2016-02-29
2016-03-31
2016-04-30
2016-05-31
2016-06-30
2016-07-31
2016-08-31
2016-09-30
2016-10-31
2016-11-30
2016-12-31
2017-01-31
2017-02-28
2017-03-31
2017-04-30
2017-05-31
2017-06-30
2017-07-31
2017-08-31
2017-09-30
2017-10-31
2017-11-30
2017-12-31
2018-01-31
2018-02-28
2018-03-31
2018-04-30
2018-05-31
2018-06-30
2018-07-31
2018-08-31
2018-09-30
2018-10-31
2018-11-30
2018-12-31
2019-01-31
2019-02-28
2019-03-31
2019-04-30
2019-05-31
2019-06-30
2019-07-31
2019-08-31
2019-09-30
2019-10-31
2019-11-30
2019-12-31
2020-01-31
2020-02-29
2020-03-31
2020-04-30
2020-05-31
2020-06-30
2020-07-31
2020-08-31
2020-09-30
2020-10-31
2020-11-30
2020-12-31
2021-01-31
2021-02-28
2021-03-31
2021-04-30
2021-05-31
2021-06-30
2021-07-31
2021-08-31
2021-09-30
2021-10-31
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.048 secs
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.046 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.064 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.08 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.073 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.663 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.675 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.683 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.677 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.684 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.076 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.67 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.672 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.671 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.664 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.662 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.08 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.683 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.075 secs
⠋ [Compustat API] : Cleaning past days cache✓ [Compustat API] : Cleaning past days cache done in 0.0 secs
auto caching이 활성화 되었습니다. /home/sronly/sr-storage/kirin_cache/kirin_api_cache/2.19.17_2021-11-23에 캐쉬를 저장합니다.
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.148 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading masking data✓ [Compustat API] : Loading get_historic_universe from cache done in 2.0 secs
⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.301 secs
setting tensorflow random seed failed
load_data: ae_m
load_data: ia_ta
load_data: rc_a
load_data: aaa_t10y
load_data: baa_t10y
load_data: aaa_ff
load_data: real_pig
load_data: federal_tg
load_data: real_gdp
load_data: op_lev
load_data: roe
load_data: std_u_e
load_data: retail_mfr
load_data: m1
load_data: m2
load_data: r_s
load_data: r_a
load_data: fred_ff
load_data: t3m
load_data: t6m
load_data: t2y
load_data: t10y
load_data: aaa
load_data: baa
load_data: core_ppi
load_data: cpi
load_data: pce
load_data: snp500_vol
load_data: gpa
load_data: rev_surp
load_data: cash_at
load_data: baa_ff
load_data: core_cpi
load_data: core_pce
load_data: corporate_tg
load_data: industrial_prod
load_data: home_pr
load_data: pr_1m_0m
load_data: mv
load_data: btm
load_data: snp500_pr
load_data: wilshire500_pr
load_data: ted
load_data: ppi
load_data: trimmed_pce
load_data: unemploy
load_data: wti
load_data: capa_util
load_data: snp500_pe
load_data: res_mom_12m_1m_0m
load_data: res_vol_6m_3m_0m
load_data: at
load_data: ret_noa
load_data: etm
load_data: ia_mv
load_data: sector_values
load_data: mv
load_data: pr_1m_0m
load_data: mv
making sample. (inference: True, date:2015-12-31 00:00:00, date_number: 348)
making sample. (inference: False, date:2012-12-31 00:00:00, date_number: 312)
making sample. (inference: False, date:2013-01-31 00:00:00, date_number: 313)
making sample. (inference: False, date:2013-02-28 00:00:00, date_number: 314)
making sample. (inference: False, date:2013-03-31 00:00:00, date_number: 315)
making sample. (inference: False, date:2013-04-30 00:00:00, date_number: 316)
making sample. (inference: False, date:2013-05-31 00:00:00, date_number: 317)
making sample. (inference: False, date:2013-06-30 00:00:00, date_number: 318)
making sample. (inference: False, date:2013-07-31 00:00:00, date_number: 319)
making sample. (inference: False, date:2013-08-31 00:00:00, date_number: 320)
making sample. (inference: False, date:2013-09-30 00:00:00, date_number: 321)
making sample. (inference: False, date:2013-10-31 00:00:00, date_number: 322)
making sample. (inference: False, date:2013-11-30 00:00:00, date_number: 323)
making sample. (inference: False, date:2013-12-31 00:00:00, date_number: 324)
making sample. (inference: False, date:2014-01-31 00:00:00, date_number: 325)
making sample. (inference: False, date:2014-02-28 00:00:00, date_number: 326)
making sample. (inference: False, date:2014-03-31 00:00:00, date_number: 327)
making sample. (inference: False, date:2014-04-30 00:00:00, date_number: 328)
making sample. (inference: False, date:2014-05-31 00:00:00, date_number: 329)
making sample. (inference: False, date:2014-06-30 00:00:00, date_number: 330)
making sample. (inference: False, date:2014-07-31 00:00:00, date_number: 331)
making sample. (inference: False, date:2014-08-31 00:00:00, date_number: 332)
making sample. (inference: False, date:2014-09-30 00:00:00, date_number: 333)
making sample. (inference: False, date:2014-10-31 00:00:00, date_number: 334)
making sample. (inference: False, date:2014-11-30 00:00:00, date_number: 335)
making sample. (inference: False, date:2014-12-31 00:00:00, date_number: 336)
making sample. (inference: False, date:2015-01-31 00:00:00, date_number: 337)
making sample. (inference: False, date:2015-02-28 00:00:00, date_number: 338)
making sample. (inference: False, date:2015-03-31 00:00:00, date_number: 339)
making sample. (inference: False, date:2015-04-30 00:00:00, date_number: 340)
making sample. (inference: False, date:2015-05-31 00:00:00, date_number: 341)
making sample. (inference: False, date:2015-06-30 00:00:00, date_number: 342)
making sample. (inference: False, date:2015-07-31 00:00:00, date_number: 343)
making sample. (inference: False, date:2015-08-31 00:00:00, date_number: 344)
making sample. (inference: False, date:2015-09-30 00:00:00, date_number: 345)
making sample. (inference: False, date:2015-10-31 00:00:00, date_number: 346)
making sample. (inference: False, date:2015-11-30 00:00:00, date_number: 347)
[AttentionModel/2015-12-31 | get_logger | INFO]: ====================AttentionModel/2015-12-31 start====================
making sample. (inference: True, date:2016-01-31 00:00:00, date_number: 349)
making sample. (inference: False, date:2015-12-31 00:00:00, date_number: 348)
[AttentionModel/2016-01-31 | get_logger | INFO]: ====================AttentionModel/2016-01-31 start====================
[strategy_integration.py] set_seed(2015-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5741)training: 2/10 (0.5741)validation : 2/10 (0.5683)training: 3/10 (0.5683)validation : 3/10 (0.5683)training: 4/10 (0.5683)validation : 4/10 (0.5683)training: 5/10 (0.5683)validation : 5/10 (0.5683)training: 6/10 (0.5683)validation : 6/10 (0.5683)early stopping at 6 with loss 0.5683
AttentionModel-training is done: 6/10
2015-12-31 | reset count: 0 | final loss: 0.5683 at epoch 2
[strategy_integration.py] set_seed(2016-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5694)training: 2/10 (0.5694)validation : 2/10 (0.5694)training: 3/10 (0.5694)validation : 3/10 (0.5694)training: 4/10 (0.5694)validation : 4/10 (0.5694)training: 5/10 (0.5694)validation : 5/10 (0.5694)training: 6/10 (0.5694)validation : 6/10 (0.5694)early stopping at 6 with loss 0.5694
AttentionModel-training is done: 6/10
2016-01-31 | reset count: 0 | final loss: 0.5694 at epoch 1
making sample. (inference: True, date:2016-02-29 00:00:00, date_number: 350)
making sample. (inference: False, date:2016-01-31 00:00:00, date_number: 349)
[AttentionModel/2016-02-29 | get_logger | INFO]: ====================AttentionModel/2016-02-29 start====================
making sample. (inference: True, date:2016-03-31 00:00:00, date_number: 351)
making sample. (inference: False, date:2016-02-29 00:00:00, date_number: 350)
[AttentionModel/2016-03-31 | get_logger | INFO]: ====================AttentionModel/2016-03-31 start====================
[strategy_integration.py] set_seed(2016-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5669)training: 2/10 (0.5669)validation : 2/10 (0.5660)training: 3/10 (0.5660)validation : 3/10 (0.5652)training: 4/10 (0.5652)validation : 4/10 (0.5652)training: 5/10 (0.5652)validation : 5/10 (0.5652)early stopping at 5 with loss 0.5652
AttentionModel-training is done: 5/10
2016-02-29 | reset count: 0 | final loss: 0.5652 at epoch 3
making sample. (inference: True, date:2016-04-30 00:00:00, date_number: 352)
making sample. (inference: False, date:2016-03-31 00:00:00, date_number: 351)
[strategy_integration.py] set_seed(2016-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5755)training: 2/10 (0.5755)validation : 2/10 (0.5681)training: 3/10 (0.5681)validation : 3/10 (0.5681)training: 4/10 (0.5681)validation : 4/10 (0.5681)training: 5/10 (0.5681)validation : 5/10 (0.5681)early stopping at 5 with loss 0.5681
AttentionModel-training is done: 5/10
2016-03-31 | reset count: 0 | final loss: 0.5681 at epoch 2
[AttentionModel/2016-04-30 | get_logger | INFO]: ====================AttentionModel/2016-04-30 start====================
making sample. (inference: True, date:2016-05-31 00:00:00, date_number: 353)
making sample. (inference: False, date:2016-04-30 00:00:00, date_number: 352)
[AttentionModel/2016-05-31 | get_logger | INFO]: ====================AttentionModel/2016-05-31 start====================
[strategy_integration.py] set_seed(2016-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5765)training: 2/10 (0.5765)validation : 2/10 (0.5762)training: 3/10 (0.5762)validation : 3/10 (0.5745)training: 4/10 (0.5745)validation : 4/10 (0.5745)training: 5/10 (0.5745)validation : 5/10 (0.5745)early stopping at 5 with loss 0.5745
AttentionModel-training is done: 5/10
2016-04-30 | reset count: 0 | final loss: 0.5745 at epoch 3
making sample. (inference: True, date:2016-06-30 00:00:00, date_number: 354)
making sample. (inference: False, date:2016-05-31 00:00:00, date_number: 353)
[AttentionModel/2016-06-30 | get_logger | INFO]: ====================AttentionModel/2016-06-30 start====================
[strategy_integration.py] set_seed(2016-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5794)training: 2/10 (0.5794)validation : 2/10 (0.5728)training: 3/10 (0.5728)validation : 3/10 (0.5728)training: 4/10 (0.5728)validation : 4/10 (0.5728)training: 5/10 (0.5728)validation : 5/10 (0.5728)training: 6/10 (0.5728)validation : 6/10 (0.5728)training: 7/10 (0.5728)validation : 7/10 (0.5728)early stopping at 7 with loss 0.5728
AttentionModel-training is done: 7/10
2016-05-31 | reset count: 0 | final loss: 0.5728 at epoch 2
[strategy_integration.py] set_seed(2016-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5726)training: 2/10 (0.5726)validation : 2/10 (0.5701)training: 3/10 (0.5701)validation : 3/10 (0.5701)training: 4/10 (0.5701)validation : 4/10 (0.5701)training: 5/10 (0.5701)validation : 5/10 (0.5701)early stopping at 5 with loss 0.5701
AttentionModel-training is done: 5/10
2016-06-30 | reset count: 0 | final loss: 0.5701 at epoch 2
making sample. (inference: True, date:2016-07-31 00:00:00, date_number: 355)
making sample. (inference: False, date:2016-06-30 00:00:00, date_number: 354)
[AttentionModel/2016-07-31 | get_logger | INFO]: ====================AttentionModel/2016-07-31 start====================
making sample. (inference: True, date:2016-08-31 00:00:00, date_number: 356)
making sample. (inference: False, date:2016-07-31 00:00:00, date_number: 355)
[AttentionModel/2016-08-31 | get_logger | INFO]: ====================AttentionModel/2016-08-31 start====================
[strategy_integration.py] set_seed(2016-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5732)training: 2/10 (0.5732)validation : 2/10 (0.5732)training: 3/10 (0.5732)validation : 3/10 (0.5725)training: 4/10 (0.5725)validation : 4/10 (0.5725)training: 5/10 (0.5725)validation : 5/10 (0.5725)training: 6/10 (0.5725)validation : 6/10 (0.5725)early stopping at 6 with loss 0.5725
AttentionModel-training is done: 6/10
2016-07-31 | reset count: 0 | final loss: 0.5725 at epoch 3
[strategy_integration.py] set_seed(2016-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5600)training: 2/10 (0.5600)validation : 2/10 (0.5600)training: 3/10 (0.5600)validation : 3/10 (0.5600)training: 4/10 (0.5600)validation : 4/10 (0.5600)training: 5/10 (0.5600)validation : 5/10 (0.5600)early stopping at 5 with loss 0.5600
AttentionModel-training is done: 5/10
2016-08-31 | reset count: 0 | final loss: 0.5600 at epoch 1
making sample. (inference: True, date:2016-09-30 00:00:00, date_number: 357)
making sample. (inference: False, date:2016-08-31 00:00:00, date_number: 356)
[AttentionModel/2016-09-30 | get_logger | INFO]: ====================AttentionModel/2016-09-30 start====================
making sample. (inference: True, date:2016-10-31 00:00:00, date_number: 358)
making sample. (inference: False, date:2016-09-30 00:00:00, date_number: 357)
[AttentionModel/2016-10-31 | get_logger | INFO]: ====================AttentionModel/2016-10-31 start====================
[strategy_integration.py] set_seed(2016-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5643)training: 2/10 (0.5643)validation : 2/10 (0.5585)training: 3/10 (0.5585)validation : 3/10 (0.5585)training: 4/10 (0.5585)validation : 4/10 (0.5585)training: 5/10 (0.5585)validation : 5/10 (0.5585)training: 6/10 (0.5585)validation : 6/10 (0.5585)early stopping at 6 with loss 0.5585
AttentionModel-training is done: 6/10
2016-09-30 | reset count: 0 | final loss: 0.5585 at epoch 2
[strategy_integration.py] set_seed(2016-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5715)training: 2/10 (0.5715)validation : 2/10 (0.5715)training: 3/10 (0.5715)validation : 3/10 (0.5715)training: 4/10 (0.5715)validation : 4/10 (0.5715)training: 5/10 (0.5715)validation : 5/10 (0.5715)early stopping at 5 with loss 0.5715
AttentionModel-training is done: 5/10
2016-10-31 | reset count: 0 | final loss: 0.5715 at epoch 1
making sample. (inference: True, date:2016-11-30 00:00:00, date_number: 359)
making sample. (inference: False, date:2016-10-31 00:00:00, date_number: 358)
[AttentionModel/2016-11-30 | get_logger | INFO]: ====================AttentionModel/2016-11-30 start====================
making sample. (inference: True, date:2016-12-31 00:00:00, date_number: 360)
making sample. (inference: False, date:2016-11-30 00:00:00, date_number: 359)
[AttentionModel/2016-12-31 | get_logger | INFO]: ====================AttentionModel/2016-12-31 start====================
[strategy_integration.py] set_seed(2016-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5653)training: 2/10 (0.5653)validation : 2/10 (0.5653)training: 3/10 (0.5653)validation : 3/10 (0.5647)training: 4/10 (0.5647)validation : 4/10 (0.5641)training: 5/10 (0.5641)validation : 5/10 (0.5640)training: 6/10 (0.5640)validation : 6/10 (0.5619)training: 7/10 (0.5619)validation : 7/10 (0.5619)early stopping at 7 with loss 0.5619
AttentionModel-training is done: 7/10
2016-11-30 | reset count: 0 | final loss: 0.5619 at epoch 6
[strategy_integration.py] set_seed(2016-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5713)training: 2/10 (0.5713)validation : 2/10 (0.5713)training: 3/10 (0.5713)validation : 3/10 (0.5697)training: 4/10 (0.5697)validation : 4/10 (0.5697)training: 5/10 (0.5697)validation : 5/10 (0.5697)training: 6/10 (0.5697)validation : 6/10 (0.5683)training: 7/10 (0.5683)validation : 7/10 (0.5677)training: 8/10 (0.5677)validation : 8/10 (0.5677)early stopping at 8 with loss 0.5677
AttentionModel-training is done: 8/10
2016-12-31 | reset count: 0 | final loss: 0.5677 at epoch 7
making sample. (inference: True, date:2017-01-31 00:00:00, date_number: 361)
making sample. (inference: False, date:2016-12-31 00:00:00, date_number: 360)
[AttentionModel/2017-01-31 | get_logger | INFO]: ====================AttentionModel/2017-01-31 start====================
making sample. (inference: True, date:2017-02-28 00:00:00, date_number: 362)
making sample. (inference: False, date:2017-01-31 00:00:00, date_number: 361)
[AttentionModel/2017-02-28 | get_logger | INFO]: ====================AttentionModel/2017-02-28 start====================
[strategy_integration.py] set_seed(2017-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5634)training: 2/10 (0.5634)validation : 2/10 (0.5585)training: 3/10 (0.5585)validation : 3/10 (0.5585)training: 4/10 (0.5585)validation : 4/10 (0.5585)training: 5/10 (0.5585)validation : 5/10 (0.5585)training: 6/10 (0.5585)validation : 6/10 (0.5585)early stopping at 6 with loss 0.5585
AttentionModel-training is done: 6/10
2017-01-31 | reset count: 0 | final loss: 0.5585 at epoch 2
[strategy_integration.py] set_seed(2017-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5701)training: 2/10 (0.5701)validation : 2/10 (0.5701)training: 3/10 (0.5701)validation : 3/10 (0.5701)training: 4/10 (0.5701)validation : 4/10 (0.5701)training: 5/10 (0.5701)validation : 5/10 (0.5701)early stopping at 5 with loss 0.5701
AttentionModel-training is done: 5/10
2017-02-28 | reset count: 0 | final loss: 0.5701 at epoch 1
making sample. (inference: True, date:2017-03-31 00:00:00, date_number: 363)
making sample. (inference: False, date:2017-02-28 00:00:00, date_number: 362)
[AttentionModel/2017-03-31 | get_logger | INFO]: ====================AttentionModel/2017-03-31 start====================
making sample. (inference: True, date:2017-04-30 00:00:00, date_number: 364)
making sample. (inference: False, date:2017-03-31 00:00:00, date_number: 363)
[AttentionModel/2017-04-30 | get_logger | INFO]: ====================AttentionModel/2017-04-30 start====================
[strategy_integration.py] set_seed(2017-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5732)training: 2/10 (0.5732)validation : 2/10 (0.5720)training: 3/10 (0.5720)validation : 3/10 (0.5695)training: 4/10 (0.5695)validation : 4/10 (0.5675)training: 5/10 (0.5675)validation : 5/10 (0.5675)training: 6/10 (0.5675)validation : 6/10 (0.5675)training: 7/10 (0.5675)validation : 7/10 (0.5675)early stopping at 7 with loss 0.5675
AttentionModel-training is done: 7/10
2017-03-31 | reset count: 0 | final loss: 0.5675 at epoch 4
[strategy_integration.py] set_seed(2017-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5749)training: 2/10 (0.5749)validation : 2/10 (0.5749)training: 3/10 (0.5749)validation : 3/10 (0.5722)training: 4/10 (0.5722)validation : 4/10 (0.5722)training: 5/10 (0.5722)validation : 5/10 (0.5722)early stopping at 5 with loss 0.5722
AttentionModel-training is done: 5/10
2017-04-30 | reset count: 0 | final loss: 0.5722 at epoch 3
making sample. (inference: True, date:2017-05-31 00:00:00, date_number: 365)
making sample. (inference: False, date:2017-04-30 00:00:00, date_number: 364)
[AttentionModel/2017-05-31 | get_logger | INFO]: ====================AttentionModel/2017-05-31 start====================
making sample. (inference: True, date:2017-06-30 00:00:00, date_number: 366)
making sample. (inference: False, date:2017-05-31 00:00:00, date_number: 365)
[AttentionModel/2017-06-30 | get_logger | INFO]: ====================AttentionModel/2017-06-30 start====================
[strategy_integration.py] set_seed(2017-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5649)training: 2/10 (0.5649)validation : 2/10 (0.5649)training: 3/10 (0.5649)validation : 3/10 (0.5649)training: 4/10 (0.5649)validation : 4/10 (0.5642)training: 5/10 (0.5642)validation : 5/10 (0.5642)early stopping at 5 with loss 0.5642
AttentionModel-training is done: 5/10
2017-05-31 | reset count: 0 | final loss: 0.5642 at epoch 4
making sample. (inference: True, date:2017-07-31 00:00:00, date_number: 367)
making sample. (inference: False, date:2017-06-30 00:00:00, date_number: 366)
[strategy_integration.py] set_seed(2017-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5620)training: 2/10 (0.5620)validation : 2/10 (0.5620)training: 3/10 (0.5620)validation : 3/10 (0.5614)training: 4/10 (0.5614)validation : 4/10 (0.5614)training: 5/10 (0.5614)validation : 5/10 (0.5614)training: 6/10 (0.5614)validation : 6/10 (0.5614)early stopping at 6 with loss 0.5614
AttentionModel-training is done: 6/10
2017-06-30 | reset count: 0 | final loss: 0.5614 at epoch 3
[AttentionModel/2017-07-31 | get_logger | INFO]: ====================AttentionModel/2017-07-31 start====================
making sample. (inference: True, date:2017-08-31 00:00:00, date_number: 368)
making sample. (inference: False, date:2017-07-31 00:00:00, date_number: 367)
[AttentionModel/2017-08-31 | get_logger | INFO]: ====================AttentionModel/2017-08-31 start====================
[strategy_integration.py] set_seed(2017-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5568)training: 2/10 (0.5568)validation : 2/10 (0.5552)training: 3/10 (0.5552)validation : 3/10 (0.5552)training: 4/10 (0.5552)validation : 4/10 (0.5552)training: 5/10 (0.5552)validation : 5/10 (0.5552)early stopping at 5 with loss 0.5552
AttentionModel-training is done: 5/10
2017-07-31 | reset count: 0 | final loss: 0.5552 at epoch 2
making sample. (inference: True, date:2017-09-30 00:00:00, date_number: 369)
making sample. (inference: False, date:2017-08-31 00:00:00, date_number: 368)
[AttentionModel/2017-09-30 | get_logger | INFO]: ====================AttentionModel/2017-09-30 start====================
[strategy_integration.py] set_seed(2017-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5624)training: 2/10 (0.5624)validation : 2/10 (0.5578)training: 3/10 (0.5578)validation : 3/10 (0.5578)training: 4/10 (0.5578)validation : 4/10 (0.5564)training: 5/10 (0.5564)validation : 5/10 (0.5564)training: 6/10 (0.5564)validation : 6/10 (0.5564)training: 7/10 (0.5564)validation : 7/10 (0.5564)early stopping at 7 with loss 0.5564
AttentionModel-training is done: 7/10
2017-08-31 | reset count: 0 | final loss: 0.5564 at epoch 4
making sample. (inference: True, date:2017-10-31 00:00:00, date_number: 370)
making sample. (inference: False, date:2017-09-30 00:00:00, date_number: 369)
[AttentionModel/2017-10-31 | get_logger | INFO]: ====================AttentionModel/2017-10-31 start====================
[strategy_integration.py] set_seed(2017-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5640)training: 2/10 (0.5640)validation : 2/10 (0.5640)training: 3/10 (0.5640)validation : 3/10 (0.5610)training: 4/10 (0.5610)validation : 4/10 (0.5610)training: 5/10 (0.5610)validation : 5/10 (0.5610)training: 6/10 (0.5610)validation : 6/10 (0.5610)early stopping at 6 with loss 0.5610
AttentionModel-training is done: 6/10
2017-09-30 | reset count: 0 | final loss: 0.5610 at epoch 3
making sample. (inference: True, date:2017-11-30 00:00:00, date_number: 371)
making sample. (inference: False, date:2017-10-31 00:00:00, date_number: 370)
[AttentionModel/2017-11-30 | get_logger | INFO]: ====================AttentionModel/2017-11-30 start====================
[strategy_integration.py] set_seed(2017-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5534)training: 2/10 (0.5534)validation : 2/10 (0.5534)training: 3/10 (0.5534)validation : 3/10 (0.5526)training: 4/10 (0.5526)validation : 4/10 (0.5526)training: 5/10 (0.5526)validation : 5/10 (0.5515)training: 6/10 (0.5515)validation : 6/10 (0.5515)training: 7/10 (0.5515)validation : 7/10 (0.5515)early stopping at 7 with loss 0.5515
AttentionModel-training is done: 7/10
2017-10-31 | reset count: 0 | final loss: 0.5515 at epoch 5
[strategy_integration.py] set_seed(2017-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5643)training: 2/10 (0.5643)validation : 2/10 (0.5547)training: 3/10 (0.5547)validation : 3/10 (0.5547)training: 4/10 (0.5547)validation : 4/10 (0.5547)training: 5/10 (0.5547)validation : 5/10 (0.5547)early stopping at 5 with loss 0.5547
AttentionModel-training is done: 5/10
2017-11-30 | reset count: 0 | final loss: 0.5547 at epoch 2
making sample. (inference: True, date:2017-12-31 00:00:00, date_number: 372)
making sample. (inference: False, date:2017-11-30 00:00:00, date_number: 371)
[AttentionModel/2017-12-31 | get_logger | INFO]: ====================AttentionModel/2017-12-31 start====================
making sample. (inference: True, date:2018-01-31 00:00:00, date_number: 373)
making sample. (inference: False, date:2017-12-31 00:00:00, date_number: 372)
[AttentionModel/2018-01-31 | get_logger | INFO]: ====================AttentionModel/2018-01-31 start====================
[strategy_integration.py] set_seed(2017-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5671)training: 2/10 (0.5671)validation : 2/10 (0.5671)training: 3/10 (0.5671)validation : 3/10 (0.5665)training: 4/10 (0.5665)validation : 4/10 (0.5665)training: 5/10 (0.5665)validation : 5/10 (0.5665)early stopping at 5 with loss 0.5665
AttentionModel-training is done: 5/10
2017-12-31 | reset count: 0 | final loss: 0.5665 at epoch 3
making sample. (inference: True, date:2018-02-28 00:00:00, date_number: 374)
making sample. (inference: False, date:2018-01-31 00:00:00, date_number: 373)
[AttentionModel/2018-02-28 | get_logger | INFO]: ====================AttentionModel/2018-02-28 start====================
[strategy_integration.py] set_seed(2018-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5652)training: 2/10 (0.5652)validation : 2/10 (0.5632)training: 3/10 (0.5632)validation : 3/10 (0.5632)training: 4/10 (0.5632)validation : 4/10 (0.5632)training: 5/10 (0.5632)validation : 5/10 (0.5632)early stopping at 5 with loss 0.5632
AttentionModel-training is done: 5/10
2018-01-31 | reset count: 0 | final loss: 0.5632 at epoch 2
making sample. (inference: True, date:2018-03-31 00:00:00, date_number: 375)
making sample. (inference: False, date:2018-02-28 00:00:00, date_number: 374)
[AttentionModel/2018-03-31 | get_logger | INFO]: ====================AttentionModel/2018-03-31 start====================
[strategy_integration.py] set_seed(2018-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5688)training: 2/10 (0.5688)validation : 2/10 (0.5640)training: 3/10 (0.5640)validation : 3/10 (0.5640)training: 4/10 (0.5640)validation : 4/10 (0.5638)training: 5/10 (0.5638)validation : 5/10 (0.5638)training: 6/10 (0.5638)validation : 6/10 (0.5638)training: 7/10 (0.5638)validation : 7/10 (0.5638)early stopping at 7 with loss 0.5638
AttentionModel-training is done: 7/10
2018-02-28 | reset count: 0 | final loss: 0.5638 at epoch 4
making sample. (inference: True, date:2018-04-30 00:00:00, date_number: 376)
making sample. (inference: False, date:2018-03-31 00:00:00, date_number: 375)
[AttentionModel/2018-04-30 | get_logger | INFO]: ====================AttentionModel/2018-04-30 start====================
[strategy_integration.py] set_seed(2018-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5620)training: 2/10 (0.5620)validation : 2/10 (0.5619)training: 3/10 (0.5619)validation : 3/10 (0.5619)training: 4/10 (0.5619)validation : 4/10 (0.5572)training: 5/10 (0.5572)validation : 5/10 (0.5572)training: 6/10 (0.5572)validation : 6/10 (0.5572)training: 7/10 (0.5572)validation : 7/10 (0.5572)early stopping at 7 with loss 0.5572
AttentionModel-training is done: 7/10
2018-03-31 | reset count: 0 | final loss: 0.5572 at epoch 4
making sample. (inference: True, date:2018-05-31 00:00:00, date_number: 377)
making sample. (inference: False, date:2018-04-30 00:00:00, date_number: 376)
[AttentionModel/2018-05-31 | get_logger | INFO]: ====================AttentionModel/2018-05-31 start====================
[strategy_integration.py] set_seed(2018-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5703)training: 2/10 (0.5703)validation : 2/10 (0.5665)training: 3/10 (0.5665)validation : 3/10 (0.5665)training: 4/10 (0.5665)validation : 4/10 (0.5665)training: 5/10 (0.5665)validation : 5/10 (0.5665)early stopping at 5 with loss 0.5665
AttentionModel-training is done: 5/10
2018-04-30 | reset count: 0 | final loss: 0.5665 at epoch 2
making sample. (inference: True, date:2018-06-30 00:00:00, date_number: 378)
making sample. (inference: False, date:2018-05-31 00:00:00, date_number: 377)
[AttentionModel/2018-06-30 | get_logger | INFO]: ====================AttentionModel/2018-06-30 start====================
[strategy_integration.py] set_seed(2018-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5612)training: 2/10 (0.5612)validation : 2/10 (0.5569)training: 3/10 (0.5569)validation : 3/10 (0.5569)training: 4/10 (0.5569)validation : 4/10 (0.5569)training: 5/10 (0.5569)validation : 5/10 (0.5569)early stopping at 5 with loss 0.5569
AttentionModel-training is done: 5/10
2018-05-31 | reset count: 0 | final loss: 0.5569 at epoch 2
making sample. (inference: True, date:2018-07-31 00:00:00, date_number: 379)
making sample. (inference: False, date:2018-06-30 00:00:00, date_number: 378)
[AttentionModel/2018-07-31 | get_logger | INFO]: ====================AttentionModel/2018-07-31 start====================
[strategy_integration.py] set_seed(2018-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5672)training: 2/10 (0.5672)validation : 2/10 (0.5672)training: 3/10 (0.5672)validation : 3/10 (0.5672)training: 4/10 (0.5672)validation : 4/10 (0.5661)training: 5/10 (0.5661)validation : 5/10 (0.5629)training: 6/10 (0.5629)validation : 6/10 (0.5629)training: 7/10 (0.5629)validation : 7/10 (0.5629)early stopping at 7 with loss 0.5629
AttentionModel-training is done: 7/10
2018-06-30 | reset count: 0 | final loss: 0.5629 at epoch 5
making sample. (inference: True, date:2018-08-31 00:00:00, date_number: 380)
making sample. (inference: False, date:2018-07-31 00:00:00, date_number: 379)
[AttentionModel/2018-08-31 | get_logger | INFO]: ====================AttentionModel/2018-08-31 start====================
[strategy_integration.py] set_seed(2018-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5601)training: 2/10 (0.5601)validation : 2/10 (0.5547)training: 3/10 (0.5547)validation : 3/10 (0.5532)training: 4/10 (0.5532)validation : 4/10 (0.5532)training: 5/10 (0.5532)validation : 5/10 (0.5532)training: 6/10 (0.5532)validation : 6/10 (0.5532)early stopping at 6 with loss 0.5532
AttentionModel-training is done: 6/10
2018-07-31 | reset count: 0 | final loss: 0.5532 at epoch 3
making sample. (inference: True, date:2018-09-30 00:00:00, date_number: 381)
making sample. (inference: False, date:2018-08-31 00:00:00, date_number: 380)
[AttentionModel/2018-09-30 | get_logger | INFO]: ====================AttentionModel/2018-09-30 start====================
[strategy_integration.py] set_seed(2018-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5789)training: 2/10 (0.5789)validation : 2/10 (0.5789)training: 3/10 (0.5789)validation : 3/10 (0.5786)training: 4/10 (0.5786)validation : 4/10 (0.5786)training: 5/10 (0.5786)validation : 5/10 (0.5754)training: 6/10 (0.5754)validation : 6/10 (0.5754)training: 7/10 (0.5754)validation : 7/10 (0.5754)early stopping at 7 with loss 0.5754
AttentionModel-training is done: 7/10
2018-08-31 | reset count: 0 | final loss: 0.5754 at epoch 5
making sample. (inference: True, date:2018-10-31 00:00:00, date_number: 382)
making sample. (inference: False, date:2018-09-30 00:00:00, date_number: 381)
[AttentionModel/2018-10-31 | get_logger | INFO]: ====================AttentionModel/2018-10-31 start====================
[strategy_integration.py] set_seed(2018-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5758)training: 2/10 (0.5758)validation : 2/10 (0.5758)training: 3/10 (0.5758)validation : 3/10 (0.5715)training: 4/10 (0.5715)validation : 4/10 (0.5715)training: 5/10 (0.5715)validation : 5/10 (0.5715)training: 6/10 (0.5715)validation : 6/10 (0.5688)training: 7/10 (0.5688)validation : 7/10 (0.5688)training: 8/10 (0.5688)validation : 8/10 (0.5688)training: 9/10 (0.5688)validation : 9/10 (0.5688)early stopping at 9 with loss 0.5688
AttentionModel-training is done: 9/10
2018-09-30 | reset count: 0 | final loss: 0.5688 at epoch 6
[strategy_integration.py] set_seed(2018-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5804)training: 2/10 (0.5804)validation : 2/10 (0.5804)training: 3/10 (0.5804)validation : 3/10 (0.5765)training: 4/10 (0.5765)validation : 4/10 (0.5765)training: 5/10 (0.5765)validation : 5/10 (0.5765)early stopping at 5 with loss 0.5765
AttentionModel-training is done: 5/10
2018-10-31 | reset count: 0 | final loss: 0.5765 at epoch 3
making sample. (inference: True, date:2018-11-30 00:00:00, date_number: 383)
making sample. (inference: False, date:2018-10-31 00:00:00, date_number: 382)
[AttentionModel/2018-11-30 | get_logger | INFO]: ====================AttentionModel/2018-11-30 start====================
making sample. (inference: True, date:2018-12-31 00:00:00, date_number: 384)
making sample. (inference: False, date:2018-11-30 00:00:00, date_number: 383)
[AttentionModel/2018-12-31 | get_logger | INFO]: ====================AttentionModel/2018-12-31 start====================
[strategy_integration.py] set_seed(2018-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5703)training: 2/10 (0.5703)validation : 2/10 (0.5699)training: 3/10 (0.5699)validation : 3/10 (0.5680)training: 4/10 (0.5680)validation : 4/10 (0.5662)training: 5/10 (0.5662)validation : 5/10 (0.5662)training: 6/10 (0.5662)validation : 6/10 (0.5649)training: 7/10 (0.5649)validation : 7/10 (0.5649)early stopping at 7 with loss 0.5649
AttentionModel-training is done: 7/10
2018-11-30 | reset count: 0 | final loss: 0.5649 at epoch 6
making sample. (inference: True, date:2019-01-31 00:00:00, date_number: 385)
making sample. (inference: False, date:2018-12-31 00:00:00, date_number: 384)
[AttentionModel/2019-01-31 | get_logger | INFO]: ====================AttentionModel/2019-01-31 start====================
[strategy_integration.py] set_seed(2018-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5825)training: 2/10 (0.5825)validation : 2/10 (0.5808)training: 3/10 (0.5808)validation : 3/10 (0.5802)training: 4/10 (0.5802)validation : 4/10 (0.5801)training: 5/10 (0.5801)validation : 5/10 (0.5795)training: 6/10 (0.5795)validation : 6/10 (0.5787)training: 7/10 (0.5787)validation : 7/10 (0.5769)training: 8/10 (0.5769)validation : 8/10 (0.5769)training: 9/10 (0.5769)validation : 9/10 (0.5766)training: 10/10 (0.5766)validation : 10/10 (0.5766)early stopping at 10 with loss 0.5766
AttentionModel-training is done: 10/10
2018-12-31 | reset count: 0 | final loss: 0.5766 at epoch 9
[strategy_integration.py] set_seed(2019-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5689)training: 2/10 (0.5689)validation : 2/10 (0.5689)training: 3/10 (0.5689)validation : 3/10 (0.5689)training: 4/10 (0.5689)validation : 4/10 (0.5689)training: 5/10 (0.5689)validation : 5/10 (0.5681)training: 6/10 (0.5681)validation : 6/10 (0.5681)training: 7/10 (0.5681)validation : 7/10 (0.5681)early stopping at 7 with loss 0.5681
AttentionModel-training is done: 7/10
2019-01-31 | reset count: 0 | final loss: 0.5681 at epoch 5
making sample. (inference: True, date:2019-02-28 00:00:00, date_number: 386)
making sample. (inference: False, date:2019-01-31 00:00:00, date_number: 385)
[AttentionModel/2019-02-28 | get_logger | INFO]: ====================AttentionModel/2019-02-28 start====================
making sample. (inference: True, date:2019-03-31 00:00:00, date_number: 387)
making sample. (inference: False, date:2019-02-28 00:00:00, date_number: 386)
[AttentionModel/2019-03-31 | get_logger | INFO]: ====================AttentionModel/2019-03-31 start====================
[strategy_integration.py] set_seed(2019-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5770)training: 2/10 (0.5770)validation : 2/10 (0.5741)training: 3/10 (0.5741)validation : 3/10 (0.5734)training: 4/10 (0.5734)validation : 4/10 (0.5734)training: 5/10 (0.5734)validation : 5/10 (0.5734)early stopping at 5 with loss 0.5734
AttentionModel-training is done: 5/10
2019-02-28 | reset count: 0 | final loss: 0.5734 at epoch 5
making sample. (inference: True, date:2019-04-30 00:00:00, date_number: 388)
making sample. (inference: False, date:2019-03-31 00:00:00, date_number: 387)
[AttentionModel/2019-04-30 | get_logger | INFO]: ====================AttentionModel/2019-04-30 start====================
[strategy_integration.py] set_seed(2019-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5931)training: 2/10 (0.5931)validation : 2/10 (0.5926)training: 3/10 (0.5926)validation : 3/10 (0.5923)training: 4/10 (0.5923)validation : 4/10 (0.5895)training: 5/10 (0.5895)validation : 5/10 (0.5895)training: 6/10 (0.5895)validation : 6/10 (0.5895)training: 7/10 (0.5895)validation : 7/10 (0.5895)early stopping at 7 with loss 0.5895
AttentionModel-training is done: 7/10
2019-03-31 | reset count: 0 | final loss: 0.5895 at epoch 4
making sample. (inference: True, date:2019-05-31 00:00:00, date_number: 389)
making sample. (inference: False, date:2019-04-30 00:00:00, date_number: 388)
[AttentionModel/2019-05-31 | get_logger | INFO]: ====================AttentionModel/2019-05-31 start====================
[strategy_integration.py] set_seed(2019-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5790)training: 2/10 (0.5790)validation : 2/10 (0.5744)training: 3/10 (0.5744)validation : 3/10 (0.5744)training: 4/10 (0.5744)validation : 4/10 (0.5744)training: 5/10 (0.5744)validation : 5/10 (0.5744)early stopping at 5 with loss 0.5744
AttentionModel-training is done: 5/10
2019-04-30 | reset count: 0 | final loss: 0.5744 at epoch 2
making sample. (inference: True, date:2019-06-30 00:00:00, date_number: 390)
making sample. (inference: False, date:2019-05-31 00:00:00, date_number: 389)
[AttentionModel/2019-06-30 | get_logger | INFO]: ====================AttentionModel/2019-06-30 start====================
[strategy_integration.py] set_seed(2019-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5782)training: 2/10 (0.5782)validation : 2/10 (0.5782)training: 3/10 (0.5782)validation : 3/10 (0.5756)training: 4/10 (0.5756)validation : 4/10 (0.5756)training: 5/10 (0.5756)validation : 5/10 (0.5756)training: 6/10 (0.5756)validation : 6/10 (0.5754)training: 7/10 (0.5754)validation : 7/10 (0.5754)training: 8/10 (0.5754)validation : 8/10 (0.5751)training: 9/10 (0.5751)validation : 9/10 (0.5751)early stopping at 9 with loss 0.5751
AttentionModel-training is done: 9/10
2019-05-31 | reset count: 0 | final loss: 0.5751 at epoch 8
[strategy_integration.py] set_seed(2019-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5829)training: 2/10 (0.5829)validation : 2/10 (0.5829)training: 3/10 (0.5829)validation : 3/10 (0.5774)training: 4/10 (0.5774)validation : 4/10 (0.5770)training: 5/10 (0.5770)validation : 5/10 (0.5770)training: 6/10 (0.5770)validation : 6/10 (0.5770)early stopping at 6 with loss 0.5770
AttentionModel-training is done: 6/10
2019-06-30 | reset count: 0 | final loss: 0.5770 at epoch 4
making sample. (inference: True, date:2019-07-31 00:00:00, date_number: 391)
making sample. (inference: False, date:2019-06-30 00:00:00, date_number: 390)
[AttentionModel/2019-07-31 | get_logger | INFO]: ====================AttentionModel/2019-07-31 start====================
making sample. (inference: True, date:2019-08-31 00:00:00, date_number: 392)
making sample. (inference: False, date:2019-07-31 00:00:00, date_number: 391)
[AttentionModel/2019-08-31 | get_logger | INFO]: ====================AttentionModel/2019-08-31 start====================
[strategy_integration.py] set_seed(2019-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5809)training: 2/10 (0.5809)validation : 2/10 (0.5809)training: 3/10 (0.5809)validation : 3/10 (0.5790)training: 4/10 (0.5790)validation : 4/10 (0.5788)training: 5/10 (0.5788)validation : 5/10 (0.5788)training: 6/10 (0.5788)validation : 6/10 (0.5788)training: 7/10 (0.5788)validation : 7/10 (0.5788)early stopping at 7 with loss 0.5788
AttentionModel-training is done: 7/10
2019-07-31 | reset count: 0 | final loss: 0.5788 at epoch 4
[strategy_integration.py] set_seed(2019-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5799)training: 2/10 (0.5799)validation : 2/10 (0.5779)training: 3/10 (0.5779)validation : 3/10 (0.5779)training: 4/10 (0.5779)validation : 4/10 (0.5779)training: 5/10 (0.5779)validation : 5/10 (0.5779)early stopping at 5 with loss 0.5779
AttentionModel-training is done: 5/10
2019-08-31 | reset count: 0 | final loss: 0.5779 at epoch 2
making sample. (inference: True, date:2019-09-30 00:00:00, date_number: 393)
making sample. (inference: False, date:2019-08-31 00:00:00, date_number: 392)
[AttentionModel/2019-09-30 | get_logger | INFO]: ====================AttentionModel/2019-09-30 start====================
making sample. (inference: True, date:2019-10-31 00:00:00, date_number: 394)
making sample. (inference: False, date:2019-09-30 00:00:00, date_number: 393)
[AttentionModel/2019-10-31 | get_logger | INFO]: ====================AttentionModel/2019-10-31 start====================
[strategy_integration.py] set_seed(2019-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5898)training: 2/10 (0.5898)validation : 2/10 (0.5866)training: 3/10 (0.5866)validation : 3/10 (0.5866)training: 4/10 (0.5866)validation : 4/10 (0.5833)training: 5/10 (0.5833)validation : 5/10 (0.5833)training: 6/10 (0.5833)validation : 6/10 (0.5833)training: 7/10 (0.5833)validation : 7/10 (0.5833)early stopping at 7 with loss 0.5833
AttentionModel-training is done: 7/10
2019-09-30 | reset count: 0 | final loss: 0.5833 at epoch 4
[strategy_integration.py] set_seed(2019-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5787)training: 2/10 (0.5787)validation : 2/10 (0.5762)training: 3/10 (0.5762)validation : 3/10 (0.5757)training: 4/10 (0.5757)validation : 4/10 (0.5757)training: 5/10 (0.5757)validation : 5/10 (0.5757)early stopping at 5 with loss 0.5757
AttentionModel-training is done: 5/10
2019-10-31 | reset count: 0 | final loss: 0.5757 at epoch 3
making sample. (inference: True, date:2019-11-30 00:00:00, date_number: 395)
making sample. (inference: False, date:2019-10-31 00:00:00, date_number: 394)
[AttentionModel/2019-11-30 | get_logger | INFO]: ====================AttentionModel/2019-11-30 start====================
making sample. (inference: True, date:2019-12-31 00:00:00, date_number: 396)
making sample. (inference: False, date:2019-11-30 00:00:00, date_number: 395)
[AttentionModel/2019-12-31 | get_logger | INFO]: ====================AttentionModel/2019-12-31 start====================
[strategy_integration.py] set_seed(2019-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5866)training: 2/10 (0.5866)validation : 2/10 (0.5866)training: 3/10 (0.5866)validation : 3/10 (0.5838)training: 4/10 (0.5838)validation : 4/10 (0.5835)training: 5/10 (0.5835)validation : 5/10 (0.5835)training: 6/10 (0.5835)validation : 6/10 (0.5799)training: 7/10 (0.5799)validation : 7/10 (0.5798)training: 8/10 (0.5798)validation : 8/10 (0.5798)early stopping at 8 with loss 0.5798
AttentionModel-training is done: 8/10
2019-11-30 | reset count: 0 | final loss: 0.5798 at epoch 7
[strategy_integration.py] set_seed(2019-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5854)training: 2/10 (0.5854)validation : 2/10 (0.5814)training: 3/10 (0.5814)validation : 3/10 (0.5787)training: 4/10 (0.5787)validation : 4/10 (0.5787)training: 5/10 (0.5787)validation : 5/10 (0.5787)early stopping at 5 with loss 0.5787
AttentionModel-training is done: 5/10
2019-12-31 | reset count: 0 | final loss: 0.5787 at epoch 3
making sample. (inference: True, date:2020-01-31 00:00:00, date_number: 397)
making sample. (inference: False, date:2019-12-31 00:00:00, date_number: 396)
[AttentionModel/2020-01-31 | get_logger | INFO]: ====================AttentionModel/2020-01-31 start====================
making sample. (inference: True, date:2020-02-29 00:00:00, date_number: 398)
making sample. (inference: False, date:2020-01-31 00:00:00, date_number: 397)
[AttentionModel/2020-02-29 | get_logger | INFO]: ====================AttentionModel/2020-02-29 start====================
[strategy_integration.py] set_seed(2020-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5867)training: 2/10 (0.5867)validation : 2/10 (0.5771)training: 3/10 (0.5771)validation : 3/10 (0.5771)training: 4/10 (0.5771)validation : 4/10 (0.5771)training: 5/10 (0.5771)validation : 5/10 (0.5767)training: 6/10 (0.5767)validation : 6/10 (0.5767)training: 7/10 (0.5767)validation : 7/10 (0.5767)training: 8/10 (0.5767)validation : 8/10 (0.5767)early stopping at 8 with loss 0.5767
AttentionModel-training is done: 8/10
2020-01-31 | reset count: 0 | final loss: 0.5767 at epoch 5
[strategy_integration.py] set_seed(2020-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5989)training: 2/10 (0.5989)validation : 2/10 (0.5989)training: 3/10 (0.5989)validation : 3/10 (0.5989)training: 4/10 (0.5989)validation : 4/10 (0.5938)training: 5/10 (0.5938)validation : 5/10 (0.5938)training: 6/10 (0.5938)validation : 6/10 (0.5938)training: 7/10 (0.5938)validation : 7/10 (0.5938)early stopping at 7 with loss 0.5938
AttentionModel-training is done: 7/10
2020-02-29 | reset count: 0 | final loss: 0.5938 at epoch 4
making sample. (inference: True, date:2020-03-31 00:00:00, date_number: 399)
making sample. (inference: False, date:2020-02-29 00:00:00, date_number: 398)
[AttentionModel/2020-03-31 | get_logger | INFO]: ====================AttentionModel/2020-03-31 start====================
making sample. (inference: True, date:2020-04-30 00:00:00, date_number: 400)
making sample. (inference: False, date:2020-03-31 00:00:00, date_number: 399)
[AttentionModel/2020-04-30 | get_logger | INFO]: ====================AttentionModel/2020-04-30 start====================
[strategy_integration.py] set_seed(2020-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5730)training: 2/10 (0.5730)validation : 2/10 (0.5708)training: 3/10 (0.5708)validation : 3/10 (0.5708)training: 4/10 (0.5708)validation : 4/10 (0.5708)training: 5/10 (0.5708)validation : 5/10 (0.5708)early stopping at 5 with loss 0.5708
AttentionModel-training is done: 5/10
2020-03-31 | reset count: 0 | final loss: 0.5708 at epoch 2
making sample. (inference: True, date:2020-05-31 00:00:00, date_number: 401)
making sample. (inference: False, date:2020-04-30 00:00:00, date_number: 400)
[strategy_integration.py] set_seed(2020-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5741)training: 2/10 (0.5741)validation : 2/10 (0.5709)training: 3/10 (0.5709)validation : 3/10 (0.5709)training: 4/10 (0.5709)validation : 4/10 (0.5709)training: 5/10 (0.5709)validation : 5/10 (0.5709)early stopping at 5 with loss 0.5709
AttentionModel-training is done: 5/10
2020-04-30 | reset count: 0 | final loss: 0.5709 at epoch 2
[AttentionModel/2020-05-31 | get_logger | INFO]: ====================AttentionModel/2020-05-31 start====================
making sample. (inference: True, date:2020-06-30 00:00:00, date_number: 402)
making sample. (inference: False, date:2020-05-31 00:00:00, date_number: 401)
[AttentionModel/2020-06-30 | get_logger | INFO]: ====================AttentionModel/2020-06-30 start====================
[strategy_integration.py] set_seed(2020-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5791)training: 2/10 (0.5791)validation : 2/10 (0.5773)training: 3/10 (0.5773)validation : 3/10 (0.5750)training: 4/10 (0.5750)validation : 4/10 (0.5750)training: 5/10 (0.5750)validation : 5/10 (0.5750)training: 6/10 (0.5750)validation : 6/10 (0.5732)training: 7/10 (0.5732)validation : 7/10 (0.5730)training: 8/10 (0.5730)validation : 8/10 (0.5730)training: 9/10 (0.5730)validation : 9/10 (0.5730)early stopping at 9 with loss 0.5730
AttentionModel-training is done: 9/10
2020-05-31 | reset count: 0 | final loss: 0.5730 at epoch 7
[strategy_integration.py] set_seed(2020-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5823)training: 2/10 (0.5823)validation : 2/10 (0.5777)training: 3/10 (0.5777)validation : 3/10 (0.5777)training: 4/10 (0.5777)validation : 4/10 (0.5762)training: 5/10 (0.5762)validation : 5/10 (0.5762)training: 6/10 (0.5762)validation : 6/10 (0.5752)training: 7/10 (0.5752)validation : 7/10 (0.5752)early stopping at 7 with loss 0.5752
AttentionModel-training is done: 7/10
2020-06-30 | reset count: 0 | final loss: 0.5752 at epoch 6
making sample. (inference: True, date:2020-07-31 00:00:00, date_number: 403)
making sample. (inference: False, date:2020-06-30 00:00:00, date_number: 402)
[AttentionModel/2020-07-31 | get_logger | INFO]: ====================AttentionModel/2020-07-31 start====================
making sample. (inference: True, date:2020-08-31 00:00:00, date_number: 404)
making sample. (inference: False, date:2020-07-31 00:00:00, date_number: 403)
[AttentionModel/2020-08-31 | get_logger | INFO]: ====================AttentionModel/2020-08-31 start====================
[strategy_integration.py] set_seed(2020-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5850)training: 2/10 (0.5850)validation : 2/10 (0.5803)training: 3/10 (0.5803)validation : 3/10 (0.5803)training: 4/10 (0.5803)validation : 4/10 (0.5803)training: 5/10 (0.5803)validation : 5/10 (0.5803)training: 6/10 (0.5803)validation : 6/10 (0.5803)training: 7/10 (0.5803)validation : 7/10 (0.5795)training: 8/10 (0.5795)validation : 8/10 (0.5795)early stopping at 8 with loss 0.5795
AttentionModel-training is done: 8/10
2020-07-31 | reset count: 0 | final loss: 0.5795 at epoch 7
[strategy_integration.py] set_seed(2020-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5738)training: 2/10 (0.5738)validation : 2/10 (0.5738)training: 3/10 (0.5738)validation : 3/10 (0.5718)training: 4/10 (0.5718)validation : 4/10 (0.5718)training: 5/10 (0.5718)validation : 5/10 (0.5718)training: 6/10 (0.5718)validation : 6/10 (0.5718)early stopping at 6 with loss 0.5718
AttentionModel-training is done: 6/10
2020-08-31 | reset count: 0 | final loss: 0.5718 at epoch 3
making sample. (inference: True, date:2020-09-30 00:00:00, date_number: 405)
making sample. (inference: False, date:2020-08-31 00:00:00, date_number: 404)
[AttentionModel/2020-09-30 | get_logger | INFO]: ====================AttentionModel/2020-09-30 start====================
making sample. (inference: True, date:2020-10-31 00:00:00, date_number: 406)
making sample. (inference: False, date:2020-09-30 00:00:00, date_number: 405)
[AttentionModel/2020-10-31 | get_logger | INFO]: ====================AttentionModel/2020-10-31 start====================
[strategy_integration.py] set_seed(2020-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5829)training: 2/10 (0.5829)validation : 2/10 (0.5812)training: 3/10 (0.5812)validation : 3/10 (0.5809)training: 4/10 (0.5809)validation : 4/10 (0.5809)training: 5/10 (0.5809)validation : 5/10 (0.5809)training: 6/10 (0.5809)validation : 6/10 (0.5804)training: 7/10 (0.5804)validation : 7/10 (0.5793)training: 8/10 (0.5793)validation : 8/10 (0.5793)training: 9/10 (0.5793)validation : 9/10 (0.5793)early stopping at 9 with loss 0.5793
AttentionModel-training is done: 9/10
2020-09-30 | reset count: 0 | final loss: 0.5793 at epoch 7
[strategy_integration.py] set_seed(2020-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5672)training: 2/10 (0.5672)validation : 2/10 (0.5672)training: 3/10 (0.5672)validation : 3/10 (0.5672)training: 4/10 (0.5672)validation : 4/10 (0.5672)training: 5/10 (0.5672)validation : 5/10 (0.5672)early stopping at 5 with loss 0.5672
AttentionModel-training is done: 5/10
2020-10-31 | reset count: 0 | final loss: 0.5672 at epoch 1
making sample. (inference: True, date:2020-11-30 00:00:00, date_number: 407)
making sample. (inference: False, date:2020-10-31 00:00:00, date_number: 406)
[AttentionModel/2020-11-30 | get_logger | INFO]: ====================AttentionModel/2020-11-30 start====================
making sample. (inference: True, date:2020-12-31 00:00:00, date_number: 408)
making sample. (inference: False, date:2020-11-30 00:00:00, date_number: 407)
[AttentionModel/2020-12-31 | get_logger | INFO]: ====================AttentionModel/2020-12-31 start====================
[strategy_integration.py] set_seed(2020-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5784)training: 2/10 (0.5784)validation : 2/10 (0.5784)training: 3/10 (0.5784)validation : 3/10 (0.5773)training: 4/10 (0.5773)validation : 4/10 (0.5773)training: 5/10 (0.5773)validation : 5/10 (0.5773)training: 6/10 (0.5773)validation : 6/10 (0.5773)early stopping at 6 with loss 0.5773
AttentionModel-training is done: 6/10
2020-11-30 | reset count: 0 | final loss: 0.5773 at epoch 3
making sample. (inference: True, date:2021-01-31 00:00:00, date_number: 409)
making sample. (inference: False, date:2020-12-31 00:00:00, date_number: 408)
[AttentionModel/2021-01-31 | get_logger | INFO]: ====================AttentionModel/2021-01-31 start====================
[strategy_integration.py] set_seed(2020-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5718)training: 2/10 (0.5718)validation : 2/10 (0.5718)training: 3/10 (0.5718)validation : 3/10 (0.5718)training: 4/10 (0.5718)validation : 4/10 (0.5688)training: 5/10 (0.5688)validation : 5/10 (0.5688)training: 6/10 (0.5688)validation : 6/10 (0.5686)training: 7/10 (0.5686)validation : 7/10 (0.5686)early stopping at 7 with loss 0.5686
AttentionModel-training is done: 7/10
2020-12-31 | reset count: 0 | final loss: 0.5686 at epoch 6
making sample. (inference: True, date:2021-02-28 00:00:00, date_number: 410)
making sample. (inference: False, date:2021-01-31 00:00:00, date_number: 409)
[AttentionModel/2021-02-28 | get_logger | INFO]: ====================AttentionModel/2021-02-28 start====================
[strategy_integration.py] set_seed(2021-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5621)training: 2/10 (0.5621)validation : 2/10 (0.5591)training: 3/10 (0.5591)validation : 3/10 (0.5585)training: 4/10 (0.5585)validation : 4/10 (0.5585)training: 5/10 (0.5585)validation : 5/10 (0.5566)training: 6/10 (0.5566)validation : 6/10 (0.5566)training: 7/10 (0.5566)validation : 7/10 (0.5566)training: 8/10 (0.5566)validation : 8/10 (0.5566)early stopping at 8 with loss 0.5566
AttentionModel-training is done: 8/10
2021-01-31 | reset count: 0 | final loss: 0.5566 at epoch 5
[strategy_integration.py] set_seed(2021-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5611)training: 2/10 (0.5611)validation : 2/10 (0.5611)training: 3/10 (0.5611)validation : 3/10 (0.5611)training: 4/10 (0.5611)validation : 4/10 (0.5611)training: 5/10 (0.5611)validation : 5/10 (0.5611)early stopping at 5 with loss 0.5611
AttentionModel-training is done: 5/10
2021-02-28 | reset count: 0 | final loss: 0.5611 at epoch 1
making sample. (inference: True, date:2021-03-31 00:00:00, date_number: 411)
making sample. (inference: False, date:2021-02-28 00:00:00, date_number: 410)
[AttentionModel/2021-03-31 | get_logger | INFO]: ====================AttentionModel/2021-03-31 start====================
making sample. (inference: True, date:2021-04-30 00:00:00, date_number: 412)
making sample. (inference: False, date:2021-03-31 00:00:00, date_number: 411)
[AttentionModel/2021-04-30 | get_logger | INFO]: ====================AttentionModel/2021-04-30 start====================
[strategy_integration.py] set_seed(2021-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5594)training: 2/10 (0.5594)validation : 2/10 (0.5581)training: 3/10 (0.5581)validation : 3/10 (0.5581)training: 4/10 (0.5581)validation : 4/10 (0.5581)training: 5/10 (0.5581)validation : 5/10 (0.5528)training: 6/10 (0.5528)validation : 6/10 (0.5528)early stopping at 6 with loss 0.5528
AttentionModel-training is done: 6/10
2021-03-31 | reset count: 0 | final loss: 0.5528 at epoch 5
making sample. (inference: True, date:2021-05-31 00:00:00, date_number: 413)
making sample. (inference: False, date:2021-04-30 00:00:00, date_number: 412)
[AttentionModel/2021-05-31 | get_logger | INFO]: ====================AttentionModel/2021-05-31 start====================
[strategy_integration.py] set_seed(2021-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5613)training: 2/10 (0.5613)validation : 2/10 (0.5590)training: 3/10 (0.5590)validation : 3/10 (0.5590)training: 4/10 (0.5590)validation : 4/10 (0.5569)training: 5/10 (0.5569)validation : 5/10 (0.5561)training: 6/10 (0.5561)validation : 6/10 (0.5561)training: 7/10 (0.5561)validation : 7/10 (0.5542)training: 8/10 (0.5542)validation : 8/10 (0.5542)training: 9/10 (0.5542)validation : 9/10 (0.5542)early stopping at 9 with loss 0.5542
AttentionModel-training is done: 9/10
2021-04-30 | reset count: 0 | final loss: 0.5542 at epoch 7
making sample. (inference: True, date:2021-06-30 00:00:00, date_number: 414)
making sample. (inference: False, date:2021-05-31 00:00:00, date_number: 413)
[AttentionModel/2021-06-30 | get_logger | INFO]: ====================AttentionModel/2021-06-30 start====================
[strategy_integration.py] set_seed(2021-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5754)training: 2/10 (0.5754)validation : 2/10 (0.5682)training: 3/10 (0.5682)validation : 3/10 (0.5682)training: 4/10 (0.5682)validation : 4/10 (0.5682)training: 5/10 (0.5682)validation : 5/10 (0.5682)training: 6/10 (0.5682)validation : 6/10 (0.5662)training: 7/10 (0.5662)validation : 7/10 (0.5662)training: 8/10 (0.5662)validation : 8/10 (0.5662)early stopping at 8 with loss 0.5662
AttentionModel-training is done: 8/10
2021-05-31 | reset count: 0 | final loss: 0.5662 at epoch 6
making sample. (inference: True, date:2021-07-31 00:00:00, date_number: 415)
making sample. (inference: False, date:2021-06-30 00:00:00, date_number: 414)
[strategy_integration.py] set_seed(2021-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5746)training: 2/10 (0.5746)validation : 2/10 (0.5710)training: 3/10 (0.5710)validation : 3/10 (0.5710)training: 4/10 (0.5710)validation : 4/10 (0.5710)training: 5/10 (0.5710)validation : 5/10 (0.5680)training: 6/10 (0.5680)validation : 6/10 (0.5680)training: 7/10 (0.5680)validation : 7/10 (0.5678)training: 8/10 (0.5678)validation : 8/10 (0.5678)early stopping at 8 with loss 0.5678
AttentionModel-training is done: 8/10
2021-06-30 | reset count: 0 | final loss: 0.5678 at epoch 7
[AttentionModel/2021-07-31 | get_logger | INFO]: ====================AttentionModel/2021-07-31 start====================
making sample. (inference: True, date:2021-08-31 00:00:00, date_number: 416)
making sample. (inference: False, date:2021-07-31 00:00:00, date_number: 415)
[AttentionModel/2021-08-31 | get_logger | INFO]: ====================AttentionModel/2021-08-31 start====================
[strategy_integration.py] set_seed(2021-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5735)training: 2/10 (0.5735)validation : 2/10 (0.5701)training: 3/10 (0.5701)validation : 3/10 (0.5693)training: 4/10 (0.5693)validation : 4/10 (0.5685)training: 5/10 (0.5685)validation : 5/10 (0.5681)training: 6/10 (0.5681)validation : 6/10 (0.5681)early stopping at 6 with loss 0.5681
AttentionModel-training is done: 6/10
2021-07-31 | reset count: 0 | final loss: 0.5681 at epoch 5
making sample. (inference: True, date:2021-09-30 00:00:00, date_number: 417)
making sample. (inference: False, date:2021-08-31 00:00:00, date_number: 416)
[AttentionModel/2021-09-30 | get_logger | INFO]: ====================AttentionModel/2021-09-30 start====================
[strategy_integration.py] set_seed(2021-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5748)training: 2/10 (0.5748)validation : 2/10 (0.5745)training: 3/10 (0.5745)validation : 3/10 (0.5689)training: 4/10 (0.5689)validation : 4/10 (0.5688)training: 5/10 (0.5688)validation : 5/10 (0.5688)training: 6/10 (0.5688)validation : 6/10 (0.5688)early stopping at 6 with loss 0.5688
AttentionModel-training is done: 6/10
2021-08-31 | reset count: 0 | final loss: 0.5688 at epoch 4
making sample. (inference: True, date:2021-10-31 00:00:00, date_number: 418)
making sample. (inference: False, date:2021-09-30 00:00:00, date_number: 417)
[AttentionModel/2021-10-31 | get_logger | INFO]: ====================AttentionModel/2021-10-31 start====================
[strategy_integration.py] set_seed(2021-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5805)training: 2/10 (0.5805)validation : 2/10 (0.5772)training: 3/10 (0.5772)validation : 3/10 (0.5772)training: 4/10 (0.5772)validation : 4/10 (0.5760)training: 5/10 (0.5760)validation : 5/10 (0.5760)training: 6/10 (0.5760)validation : 6/10 (0.5758)training: 7/10 (0.5758)validation : 7/10 (0.5758)early stopping at 7 with loss 0.5758
AttentionModel-training is done: 7/10
2021-09-30 | reset count: 0 | final loss: 0.5758 at epoch 6
[strategy_integration.py] set_seed(2021-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5856)training: 2/10 (0.5856)validation : 2/10 (0.5795)training: 3/10 (0.5795)validation : 3/10 (0.5792)training: 4/10 (0.5792)validation : 4/10 (0.5786)training: 5/10 (0.5786)validation : 5/10 (0.5786)training: 6/10 (0.5786)validation : 6/10 (0.5786)early stopping at 6 with loss 0.5786
AttentionModel-training is done: 6/10
2021-10-31 | reset count: 0 | final loss: 0.5786 at epoch 4
[strategy | get_logger | INFO]: ====================strategy start====================
[strategy | _get_infer_dirs | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick108_test01/infer 이 strategy 대상에 추가됨
[strategy | _backtest | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick108_test01/infer is starting
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.158 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠹ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache⠦ [Compustat API] : Loading masking data⠦ [Compustat API] : Loading get_historic_universe from cache✓ [Compustat API] : Loading get_historic_universe from cache done in 1.933 secs
⠧ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.234 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.064 secs
⠋ [Compustat API] : Loading get_monthly_total_return from cache✓ [Compustat API] : Loading get_monthly_total_return from cache done in 0.062 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.076 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.072 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.07 secs
⠋ [Compustat API] : Loading get_monthly_volume_data from cache✓ [Compustat API] : Loading get_monthly_volume_data from cache done in 0.08 secs
[strategy | _allocating | INFO]: [STRATEGY] allocating start
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_weight_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] performance.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] combined.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] weight.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] universe.csv is saved.
[comparison | get_logger | INFO]: ====================comparison start====================
[comparison | compare | INFO]: [5. COMPARISON] comparison is saved.
[comparison | compare | INFO]:                        Harvest_Green_concept001_pick108_test01_price_return  ...  SPCOMP_total_win
2016-01-31_2021-04-30                                           0.242131     ...          0.765625

[1 rows x 27 columns]
2015-12-31
2016-01-31
2016-02-29
2016-03-31
2016-04-30
2016-05-31
2016-06-30
2016-07-31
2016-08-31
2016-09-30
2016-10-31
2016-11-30
2016-12-31
2017-01-31
2017-02-28
2017-03-31
2017-04-30
2017-05-31
2017-06-30
2017-07-31
2017-08-31
2017-09-30
2017-10-31
2017-11-30
2017-12-31
2018-01-31
2018-02-28
2018-03-31
2018-04-30
2018-05-31
2018-06-30
2018-07-31
2018-08-31
2018-09-30
2018-10-31
2018-11-30
2018-12-31
2019-01-31
2019-02-28
2019-03-31
2019-04-30
2019-05-31
2019-06-30
2019-07-31
2019-08-31
2019-09-30
2019-10-31
2019-11-30
2019-12-31
2020-01-31
2020-02-29
2020-03-31
2020-04-30
2020-05-31
2020-06-30
2020-07-31
2020-08-31
2020-09-30
2020-10-31
2020-11-30
2020-12-31
2021-01-31
2021-02-28
2021-03-31
2021-04-30
2021-05-31
2021-06-30
2021-07-31
2021-08-31
2021-09-30
2021-10-31
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.048 secs
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.045 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.063 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.079 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.073 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.659 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.675 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.679 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.674 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.676 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.076 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.665 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.667 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.665 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.659 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.658 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.078 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.68 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.074 secs
⠋ [Compustat API] : Cleaning past days cache✓ [Compustat API] : Cleaning past days cache done in 0.001 secs
auto caching이 활성화 되었습니다. /home/sronly/sr-storage/kirin_cache/kirin_api_cache/2.19.17_2021-11-23에 캐쉬를 저장합니다.
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.149 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading masking data✓ [Compustat API] : Loading get_historic_universe from cache done in 2.03 secs
⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.331 secs
setting tensorflow random seed failed
load_data: op_lev
load_data: roe
load_data: std_u_e
load_data: retail_mfr
load_data: m1
load_data: m2
load_data: res_mom_12m_1m_0m
load_data: res_vol_6m_3m_0m
load_data: at
load_data: mom_12m_1m
load_data: ram_12m_0m
load_data: vol_3m
load_data: t3y
load_data: t5y
load_data: t7y
load_data: aaa_t10y
load_data: baa_t10y
load_data: aaa_ff
load_data: real_pig
load_data: federal_tg
load_data: real_gdp
load_data: ae_m
load_data: ia_ta
load_data: rc_a
load_data: t1y_ff
load_data: t5y_ff
load_data: t10y_t2y
load_data: export_growth
load_data: import_growth
load_data: real_gig
load_data: ret_noa
load_data: etm
load_data: ia_mv
load_data: gpa
load_data: rev_surp
load_data: cash_at
load_data: baa_ff
load_data: core_cpi
load_data: core_pce
load_data: corporate_tg
load_data: industrial_prod
load_data: home_pr
load_data: pr_1m_0m
load_data: mv
load_data: btm
load_data: snp500_pr
load_data: wilshire500_pr
load_data: ted
load_data: ppi
load_data: trimmed_pce
load_data: unemploy
load_data: wti
load_data: capa_util
load_data: snp500_pe
load_data: r_s
load_data: r_a
load_data: fred_ff
load_data: t3m
load_data: t6m
load_data: t2y
load_data: t10y
load_data: aaa
load_data: baa
load_data: core_ppi
load_data: cpi
load_data: pce
load_data: snp500_vol
load_data: sector_values
load_data: mv
load_data: pr_1m_0m
load_data: mv
making sample. (inference: True, date:2015-12-31 00:00:00, date_number: 348)
making sample. (inference: False, date:2012-12-31 00:00:00, date_number: 312)
making sample. (inference: False, date:2013-01-31 00:00:00, date_number: 313)
making sample. (inference: False, date:2013-02-28 00:00:00, date_number: 314)
making sample. (inference: False, date:2013-03-31 00:00:00, date_number: 315)
making sample. (inference: False, date:2013-04-30 00:00:00, date_number: 316)
making sample. (inference: False, date:2013-05-31 00:00:00, date_number: 317)
making sample. (inference: False, date:2013-06-30 00:00:00, date_number: 318)
making sample. (inference: False, date:2013-07-31 00:00:00, date_number: 319)
making sample. (inference: False, date:2013-08-31 00:00:00, date_number: 320)
making sample. (inference: False, date:2013-09-30 00:00:00, date_number: 321)
making sample. (inference: False, date:2013-10-31 00:00:00, date_number: 322)
making sample. (inference: False, date:2013-11-30 00:00:00, date_number: 323)
making sample. (inference: False, date:2013-12-31 00:00:00, date_number: 324)
making sample. (inference: False, date:2014-01-31 00:00:00, date_number: 325)
making sample. (inference: False, date:2014-02-28 00:00:00, date_number: 326)
making sample. (inference: False, date:2014-03-31 00:00:00, date_number: 327)
making sample. (inference: False, date:2014-04-30 00:00:00, date_number: 328)
making sample. (inference: False, date:2014-05-31 00:00:00, date_number: 329)
making sample. (inference: False, date:2014-06-30 00:00:00, date_number: 330)
making sample. (inference: False, date:2014-07-31 00:00:00, date_number: 331)
making sample. (inference: False, date:2014-08-31 00:00:00, date_number: 332)
making sample. (inference: False, date:2014-09-30 00:00:00, date_number: 333)
making sample. (inference: False, date:2014-10-31 00:00:00, date_number: 334)
making sample. (inference: False, date:2014-11-30 00:00:00, date_number: 335)
making sample. (inference: False, date:2014-12-31 00:00:00, date_number: 336)
making sample. (inference: False, date:2015-01-31 00:00:00, date_number: 337)
making sample. (inference: False, date:2015-02-28 00:00:00, date_number: 338)
making sample. (inference: False, date:2015-03-31 00:00:00, date_number: 339)
making sample. (inference: False, date:2015-04-30 00:00:00, date_number: 340)
making sample. (inference: False, date:2015-05-31 00:00:00, date_number: 341)
making sample. (inference: False, date:2015-06-30 00:00:00, date_number: 342)
making sample. (inference: False, date:2015-07-31 00:00:00, date_number: 343)
making sample. (inference: False, date:2015-08-31 00:00:00, date_number: 344)
making sample. (inference: False, date:2015-09-30 00:00:00, date_number: 345)
making sample. (inference: False, date:2015-10-31 00:00:00, date_number: 346)
making sample. (inference: False, date:2015-11-30 00:00:00, date_number: 347)
[AttentionModel/2015-12-31 | get_logger | INFO]: ====================AttentionModel/2015-12-31 start====================
making sample. (inference: True, date:2016-01-31 00:00:00, date_number: 349)
making sample. (inference: False, date:2015-12-31 00:00:00, date_number: 348)
[AttentionModel/2016-01-31 | get_logger | INFO]: ====================AttentionModel/2016-01-31 start====================
[strategy_integration.py] set_seed(2015-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5762)training: 2/10 (0.5762)validation : 2/10 (0.5728)training: 3/10 (0.5728)validation : 3/10 (0.5728)training: 4/10 (0.5728)validation : 4/10 (0.5706)training: 5/10 (0.5706)validation : 5/10 (0.5706)training: 6/10 (0.5706)validation : 6/10 (0.5706)training: 7/10 (0.5706)validation : 7/10 (0.5692)training: 8/10 (0.5692)validation : 8/10 (0.5692)training: 9/10 (0.5692)validation : 9/10 (0.5692)training: 10/10 (0.5692)validation : 10/10 (0.5692)early stopping at 10 with loss 0.5692
AttentionModel-training is done: 10/10
2015-12-31 | reset count: 0 | final loss: 0.5692 at epoch 7
[strategy_integration.py] set_seed(2016-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5865)training: 2/10 (0.5865)validation : 2/10 (0.5824)training: 3/10 (0.5824)validation : 3/10 (0.5824)training: 4/10 (0.5824)validation : 4/10 (0.5824)training: 5/10 (0.5824)validation : 5/10 (0.5824)early stopping at 5 with loss 0.5824
AttentionModel-training is done: 5/10
2016-01-31 | reset count: 0 | final loss: 0.5824 at epoch 2
making sample. (inference: True, date:2016-02-29 00:00:00, date_number: 350)
making sample. (inference: False, date:2016-01-31 00:00:00, date_number: 349)
[AttentionModel/2016-02-29 | get_logger | INFO]: ====================AttentionModel/2016-02-29 start====================
making sample. (inference: True, date:2016-03-31 00:00:00, date_number: 351)
making sample. (inference: False, date:2016-02-29 00:00:00, date_number: 350)
[AttentionModel/2016-03-31 | get_logger | INFO]: ====================AttentionModel/2016-03-31 start====================
[strategy_integration.py] set_seed(2016-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5795)training: 2/10 (0.5795)validation : 2/10 (0.5755)training: 3/10 (0.5755)validation : 3/10 (0.5720)training: 4/10 (0.5720)validation : 4/10 (0.5720)training: 5/10 (0.5720)validation : 5/10 (0.5720)training: 6/10 (0.5720)validation : 6/10 (0.5720)early stopping at 6 with loss 0.5720
AttentionModel-training is done: 6/10
2016-02-29 | reset count: 0 | final loss: 0.5720 at epoch 3
[strategy_integration.py] set_seed(2016-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5770)training: 2/10 (0.5770)validation : 2/10 (0.5740)training: 3/10 (0.5740)validation : 3/10 (0.5715)training: 4/10 (0.5715)validation : 4/10 (0.5715)training: 5/10 (0.5715)validation : 5/10 (0.5715)early stopping at 5 with loss 0.5715
AttentionModel-training is done: 5/10
2016-03-31 | reset count: 0 | final loss: 0.5715 at epoch 3
making sample. (inference: True, date:2016-04-30 00:00:00, date_number: 352)
making sample. (inference: False, date:2016-03-31 00:00:00, date_number: 351)
[AttentionModel/2016-04-30 | get_logger | INFO]: ====================AttentionModel/2016-04-30 start====================
making sample. (inference: True, date:2016-05-31 00:00:00, date_number: 353)
making sample. (inference: False, date:2016-04-30 00:00:00, date_number: 352)
[AttentionModel/2016-05-31 | get_logger | INFO]: ====================AttentionModel/2016-05-31 start====================
[strategy_integration.py] set_seed(2016-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5786)training: 2/10 (0.5786)validation : 2/10 (0.5786)training: 3/10 (0.5786)validation : 3/10 (0.5777)training: 4/10 (0.5777)validation : 4/10 (0.5757)training: 5/10 (0.5757)validation : 5/10 (0.5757)training: 6/10 (0.5757)validation : 6/10 (0.5732)training: 7/10 (0.5732)validation : 7/10 (0.5732)early stopping at 7 with loss 0.5732
AttentionModel-training is done: 7/10
2016-04-30 | reset count: 0 | final loss: 0.5732 at epoch 6
[strategy_integration.py] set_seed(2016-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5724)training: 2/10 (0.5724)validation : 2/10 (0.5710)training: 3/10 (0.5710)validation : 3/10 (0.5682)training: 4/10 (0.5682)validation : 4/10 (0.5682)training: 5/10 (0.5682)validation : 5/10 (0.5682)early stopping at 5 with loss 0.5682
AttentionModel-training is done: 5/10
2016-05-31 | reset count: 0 | final loss: 0.5682 at epoch 3
making sample. (inference: True, date:2016-06-30 00:00:00, date_number: 354)
making sample. (inference: False, date:2016-05-31 00:00:00, date_number: 353)
[AttentionModel/2016-06-30 | get_logger | INFO]: ====================AttentionModel/2016-06-30 start====================
making sample. (inference: True, date:2016-07-31 00:00:00, date_number: 355)
making sample. (inference: False, date:2016-06-30 00:00:00, date_number: 354)
[AttentionModel/2016-07-31 | get_logger | INFO]: ====================AttentionModel/2016-07-31 start====================
[strategy_integration.py] set_seed(2016-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5779)training: 2/10 (0.5779)validation : 2/10 (0.5707)training: 3/10 (0.5707)validation : 3/10 (0.5707)training: 4/10 (0.5707)validation : 4/10 (0.5707)training: 5/10 (0.5707)validation : 5/10 (0.5707)training: 6/10 (0.5707)validation : 6/10 (0.5707)early stopping at 6 with loss 0.5707
AttentionModel-training is done: 6/10
2016-06-30 | reset count: 0 | final loss: 0.5707 at epoch 2
making sample. (inference: True, date:2016-08-31 00:00:00, date_number: 356)
making sample. (inference: False, date:2016-07-31 00:00:00, date_number: 355)
[AttentionModel/2016-08-31 | get_logger | INFO]: ====================AttentionModel/2016-08-31 start====================
[strategy_integration.py] set_seed(2016-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5728)training: 2/10 (0.5728)validation : 2/10 (0.5728)training: 3/10 (0.5728)validation : 3/10 (0.5728)training: 4/10 (0.5728)validation : 4/10 (0.5728)training: 5/10 (0.5728)validation : 5/10 (0.5714)training: 6/10 (0.5714)validation : 6/10 (0.5714)early stopping at 6 with loss 0.5714
AttentionModel-training is done: 6/10
2016-07-31 | reset count: 0 | final loss: 0.5714 at epoch 5
making sample. (inference: True, date:2016-09-30 00:00:00, date_number: 357)
making sample. (inference: False, date:2016-08-31 00:00:00, date_number: 356)
[AttentionModel/2016-09-30 | get_logger | INFO]: ====================AttentionModel/2016-09-30 start====================
[strategy_integration.py] set_seed(2016-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5754)training: 2/10 (0.5754)validation : 2/10 (0.5749)training: 3/10 (0.5749)validation : 3/10 (0.5726)training: 4/10 (0.5726)validation : 4/10 (0.5726)training: 5/10 (0.5726)validation : 5/10 (0.5704)training: 6/10 (0.5704)validation : 6/10 (0.5704)early stopping at 6 with loss 0.5704
AttentionModel-training is done: 6/10
2016-08-31 | reset count: 0 | final loss: 0.5704 at epoch 5
[strategy_integration.py] set_seed(2016-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5639)training: 2/10 (0.5639)validation : 2/10 (0.5639)training: 3/10 (0.5639)validation : 3/10 (0.5597)training: 4/10 (0.5597)validation : 4/10 (0.5597)training: 5/10 (0.5597)validation : 5/10 (0.5597)training: 6/10 (0.5597)validation : 6/10 (0.5597)early stopping at 6 with loss 0.5597
AttentionModel-training is done: 6/10
2016-09-30 | reset count: 0 | final loss: 0.5597 at epoch 3
making sample. (inference: True, date:2016-10-31 00:00:00, date_number: 358)
making sample. (inference: False, date:2016-09-30 00:00:00, date_number: 357)
[AttentionModel/2016-10-31 | get_logger | INFO]: ====================AttentionModel/2016-10-31 start====================
making sample. (inference: True, date:2016-11-30 00:00:00, date_number: 359)
making sample. (inference: False, date:2016-10-31 00:00:00, date_number: 358)
[AttentionModel/2016-11-30 | get_logger | INFO]: ====================AttentionModel/2016-11-30 start====================
[strategy_integration.py] set_seed(2016-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5638)training: 2/10 (0.5638)validation : 2/10 (0.5638)training: 3/10 (0.5638)validation : 3/10 (0.5638)training: 4/10 (0.5638)validation : 4/10 (0.5638)training: 5/10 (0.5638)validation : 5/10 (0.5638)training: 6/10 (0.5638)validation : 6/10 (0.5633)training: 7/10 (0.5633)validation : 7/10 (0.5609)training: 8/10 (0.5609)validation : 8/10 (0.5609)early stopping at 8 with loss 0.5609
AttentionModel-training is done: 8/10
2016-10-31 | reset count: 0 | final loss: 0.5609 at epoch 7
[strategy_integration.py] set_seed(2016-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5706)training: 2/10 (0.5706)validation : 2/10 (0.5669)training: 3/10 (0.5669)validation : 3/10 (0.5669)training: 4/10 (0.5669)validation : 4/10 (0.5669)training: 5/10 (0.5669)validation : 5/10 (0.5669)early stopping at 5 with loss 0.5669
AttentionModel-training is done: 5/10
2016-11-30 | reset count: 0 | final loss: 0.5669 at epoch 2
making sample. (inference: True, date:2016-12-31 00:00:00, date_number: 360)
making sample. (inference: False, date:2016-11-30 00:00:00, date_number: 359)
[AttentionModel/2016-12-31 | get_logger | INFO]: ====================AttentionModel/2016-12-31 start====================
making sample. (inference: True, date:2017-01-31 00:00:00, date_number: 361)
making sample. (inference: False, date:2016-12-31 00:00:00, date_number: 360)
[AttentionModel/2017-01-31 | get_logger | INFO]: ====================AttentionModel/2017-01-31 start====================
[strategy_integration.py] set_seed(2016-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5646)training: 2/10 (0.5646)validation : 2/10 (0.5621)training: 3/10 (0.5621)validation : 3/10 (0.5621)training: 4/10 (0.5621)validation : 4/10 (0.5621)training: 5/10 (0.5621)validation : 5/10 (0.5598)training: 6/10 (0.5598)validation : 6/10 (0.5598)training: 7/10 (0.5598)validation : 7/10 (0.5598)early stopping at 7 with loss 0.5598
AttentionModel-training is done: 7/10
2016-12-31 | reset count: 0 | final loss: 0.5598 at epoch 5
[strategy_integration.py] set_seed(2017-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5679)training: 2/10 (0.5679)validation : 2/10 (0.5679)training: 3/10 (0.5679)validation : 3/10 (0.5662)training: 4/10 (0.5662)validation : 4/10 (0.5662)training: 5/10 (0.5662)validation : 5/10 (0.5662)training: 6/10 (0.5662)validation : 6/10 (0.5662)early stopping at 6 with loss 0.5662
AttentionModel-training is done: 6/10
2017-01-31 | reset count: 0 | final loss: 0.5662 at epoch 3
making sample. (inference: True, date:2017-02-28 00:00:00, date_number: 362)
making sample. (inference: False, date:2017-01-31 00:00:00, date_number: 361)
[AttentionModel/2017-02-28 | get_logger | INFO]: ====================AttentionModel/2017-02-28 start====================
making sample. (inference: True, date:2017-03-31 00:00:00, date_number: 363)
making sample. (inference: False, date:2017-02-28 00:00:00, date_number: 362)
[AttentionModel/2017-03-31 | get_logger | INFO]: ====================AttentionModel/2017-03-31 start====================
[strategy_integration.py] set_seed(2017-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5670)training: 2/10 (0.5670)validation : 2/10 (0.5670)training: 3/10 (0.5670)validation : 3/10 (0.5666)training: 4/10 (0.5666)validation : 4/10 (0.5666)training: 5/10 (0.5666)validation : 5/10 (0.5663)training: 6/10 (0.5663)validation : 6/10 (0.5619)training: 7/10 (0.5619)validation : 7/10 (0.5619)early stopping at 7 with loss 0.5619
AttentionModel-training is done: 7/10
2017-02-28 | reset count: 0 | final loss: 0.5619 at epoch 6
[strategy_integration.py] set_seed(2017-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5712)training: 2/10 (0.5712)validation : 2/10 (0.5664)training: 3/10 (0.5664)validation : 3/10 (0.5664)training: 4/10 (0.5664)validation : 4/10 (0.5664)training: 5/10 (0.5664)validation : 5/10 (0.5664)early stopping at 5 with loss 0.5664
AttentionModel-training is done: 5/10
2017-03-31 | reset count: 0 | final loss: 0.5664 at epoch 2
making sample. (inference: True, date:2017-04-30 00:00:00, date_number: 364)
making sample. (inference: False, date:2017-03-31 00:00:00, date_number: 363)
[AttentionModel/2017-04-30 | get_logger | INFO]: ====================AttentionModel/2017-04-30 start====================
making sample. (inference: True, date:2017-05-31 00:00:00, date_number: 365)
making sample. (inference: False, date:2017-04-30 00:00:00, date_number: 364)
[AttentionModel/2017-05-31 | get_logger | INFO]: ====================AttentionModel/2017-05-31 start====================
[strategy_integration.py] set_seed(2017-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5665)training: 2/10 (0.5665)validation : 2/10 (0.5618)training: 3/10 (0.5618)validation : 3/10 (0.5618)training: 4/10 (0.5618)validation : 4/10 (0.5618)training: 5/10 (0.5618)validation : 5/10 (0.5618)training: 6/10 (0.5618)validation : 6/10 (0.5604)training: 7/10 (0.5604)validation : 7/10 (0.5604)early stopping at 7 with loss 0.5604
AttentionModel-training is done: 7/10
2017-04-30 | reset count: 0 | final loss: 0.5604 at epoch 6
[strategy_integration.py] set_seed(2017-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5605)training: 2/10 (0.5605)validation : 2/10 (0.5569)training: 3/10 (0.5569)validation : 3/10 (0.5538)training: 4/10 (0.5538)validation : 4/10 (0.5538)training: 5/10 (0.5538)validation : 5/10 (0.5538)training: 6/10 (0.5538)validation : 6/10 (0.5538)early stopping at 6 with loss 0.5538
AttentionModel-training is done: 6/10
2017-05-31 | reset count: 0 | final loss: 0.5538 at epoch 3
making sample. (inference: True, date:2017-06-30 00:00:00, date_number: 366)
making sample. (inference: False, date:2017-05-31 00:00:00, date_number: 365)
[AttentionModel/2017-06-30 | get_logger | INFO]: ====================AttentionModel/2017-06-30 start====================
making sample. (inference: True, date:2017-07-31 00:00:00, date_number: 367)
making sample. (inference: False, date:2017-06-30 00:00:00, date_number: 366)
[AttentionModel/2017-07-31 | get_logger | INFO]: ====================AttentionModel/2017-07-31 start====================
[strategy_integration.py] set_seed(2017-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5688)training: 2/10 (0.5688)validation : 2/10 (0.5683)training: 3/10 (0.5683)validation : 3/10 (0.5683)training: 4/10 (0.5683)validation : 4/10 (0.5653)training: 5/10 (0.5653)validation : 5/10 (0.5653)training: 6/10 (0.5653)validation : 6/10 (0.5653)training: 7/10 (0.5653)validation : 7/10 (0.5653)early stopping at 7 with loss 0.5653
AttentionModel-training is done: 7/10
2017-06-30 | reset count: 0 | final loss: 0.5653 at epoch 4
making sample. (inference: True, date:2017-08-31 00:00:00, date_number: 368)
making sample. (inference: False, date:2017-07-31 00:00:00, date_number: 367)
[AttentionModel/2017-08-31 | get_logger | INFO]: ====================AttentionModel/2017-08-31 start====================
[strategy_integration.py] set_seed(2017-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5661)training: 2/10 (0.5661)validation : 2/10 (0.5652)training: 3/10 (0.5652)validation : 3/10 (0.5630)training: 4/10 (0.5630)validation : 4/10 (0.5630)training: 5/10 (0.5630)validation : 5/10 (0.5627)training: 6/10 (0.5627)validation : 6/10 (0.5607)training: 7/10 (0.5607)validation : 7/10 (0.5607)training: 8/10 (0.5607)validation : 8/10 (0.5607)training: 9/10 (0.5607)validation : 9/10 (0.5607)early stopping at 9 with loss 0.5607
AttentionModel-training is done: 9/10
2017-07-31 | reset count: 0 | final loss: 0.5607 at epoch 6
making sample. (inference: True, date:2017-09-30 00:00:00, date_number: 369)
making sample. (inference: False, date:2017-08-31 00:00:00, date_number: 368)
[AttentionModel/2017-09-30 | get_logger | INFO]: ====================AttentionModel/2017-09-30 start====================
[strategy_integration.py] set_seed(2017-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5681)training: 2/10 (0.5681)validation : 2/10 (0.5641)training: 3/10 (0.5641)validation : 3/10 (0.5641)training: 4/10 (0.5641)validation : 4/10 (0.5615)training: 5/10 (0.5615)validation : 5/10 (0.5615)training: 6/10 (0.5615)validation : 6/10 (0.5615)training: 7/10 (0.5615)validation : 7/10 (0.5615)early stopping at 7 with loss 0.5615
AttentionModel-training is done: 7/10
2017-08-31 | reset count: 0 | final loss: 0.5615 at epoch 4
[strategy_integration.py] set_seed(2017-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5646)training: 2/10 (0.5646)validation : 2/10 (0.5574)training: 3/10 (0.5574)validation : 3/10 (0.5574)training: 4/10 (0.5574)validation : 4/10 (0.5574)training: 5/10 (0.5574)validation : 5/10 (0.5574)early stopping at 5 with loss 0.5574
AttentionModel-training is done: 5/10
2017-09-30 | reset count: 0 | final loss: 0.5574 at epoch 2
making sample. (inference: True, date:2017-10-31 00:00:00, date_number: 370)
making sample. (inference: False, date:2017-09-30 00:00:00, date_number: 369)
[AttentionModel/2017-10-31 | get_logger | INFO]: ====================AttentionModel/2017-10-31 start====================
making sample. (inference: True, date:2017-11-30 00:00:00, date_number: 371)
making sample. (inference: False, date:2017-10-31 00:00:00, date_number: 370)
[AttentionModel/2017-11-30 | get_logger | INFO]: ====================AttentionModel/2017-11-30 start====================
[strategy_integration.py] set_seed(2017-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5649)training: 2/10 (0.5649)validation : 2/10 (0.5628)training: 3/10 (0.5628)validation : 3/10 (0.5628)training: 4/10 (0.5628)validation : 4/10 (0.5628)training: 5/10 (0.5628)validation : 5/10 (0.5628)early stopping at 5 with loss 0.5628
AttentionModel-training is done: 5/10
2017-10-31 | reset count: 0 | final loss: 0.5628 at epoch 2
[strategy_integration.py] set_seed(2017-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5650)training: 2/10 (0.5650)validation : 2/10 (0.5650)training: 3/10 (0.5650)validation : 3/10 (0.5637)training: 4/10 (0.5637)validation : 4/10 (0.5627)training: 5/10 (0.5627)validation : 5/10 (0.5605)training: 6/10 (0.5605)validation : 6/10 (0.5605)early stopping at 6 with loss 0.5605
AttentionModel-training is done: 6/10
2017-11-30 | reset count: 0 | final loss: 0.5605 at epoch 5
making sample. (inference: True, date:2017-12-31 00:00:00, date_number: 372)
making sample. (inference: False, date:2017-11-30 00:00:00, date_number: 371)
[AttentionModel/2017-12-31 | get_logger | INFO]: ====================AttentionModel/2017-12-31 start====================
making sample. (inference: True, date:2018-01-31 00:00:00, date_number: 373)
making sample. (inference: False, date:2017-12-31 00:00:00, date_number: 372)
[AttentionModel/2018-01-31 | get_logger | INFO]: ====================AttentionModel/2018-01-31 start====================
[strategy_integration.py] set_seed(2017-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5707)training: 2/10 (0.5707)validation : 2/10 (0.5669)training: 3/10 (0.5669)validation : 3/10 (0.5657)training: 4/10 (0.5657)validation : 4/10 (0.5657)training: 5/10 (0.5657)validation : 5/10 (0.5657)training: 6/10 (0.5657)validation : 6/10 (0.5648)training: 7/10 (0.5648)validation : 7/10 (0.5648)early stopping at 7 with loss 0.5648
AttentionModel-training is done: 7/10
2017-12-31 | reset count: 0 | final loss: 0.5648 at epoch 6
[strategy_integration.py] set_seed(2018-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5665)training: 2/10 (0.5665)validation : 2/10 (0.5634)training: 3/10 (0.5634)validation : 3/10 (0.5634)training: 4/10 (0.5634)validation : 4/10 (0.5634)training: 5/10 (0.5634)validation : 5/10 (0.5634)early stopping at 5 with loss 0.5634
AttentionModel-training is done: 5/10
2018-01-31 | reset count: 0 | final loss: 0.5634 at epoch 2
making sample. (inference: True, date:2018-02-28 00:00:00, date_number: 374)
making sample. (inference: False, date:2018-01-31 00:00:00, date_number: 373)
[AttentionModel/2018-02-28 | get_logger | INFO]: ====================AttentionModel/2018-02-28 start====================
making sample. (inference: True, date:2018-03-31 00:00:00, date_number: 375)
making sample. (inference: False, date:2018-02-28 00:00:00, date_number: 374)
[AttentionModel/2018-03-31 | get_logger | INFO]: ====================AttentionModel/2018-03-31 start====================
[strategy_integration.py] set_seed(2018-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5677)training: 2/10 (0.5677)validation : 2/10 (0.5665)training: 3/10 (0.5665)validation : 3/10 (0.5665)training: 4/10 (0.5665)validation : 4/10 (0.5646)training: 5/10 (0.5646)validation : 5/10 (0.5646)training: 6/10 (0.5646)validation : 6/10 (0.5639)training: 7/10 (0.5639)validation : 7/10 (0.5639)early stopping at 7 with loss 0.5639
AttentionModel-training is done: 7/10
2018-02-28 | reset count: 0 | final loss: 0.5639 at epoch 6
[strategy_integration.py] set_seed(2018-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5654)training: 2/10 (0.5654)validation : 2/10 (0.5630)training: 3/10 (0.5630)validation : 3/10 (0.5616)training: 4/10 (0.5616)validation : 4/10 (0.5616)training: 5/10 (0.5616)validation : 5/10 (0.5586)training: 6/10 (0.5586)validation : 6/10 (0.5586)early stopping at 6 with loss 0.5586
AttentionModel-training is done: 6/10
2018-03-31 | reset count: 0 | final loss: 0.5586 at epoch 5
making sample. (inference: True, date:2018-04-30 00:00:00, date_number: 376)
making sample. (inference: False, date:2018-03-31 00:00:00, date_number: 375)
[AttentionModel/2018-04-30 | get_logger | INFO]: ====================AttentionModel/2018-04-30 start====================
making sample. (inference: True, date:2018-05-31 00:00:00, date_number: 377)
making sample. (inference: False, date:2018-04-30 00:00:00, date_number: 376)
[AttentionModel/2018-05-31 | get_logger | INFO]: ====================AttentionModel/2018-05-31 start====================
[strategy_integration.py] set_seed(2018-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5702)training: 2/10 (0.5702)validation : 2/10 (0.5702)training: 3/10 (0.5702)validation : 3/10 (0.5702)training: 4/10 (0.5702)validation : 4/10 (0.5702)training: 5/10 (0.5702)validation : 5/10 (0.5679)early stopping at 5 with loss 0.5679
AttentionModel-training is done: 5/10
2018-04-30 | reset count: 0 | final loss: 0.5679 at epoch 5
making sample. (inference: True, date:2018-06-30 00:00:00, date_number: 378)
making sample. (inference: False, date:2018-05-31 00:00:00, date_number: 377)
[AttentionModel/2018-06-30 | get_logger | INFO]: ====================AttentionModel/2018-06-30 start====================
[strategy_integration.py] set_seed(2018-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5582)training: 2/10 (0.5582)validation : 2/10 (0.5570)training: 3/10 (0.5570)validation : 3/10 (0.5503)training: 4/10 (0.5503)validation : 4/10 (0.5503)training: 5/10 (0.5503)validation : 5/10 (0.5496)training: 6/10 (0.5496)validation : 6/10 (0.5496)early stopping at 6 with loss 0.5496
AttentionModel-training is done: 6/10
2018-05-31 | reset count: 0 | final loss: 0.5496 at epoch 5
making sample. (inference: True, date:2018-07-31 00:00:00, date_number: 379)
making sample. (inference: False, date:2018-06-30 00:00:00, date_number: 378)
[AttentionModel/2018-07-31 | get_logger | INFO]: ====================AttentionModel/2018-07-31 start====================
[strategy_integration.py] set_seed(2018-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5664)training: 2/10 (0.5664)validation : 2/10 (0.5664)training: 3/10 (0.5664)validation : 3/10 (0.5664)training: 4/10 (0.5664)validation : 4/10 (0.5651)training: 5/10 (0.5651)validation : 5/10 (0.5638)training: 6/10 (0.5638)validation : 6/10 (0.5638)training: 7/10 (0.5638)validation : 7/10 (0.5638)training: 8/10 (0.5638)validation : 8/10 (0.5627)training: 9/10 (0.5627)validation : 9/10 (0.5627)early stopping at 9 with loss 0.5627
AttentionModel-training is done: 9/10
2018-06-30 | reset count: 0 | final loss: 0.5627 at epoch 8
[strategy_integration.py] set_seed(2018-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5703)training: 2/10 (0.5703)validation : 2/10 (0.5703)training: 3/10 (0.5703)validation : 3/10 (0.5703)training: 4/10 (0.5703)validation : 4/10 (0.5703)training: 5/10 (0.5703)validation : 5/10 (0.5703)early stopping at 5 with loss 0.5703
AttentionModel-training is done: 5/10
2018-07-31 | reset count: 0 | final loss: 0.5703 at epoch 1
making sample. (inference: True, date:2018-08-31 00:00:00, date_number: 380)
making sample. (inference: False, date:2018-07-31 00:00:00, date_number: 379)
[AttentionModel/2018-08-31 | get_logger | INFO]: ====================AttentionModel/2018-08-31 start====================
making sample. (inference: True, date:2018-09-30 00:00:00, date_number: 381)
making sample. (inference: False, date:2018-08-31 00:00:00, date_number: 380)
[AttentionModel/2018-09-30 | get_logger | INFO]: ====================AttentionModel/2018-09-30 start====================
[strategy_integration.py] set_seed(2018-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5721)training: 2/10 (0.5721)validation : 2/10 (0.5721)training: 3/10 (0.5721)validation : 3/10 (0.5721)training: 4/10 (0.5721)validation : 4/10 (0.5716)training: 5/10 (0.5716)validation : 5/10 (0.5690)training: 6/10 (0.5690)validation : 6/10 (0.5665)training: 7/10 (0.5665)validation : 7/10 (0.5665)training: 8/10 (0.5665)validation : 8/10 (0.5665)early stopping at 8 with loss 0.5665
AttentionModel-training is done: 8/10
2018-08-31 | reset count: 0 | final loss: 0.5665 at epoch 6
[strategy_integration.py] set_seed(2018-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5734)training: 2/10 (0.5734)validation : 2/10 (0.5704)training: 3/10 (0.5704)validation : 3/10 (0.5704)training: 4/10 (0.5704)validation : 4/10 (0.5704)training: 5/10 (0.5704)validation : 5/10 (0.5704)early stopping at 5 with loss 0.5704
AttentionModel-training is done: 5/10
2018-09-30 | reset count: 0 | final loss: 0.5704 at epoch 2
making sample. (inference: True, date:2018-10-31 00:00:00, date_number: 382)
making sample. (inference: False, date:2018-09-30 00:00:00, date_number: 381)
[AttentionModel/2018-10-31 | get_logger | INFO]: ====================AttentionModel/2018-10-31 start====================
making sample. (inference: True, date:2018-11-30 00:00:00, date_number: 383)
making sample. (inference: False, date:2018-10-31 00:00:00, date_number: 382)
[AttentionModel/2018-11-30 | get_logger | INFO]: ====================AttentionModel/2018-11-30 start====================
[strategy_integration.py] set_seed(2018-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5821)training: 2/10 (0.5821)validation : 2/10 (0.5802)training: 3/10 (0.5802)validation : 3/10 (0.5801)training: 4/10 (0.5801)validation : 4/10 (0.5801)training: 5/10 (0.5801)validation : 5/10 (0.5801)training: 6/10 (0.5801)validation : 6/10 (0.5801)early stopping at 6 with loss 0.5801
AttentionModel-training is done: 6/10
2018-10-31 | reset count: 0 | final loss: 0.5801 at epoch 3
making sample. (inference: True, date:2018-12-31 00:00:00, date_number: 384)
making sample. (inference: False, date:2018-11-30 00:00:00, date_number: 383)
[AttentionModel/2018-12-31 | get_logger | INFO]: ====================AttentionModel/2018-12-31 start====================
[strategy_integration.py] set_seed(2018-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5714)training: 2/10 (0.5714)validation : 2/10 (0.5675)training: 3/10 (0.5675)validation : 3/10 (0.5675)training: 4/10 (0.5675)validation : 4/10 (0.5675)training: 5/10 (0.5675)validation : 5/10 (0.5675)training: 6/10 (0.5675)validation : 6/10 (0.5675)early stopping at 6 with loss 0.5675
AttentionModel-training is done: 6/10
2018-11-30 | reset count: 0 | final loss: 0.5675 at epoch 2
making sample. (inference: True, date:2019-01-31 00:00:00, date_number: 385)
making sample. (inference: False, date:2018-12-31 00:00:00, date_number: 384)
[AttentionModel/2019-01-31 | get_logger | INFO]: ====================AttentionModel/2019-01-31 start====================
[strategy_integration.py] set_seed(2018-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5685)training: 2/10 (0.5685)validation : 2/10 (0.5685)training: 3/10 (0.5685)validation : 3/10 (0.5685)training: 4/10 (0.5685)validation : 4/10 (0.5651)training: 5/10 (0.5651)validation : 5/10 (0.5651)training: 6/10 (0.5651)validation : 6/10 (0.5651)training: 7/10 (0.5651)validation : 7/10 (0.5651)early stopping at 7 with loss 0.5651
AttentionModel-training is done: 7/10
2018-12-31 | reset count: 0 | final loss: 0.5651 at epoch 4
making sample. (inference: True, date:2019-02-28 00:00:00, date_number: 386)
making sample. (inference: False, date:2019-01-31 00:00:00, date_number: 385)
[AttentionModel/2019-02-28 | get_logger | INFO]: ====================AttentionModel/2019-02-28 start====================
[strategy_integration.py] set_seed(2019-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5901)training: 2/10 (0.5901)validation : 2/10 (0.5867)training: 3/10 (0.5867)validation : 3/10 (0.5866)training: 4/10 (0.5866)validation : 4/10 (0.5831)training: 5/10 (0.5831)validation : 5/10 (0.5831)training: 6/10 (0.5831)validation : 6/10 (0.5831)training: 7/10 (0.5831)validation : 7/10 (0.5831)early stopping at 7 with loss 0.5831
AttentionModel-training is done: 7/10
2019-01-31 | reset count: 0 | final loss: 0.5831 at epoch 4
making sample. (inference: True, date:2019-03-31 00:00:00, date_number: 387)
making sample. (inference: False, date:2019-02-28 00:00:00, date_number: 386)
[AttentionModel/2019-03-31 | get_logger | INFO]: ====================AttentionModel/2019-03-31 start====================
[strategy_integration.py] set_seed(2019-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5821)training: 2/10 (0.5821)validation : 2/10 (0.5812)training: 3/10 (0.5812)validation : 3/10 (0.5799)training: 4/10 (0.5799)validation : 4/10 (0.5776)training: 5/10 (0.5776)validation : 5/10 (0.5776)training: 6/10 (0.5776)validation : 6/10 (0.5776)early stopping at 6 with loss 0.5776
AttentionModel-training is done: 6/10
2019-02-28 | reset count: 0 | final loss: 0.5776 at epoch 4
making sample. (inference: True, date:2019-04-30 00:00:00, date_number: 388)
making sample. (inference: False, date:2019-03-31 00:00:00, date_number: 387)
[AttentionModel/2019-04-30 | get_logger | INFO]: ====================AttentionModel/2019-04-30 start====================
[strategy_integration.py] set_seed(2019-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5800)training: 2/10 (0.5800)validation : 2/10 (0.5774)training: 3/10 (0.5774)validation : 3/10 (0.5774)training: 4/10 (0.5774)validation : 4/10 (0.5733)training: 5/10 (0.5733)validation : 5/10 (0.5733)training: 6/10 (0.5733)validation : 6/10 (0.5733)training: 7/10 (0.5733)validation : 7/10 (0.5731)training: 8/10 (0.5731)validation : 8/10 (0.5731)training: 9/10 (0.5731)validation : 9/10 (0.5731)training: 10/10 (0.5731)validation : 10/10 (0.5731)early stopping at 10 with loss 0.5731
AttentionModel-training is done: 10/10
2019-03-31 | reset count: 0 | final loss: 0.5731 at epoch 7
[strategy_integration.py] set_seed(2019-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5984)training: 2/10 (0.5984)validation : 2/10 (0.5945)training: 3/10 (0.5945)validation : 3/10 (0.5945)training: 4/10 (0.5945)validation : 4/10 (0.5930)training: 5/10 (0.5930)validation : 5/10 (0.5930)training: 6/10 (0.5930)validation : 6/10 (0.5911)training: 7/10 (0.5911)validation : 7/10 (0.5911)early stopping at 7 with loss 0.5911
AttentionModel-training is done: 7/10
2019-04-30 | reset count: 0 | final loss: 0.5911 at epoch 6
making sample. (inference: True, date:2019-05-31 00:00:00, date_number: 389)
making sample. (inference: False, date:2019-04-30 00:00:00, date_number: 388)
[AttentionModel/2019-05-31 | get_logger | INFO]: ====================AttentionModel/2019-05-31 start====================
making sample. (inference: True, date:2019-06-30 00:00:00, date_number: 390)
making sample. (inference: False, date:2019-05-31 00:00:00, date_number: 389)
[AttentionModel/2019-06-30 | get_logger | INFO]: ====================AttentionModel/2019-06-30 start====================
[strategy_integration.py] set_seed(2019-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5826)training: 2/10 (0.5826)validation : 2/10 (0.5826)training: 3/10 (0.5826)validation : 3/10 (0.5826)training: 4/10 (0.5826)validation : 4/10 (0.5826)training: 5/10 (0.5826)validation : 5/10 (0.5826)early stopping at 5 with loss 0.5826
AttentionModel-training is done: 5/10
2019-05-31 | reset count: 0 | final loss: 0.5826 at epoch 1
making sample. (inference: True, date:2019-07-31 00:00:00, date_number: 391)
making sample. (inference: False, date:2019-06-30 00:00:00, date_number: 390)
[AttentionModel/2019-07-31 | get_logger | INFO]: ====================AttentionModel/2019-07-31 start====================
[strategy_integration.py] set_seed(2019-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5846)training: 2/10 (0.5846)validation : 2/10 (0.5846)training: 3/10 (0.5846)validation : 3/10 (0.5826)training: 4/10 (0.5826)validation : 4/10 (0.5826)training: 5/10 (0.5826)validation : 5/10 (0.5820)training: 6/10 (0.5820)validation : 6/10 (0.5820)early stopping at 6 with loss 0.5820
AttentionModel-training is done: 6/10
2019-06-30 | reset count: 0 | final loss: 0.5820 at epoch 5
making sample. (inference: True, date:2019-08-31 00:00:00, date_number: 392)
making sample. (inference: False, date:2019-07-31 00:00:00, date_number: 391)
[AttentionModel/2019-08-31 | get_logger | INFO]: ====================AttentionModel/2019-08-31 start====================
[strategy_integration.py] set_seed(2019-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5836)training: 2/10 (0.5836)validation : 2/10 (0.5807)training: 3/10 (0.5807)validation : 3/10 (0.5797)training: 4/10 (0.5797)validation : 4/10 (0.5794)training: 5/10 (0.5794)validation : 5/10 (0.5794)training: 6/10 (0.5794)validation : 6/10 (0.5781)training: 7/10 (0.5781)validation : 7/10 (0.5781)early stopping at 7 with loss 0.5781
AttentionModel-training is done: 7/10
2019-07-31 | reset count: 0 | final loss: 0.5781 at epoch 6
[strategy_integration.py] set_seed(2019-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5808)training: 2/10 (0.5808)validation : 2/10 (0.5784)training: 3/10 (0.5784)validation : 3/10 (0.5784)training: 4/10 (0.5784)validation : 4/10 (0.5784)training: 5/10 (0.5784)validation : 5/10 (0.5784)early stopping at 5 with loss 0.5784
AttentionModel-training is done: 5/10
2019-08-31 | reset count: 0 | final loss: 0.5784 at epoch 2
making sample. (inference: True, date:2019-09-30 00:00:00, date_number: 393)
making sample. (inference: False, date:2019-08-31 00:00:00, date_number: 392)
[AttentionModel/2019-09-30 | get_logger | INFO]: ====================AttentionModel/2019-09-30 start====================
making sample. (inference: True, date:2019-10-31 00:00:00, date_number: 394)
making sample. (inference: False, date:2019-09-30 00:00:00, date_number: 393)
[AttentionModel/2019-10-31 | get_logger | INFO]: ====================AttentionModel/2019-10-31 start====================
[strategy_integration.py] set_seed(2019-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5855)training: 2/10 (0.5855)validation : 2/10 (0.5837)training: 3/10 (0.5837)validation : 3/10 (0.5837)training: 4/10 (0.5837)validation : 4/10 (0.5837)training: 5/10 (0.5837)validation : 5/10 (0.5837)early stopping at 5 with loss 0.5837
AttentionModel-training is done: 5/10
2019-09-30 | reset count: 0 | final loss: 0.5837 at epoch 2
making sample. (inference: True, date:2019-11-30 00:00:00, date_number: 395)
making sample. (inference: False, date:2019-10-31 00:00:00, date_number: 394)
[AttentionModel/2019-11-30 | get_logger | INFO]: ====================AttentionModel/2019-11-30 start====================
[strategy_integration.py] set_seed(2019-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5861)training: 2/10 (0.5861)validation : 2/10 (0.5809)training: 3/10 (0.5809)validation : 3/10 (0.5785)training: 4/10 (0.5785)validation : 4/10 (0.5785)training: 5/10 (0.5785)validation : 5/10 (0.5785)training: 6/10 (0.5785)validation : 6/10 (0.5785)early stopping at 6 with loss 0.5785
AttentionModel-training is done: 6/10
2019-10-31 | reset count: 0 | final loss: 0.5785 at epoch 3
making sample. (inference: True, date:2019-12-31 00:00:00, date_number: 396)
making sample. (inference: False, date:2019-11-30 00:00:00, date_number: 395)
[AttentionModel/2019-12-31 | get_logger | INFO]: ====================AttentionModel/2019-12-31 start====================
[strategy_integration.py] set_seed(2019-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5858)training: 2/10 (0.5858)validation : 2/10 (0.5858)training: 3/10 (0.5858)validation : 3/10 (0.5829)training: 4/10 (0.5829)validation : 4/10 (0.5829)training: 5/10 (0.5829)validation : 5/10 (0.5829)training: 6/10 (0.5829)validation : 6/10 (0.5818)training: 7/10 (0.5818)validation : 7/10 (0.5806)training: 8/10 (0.5806)validation : 8/10 (0.5806)training: 9/10 (0.5806)validation : 9/10 (0.5806)training: 10/10 (0.5806)validation : 10/10 (0.5806)early stopping at 10 with loss 0.5806
AttentionModel-training is done: 10/10
2019-11-30 | reset count: 0 | final loss: 0.5806 at epoch 7
[strategy_integration.py] set_seed(2019-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5691)training: 2/10 (0.5691)validation : 2/10 (0.5691)training: 3/10 (0.5691)validation : 3/10 (0.5691)training: 4/10 (0.5691)validation : 4/10 (0.5691)training: 5/10 (0.5691)validation : 5/10 (0.5691)early stopping at 5 with loss 0.5691
AttentionModel-training is done: 5/10
2019-12-31 | reset count: 0 | final loss: 0.5691 at epoch 1
making sample. (inference: True, date:2020-01-31 00:00:00, date_number: 397)
making sample. (inference: False, date:2019-12-31 00:00:00, date_number: 396)
[AttentionModel/2020-01-31 | get_logger | INFO]: ====================AttentionModel/2020-01-31 start====================
making sample. (inference: True, date:2020-02-29 00:00:00, date_number: 398)
making sample. (inference: False, date:2020-01-31 00:00:00, date_number: 397)
[AttentionModel/2020-02-29 | get_logger | INFO]: ====================AttentionModel/2020-02-29 start====================
[strategy_integration.py] set_seed(2020-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5851)training: 2/10 (0.5851)validation : 2/10 (0.5788)training: 3/10 (0.5788)validation : 3/10 (0.5788)training: 4/10 (0.5788)validation : 4/10 (0.5788)training: 5/10 (0.5788)validation : 5/10 (0.5788)training: 6/10 (0.5788)validation : 6/10 (0.5788)training: 7/10 (0.5788)validation : 7/10 (0.5776)training: 8/10 (0.5776)validation : 8/10 (0.5776)training: 9/10 (0.5776)validation : 9/10 (0.5776)early stopping at 9 with loss 0.5776
AttentionModel-training is done: 9/10
2020-01-31 | reset count: 0 | final loss: 0.5776 at epoch 7
[strategy_integration.py] set_seed(2020-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5823)training: 2/10 (0.5823)validation : 2/10 (0.5780)training: 3/10 (0.5780)validation : 3/10 (0.5737)training: 4/10 (0.5737)validation : 4/10 (0.5737)training: 5/10 (0.5737)validation : 5/10 (0.5737)training: 6/10 (0.5737)validation : 6/10 (0.5737)early stopping at 6 with loss 0.5737
AttentionModel-training is done: 6/10
2020-02-29 | reset count: 0 | final loss: 0.5737 at epoch 3
making sample. (inference: True, date:2020-03-31 00:00:00, date_number: 399)
making sample. (inference: False, date:2020-02-29 00:00:00, date_number: 398)
[AttentionModel/2020-03-31 | get_logger | INFO]: ====================AttentionModel/2020-03-31 start====================
making sample. (inference: True, date:2020-04-30 00:00:00, date_number: 400)
making sample. (inference: False, date:2020-03-31 00:00:00, date_number: 399)
[AttentionModel/2020-04-30 | get_logger | INFO]: ====================AttentionModel/2020-04-30 start====================
[strategy_integration.py] set_seed(2020-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5686)training: 2/10 (0.5686)validation : 2/10 (0.5668)training: 3/10 (0.5668)validation : 3/10 (0.5668)training: 4/10 (0.5668)validation : 4/10 (0.5661)training: 5/10 (0.5661)validation : 5/10 (0.5661)early stopping at 5 with loss 0.5661
AttentionModel-training is done: 5/10
2020-03-31 | reset count: 0 | final loss: 0.5661 at epoch 4
making sample. (inference: True, date:2020-05-31 00:00:00, date_number: 401)
making sample. (inference: False, date:2020-04-30 00:00:00, date_number: 400)
[strategy_integration.py] set_seed(2020-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5829)training: 2/10 (0.5829)validation : 2/10 (0.5809)training: 3/10 (0.5809)validation : 3/10 (0.5809)training: 4/10 (0.5809)validation : 4/10 (0.5809)training: 5/10 (0.5809)validation : 5/10 (0.5809)training: 6/10 (0.5809)validation : 6/10 (0.5809)early stopping at 6 with loss 0.5809
AttentionModel-training is done: 6/10
2020-04-30 | reset count: 0 | final loss: 0.5809 at epoch 2
[AttentionModel/2020-05-31 | get_logger | INFO]: ====================AttentionModel/2020-05-31 start====================
making sample. (inference: True, date:2020-06-30 00:00:00, date_number: 402)
making sample. (inference: False, date:2020-05-31 00:00:00, date_number: 401)
[AttentionModel/2020-06-30 | get_logger | INFO]: ====================AttentionModel/2020-06-30 start====================
[strategy_integration.py] set_seed(2020-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5739)training: 2/10 (0.5739)validation : 2/10 (0.5739)training: 3/10 (0.5739)validation : 3/10 (0.5731)training: 4/10 (0.5731)validation : 4/10 (0.5721)training: 5/10 (0.5721)validation : 5/10 (0.5720)training: 6/10 (0.5720)validation : 6/10 (0.5707)training: 7/10 (0.5707)validation : 7/10 (0.5707)early stopping at 7 with loss 0.5707
AttentionModel-training is done: 7/10
2020-05-31 | reset count: 0 | final loss: 0.5707 at epoch 6
[strategy_integration.py] set_seed(2020-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5860)training: 2/10 (0.5860)validation : 2/10 (0.5825)training: 3/10 (0.5825)validation : 3/10 (0.5825)training: 4/10 (0.5825)validation : 4/10 (0.5825)training: 5/10 (0.5825)validation : 5/10 (0.5825)training: 6/10 (0.5825)validation : 6/10 (0.5825)training: 7/10 (0.5825)validation : 7/10 (0.5825)early stopping at 7 with loss 0.5825
AttentionModel-training is done: 7/10
2020-06-30 | reset count: 0 | final loss: 0.5825 at epoch 2
making sample. (inference: True, date:2020-07-31 00:00:00, date_number: 403)
making sample. (inference: False, date:2020-06-30 00:00:00, date_number: 402)
[AttentionModel/2020-07-31 | get_logger | INFO]: ====================AttentionModel/2020-07-31 start====================
making sample. (inference: True, date:2020-08-31 00:00:00, date_number: 404)
making sample. (inference: False, date:2020-07-31 00:00:00, date_number: 403)
[AttentionModel/2020-08-31 | get_logger | INFO]: ====================AttentionModel/2020-08-31 start====================
[strategy_integration.py] set_seed(2020-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5733)training: 2/10 (0.5733)validation : 2/10 (0.5720)training: 3/10 (0.5720)validation : 3/10 (0.5692)training: 4/10 (0.5692)validation : 4/10 (0.5692)training: 5/10 (0.5692)validation : 5/10 (0.5692)early stopping at 5 with loss 0.5692
AttentionModel-training is done: 5/10
2020-07-31 | reset count: 0 | final loss: 0.5692 at epoch 3
making sample. (inference: True, date:2020-09-30 00:00:00, date_number: 405)
making sample. (inference: False, date:2020-08-31 00:00:00, date_number: 404)
[AttentionModel/2020-09-30 | get_logger | INFO]: ====================AttentionModel/2020-09-30 start====================
[strategy_integration.py] set_seed(2020-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5765)training: 2/10 (0.5765)validation : 2/10 (0.5708)training: 3/10 (0.5708)validation : 3/10 (0.5691)training: 4/10 (0.5691)validation : 4/10 (0.5691)training: 5/10 (0.5691)validation : 5/10 (0.5691)training: 6/10 (0.5691)validation : 6/10 (0.5691)early stopping at 6 with loss 0.5691
AttentionModel-training is done: 6/10
2020-08-31 | reset count: 0 | final loss: 0.5691 at epoch 3
making sample. (inference: True, date:2020-10-31 00:00:00, date_number: 406)
making sample. (inference: False, date:2020-09-30 00:00:00, date_number: 405)
[AttentionModel/2020-10-31 | get_logger | INFO]: ====================AttentionModel/2020-10-31 start====================
[strategy_integration.py] set_seed(2020-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5731)training: 2/10 (0.5731)validation : 2/10 (0.5656)training: 3/10 (0.5656)validation : 3/10 (0.5656)training: 4/10 (0.5656)validation : 4/10 (0.5656)training: 5/10 (0.5656)validation : 5/10 (0.5656)early stopping at 5 with loss 0.5656
AttentionModel-training is done: 5/10
2020-09-30 | reset count: 0 | final loss: 0.5656 at epoch 2
making sample. (inference: True, date:2020-11-30 00:00:00, date_number: 407)
making sample. (inference: False, date:2020-10-31 00:00:00, date_number: 406)
[AttentionModel/2020-11-30 | get_logger | INFO]: ====================AttentionModel/2020-11-30 start====================
[strategy_integration.py] set_seed(2020-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5822)training: 2/10 (0.5822)validation : 2/10 (0.5812)training: 3/10 (0.5812)validation : 3/10 (0.5812)training: 4/10 (0.5812)validation : 4/10 (0.5812)training: 5/10 (0.5812)validation : 5/10 (0.5810)training: 6/10 (0.5810)validation : 6/10 (0.5801)training: 7/10 (0.5801)validation : 7/10 (0.5801)training: 8/10 (0.5801)validation : 8/10 (0.5801)early stopping at 8 with loss 0.5801
AttentionModel-training is done: 8/10
2020-10-31 | reset count: 0 | final loss: 0.5801 at epoch 6
[strategy_integration.py] set_seed(2020-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5757)training: 2/10 (0.5757)validation : 2/10 (0.5745)training: 3/10 (0.5745)validation : 3/10 (0.5745)training: 4/10 (0.5745)validation : 4/10 (0.5745)training: 5/10 (0.5745)validation : 5/10 (0.5741)early stopping at 5 with loss 0.5741
AttentionModel-training is done: 5/10
2020-11-30 | reset count: 0 | final loss: 0.5741 at epoch 5
making sample. (inference: True, date:2020-12-31 00:00:00, date_number: 408)
making sample. (inference: False, date:2020-11-30 00:00:00, date_number: 407)
[AttentionModel/2020-12-31 | get_logger | INFO]: ====================AttentionModel/2020-12-31 start====================
making sample. (inference: True, date:2021-01-31 00:00:00, date_number: 409)
making sample. (inference: False, date:2020-12-31 00:00:00, date_number: 408)
[AttentionModel/2021-01-31 | get_logger | INFO]: ====================AttentionModel/2021-01-31 start====================
[strategy_integration.py] set_seed(2020-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5638)training: 2/10 (0.5638)validation : 2/10 (0.5621)training: 3/10 (0.5621)validation : 3/10 (0.5594)training: 4/10 (0.5594)validation : 4/10 (0.5594)training: 5/10 (0.5594)validation : 5/10 (0.5586)training: 6/10 (0.5586)validation : 6/10 (0.5570)training: 7/10 (0.5570)validation : 7/10 (0.5570)training: 8/10 (0.5570)validation : 8/10 (0.5570)training: 9/10 (0.5570)validation : 9/10 (0.5570)early stopping at 9 with loss 0.5570
AttentionModel-training is done: 9/10
2020-12-31 | reset count: 0 | final loss: 0.5570 at epoch 6
making sample. (inference: True, date:2021-02-28 00:00:00, date_number: 410)
making sample. (inference: False, date:2021-01-31 00:00:00, date_number: 409)
[AttentionModel/2021-02-28 | get_logger | INFO]: ====================AttentionModel/2021-02-28 start====================
[strategy_integration.py] set_seed(2021-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5649)training: 2/10 (0.5649)validation : 2/10 (0.5584)training: 3/10 (0.5584)validation : 3/10 (0.5577)training: 4/10 (0.5577)validation : 4/10 (0.5577)training: 5/10 (0.5577)validation : 5/10 (0.5577)training: 6/10 (0.5577)validation : 6/10 (0.5542)training: 7/10 (0.5542)validation : 7/10 (0.5542)training: 8/10 (0.5542)validation : 8/10 (0.5542)training: 9/10 (0.5542)validation : 9/10 (0.5542)early stopping at 9 with loss 0.5542
AttentionModel-training is done: 9/10
2021-01-31 | reset count: 0 | final loss: 0.5542 at epoch 6
making sample. (inference: True, date:2021-03-31 00:00:00, date_number: 411)
making sample. (inference: False, date:2021-02-28 00:00:00, date_number: 410)
[AttentionModel/2021-03-31 | get_logger | INFO]: ====================AttentionModel/2021-03-31 start====================
[strategy_integration.py] set_seed(2021-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5655)training: 2/10 (0.5655)validation : 2/10 (0.5655)training: 3/10 (0.5655)validation : 3/10 (0.5655)training: 4/10 (0.5655)validation : 4/10 (0.5630)training: 5/10 (0.5630)validation : 5/10 (0.5590)training: 6/10 (0.5590)validation : 6/10 (0.5590)training: 7/10 (0.5590)validation : 7/10 (0.5590)early stopping at 7 with loss 0.5590
AttentionModel-training is done: 7/10
2021-02-28 | reset count: 0 | final loss: 0.5590 at epoch 5
making sample. (inference: True, date:2021-04-30 00:00:00, date_number: 412)
making sample. (inference: False, date:2021-03-31 00:00:00, date_number: 411)
[AttentionModel/2021-04-30 | get_logger | INFO]: ====================AttentionModel/2021-04-30 start====================
[strategy_integration.py] set_seed(2021-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5660)training: 2/10 (0.5660)validation : 2/10 (0.5660)training: 3/10 (0.5660)validation : 3/10 (0.5660)training: 4/10 (0.5660)validation : 4/10 (0.5653)training: 5/10 (0.5653)validation : 5/10 (0.5653)training: 6/10 (0.5653)validation : 6/10 (0.5653)training: 7/10 (0.5653)validation : 7/10 (0.5620)training: 8/10 (0.5620)validation : 8/10 (0.5620)training: 9/10 (0.5620)validation : 9/10 (0.5620)training: 10/10 (0.5620)validation : 10/10 (0.5620)early stopping at 10 with loss 0.5620
AttentionModel-training is done: 10/10
2021-03-31 | reset count: 0 | final loss: 0.5620 at epoch 7
[strategy_integration.py] set_seed(2021-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5671)training: 2/10 (0.5671)validation : 2/10 (0.5630)training: 3/10 (0.5630)validation : 3/10 (0.5630)training: 4/10 (0.5630)validation : 4/10 (0.5630)training: 5/10 (0.5630)validation : 5/10 (0.5621)training: 6/10 (0.5621)validation : 6/10 (0.5621)early stopping at 6 with loss 0.5621
AttentionModel-training is done: 6/10
2021-04-30 | reset count: 0 | final loss: 0.5621 at epoch 5
making sample. (inference: True, date:2021-05-31 00:00:00, date_number: 413)
making sample. (inference: False, date:2021-04-30 00:00:00, date_number: 412)
[AttentionModel/2021-05-31 | get_logger | INFO]: ====================AttentionModel/2021-05-31 start====================
making sample. (inference: True, date:2021-06-30 00:00:00, date_number: 414)
making sample. (inference: False, date:2021-05-31 00:00:00, date_number: 413)
[AttentionModel/2021-06-30 | get_logger | INFO]: ====================AttentionModel/2021-06-30 start====================
[strategy_integration.py] set_seed(2021-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5616)training: 2/10 (0.5616)validation : 2/10 (0.5616)training: 3/10 (0.5616)validation : 3/10 (0.5616)training: 4/10 (0.5616)validation : 4/10 (0.5583)training: 5/10 (0.5583)validation : 5/10 (0.5583)training: 6/10 (0.5583)validation : 6/10 (0.5583)early stopping at 6 with loss 0.5583
AttentionModel-training is done: 6/10
2021-05-31 | reset count: 0 | final loss: 0.5583 at epoch 4
[strategy_integration.py] set_seed(2021-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5755)training: 2/10 (0.5755)validation : 2/10 (0.5655)training: 3/10 (0.5655)validation : 3/10 (0.5655)training: 4/10 (0.5655)validation : 4/10 (0.5655)training: 5/10 (0.5655)validation : 5/10 (0.5655)early stopping at 5 with loss 0.5655
AttentionModel-training is done: 5/10
2021-06-30 | reset count: 0 | final loss: 0.5655 at epoch 2
making sample. (inference: True, date:2021-07-31 00:00:00, date_number: 415)
making sample. (inference: False, date:2021-06-30 00:00:00, date_number: 414)
[AttentionModel/2021-07-31 | get_logger | INFO]: ====================AttentionModel/2021-07-31 start====================
making sample. (inference: True, date:2021-08-31 00:00:00, date_number: 416)
making sample. (inference: False, date:2021-07-31 00:00:00, date_number: 415)
[AttentionModel/2021-08-31 | get_logger | INFO]: ====================AttentionModel/2021-08-31 start====================
[strategy_integration.py] set_seed(2021-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5826)training: 2/10 (0.5826)validation : 2/10 (0.5734)training: 3/10 (0.5734)validation : 3/10 (0.5734)training: 4/10 (0.5734)validation : 4/10 (0.5734)training: 5/10 (0.5734)validation : 5/10 (0.5734)early stopping at 5 with loss 0.5734
AttentionModel-training is done: 5/10
2021-08-31 | reset count: 0 | final loss: 0.5734 at epoch 2
[strategy_integration.py] set_seed(2021-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5610)training: 2/10 (0.5610)validation : 2/10 (0.5578)training: 3/10 (0.5578)validation : 3/10 (0.5578)training: 4/10 (0.5578)validation : 4/10 (0.5578)training: 5/10 (0.5578)validation : 5/10 (0.5578)training: 6/10 (0.5578)validation : 6/10 (0.5578)training: 7/10 (0.5578)validation : 7/10 (0.5573)training: 8/10 (0.5573)validation : 8/10 (0.5573)training: 9/10 (0.5573)validation : 9/10 (0.5573)training: 10/10 (0.5573)validation : 10/10 (0.5573)early stopping at 10 with loss 0.5573
AttentionModel-training is done: 10/10
2021-07-31 | reset count: 0 | final loss: 0.5573 at epoch 7
making sample. (inference: True, date:2021-09-30 00:00:00, date_number: 417)
making sample. (inference: False, date:2021-08-31 00:00:00, date_number: 416)
[AttentionModel/2021-09-30 | get_logger | INFO]: ====================AttentionModel/2021-09-30 start====================
making sample. (inference: True, date:2021-10-31 00:00:00, date_number: 418)
making sample. (inference: False, date:2021-09-30 00:00:00, date_number: 417)
[AttentionModel/2021-10-31 | get_logger | INFO]: ====================AttentionModel/2021-10-31 start====================
[strategy_integration.py] set_seed(2021-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5745)training: 2/10 (0.5745)validation : 2/10 (0.5745)training: 3/10 (0.5745)validation : 3/10 (0.5726)training: 4/10 (0.5726)validation : 4/10 (0.5726)training: 5/10 (0.5726)validation : 5/10 (0.5721)training: 6/10 (0.5721)validation : 6/10 (0.5706)training: 7/10 (0.5706)validation : 7/10 (0.5706)training: 8/10 (0.5706)validation : 8/10 (0.5704)training: 9/10 (0.5704)validation : 9/10 (0.5704)early stopping at 9 with loss 0.5704
AttentionModel-training is done: 9/10
2021-09-30 | reset count: 0 | final loss: 0.5704 at epoch 8
[strategy_integration.py] set_seed(2021-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5820)training: 2/10 (0.5820)validation : 2/10 (0.5818)training: 3/10 (0.5818)validation : 3/10 (0.5817)training: 4/10 (0.5817)validation : 4/10 (0.5789)training: 5/10 (0.5789)validation : 5/10 (0.5789)training: 6/10 (0.5789)validation : 6/10 (0.5789)early stopping at 6 with loss 0.5789
AttentionModel-training is done: 6/10
2021-10-31 | reset count: 0 | final loss: 0.5789 at epoch 4
[strategy | get_logger | INFO]: ====================strategy start====================
[strategy | _get_infer_dirs | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick108_test02/infer 이 strategy 대상에 추가됨
[strategy | _backtest | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick108_test02/infer is starting
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.157 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache⠦ [Compustat API] : Loading get_historic_universe from cache⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading get_historic_universe from cache done in 2.007 secs
⠧ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.308 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.063 secs
⠋ [Compustat API] : Loading get_monthly_total_return from cache✓ [Compustat API] : Loading get_monthly_total_return from cache done in 0.062 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.076 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.071 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.07 secs
⠋ [Compustat API] : Loading get_monthly_volume_data from cache✓ [Compustat API] : Loading get_monthly_volume_data from cache done in 0.08 secs
[strategy | _allocating | INFO]: [STRATEGY] allocating start
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_weight_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] performance.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] combined.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] weight.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] universe.csv is saved.
[comparison | get_logger | INFO]: ====================comparison start====================
[comparison | compare | INFO]: [5. COMPARISON] comparison is saved.
[comparison | compare | INFO]:                        Harvest_Green_concept001_pick108_test02_price_return  ...  SPCOMP_total_win
2016-01-31_2021-04-30                                            0.24796     ...          0.765625

[1 rows x 27 columns]
2015-12-31
2016-01-31
2016-02-29
2016-03-31
2016-04-30
2016-05-31
2016-06-30
2016-07-31
2016-08-31
2016-09-30
2016-10-31
2016-11-30
2016-12-31
2017-01-31
2017-02-28
2017-03-31
2017-04-30
2017-05-31
2017-06-30
2017-07-31
2017-08-31
2017-09-30
2017-10-31
2017-11-30
2017-12-31
2018-01-31
2018-02-28
2018-03-31
2018-04-30
2018-05-31
2018-06-30
2018-07-31
2018-08-31
2018-09-30
2018-10-31
2018-11-30
2018-12-31
2019-01-31
2019-02-28
2019-03-31
2019-04-30
2019-05-31
2019-06-30
2019-07-31
2019-08-31
2019-09-30
2019-10-31
2019-11-30
2019-12-31
2020-01-31
2020-02-29
2020-03-31
2020-04-30
2020-05-31
2020-06-30
2020-07-31
2020-08-31
2020-09-30
2020-10-31
2020-11-30
2020-12-31
2021-01-31
2021-02-28
2021-03-31
2021-04-30
2021-05-31
2021-06-30
2021-07-31
2021-08-31
2021-09-30
2021-10-31
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.053 secs
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.046 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.064 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.079 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.074 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.657 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.674 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.681 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.68 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.682 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.076 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.668 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.668 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.669 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.661 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.659 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.078 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.683 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.076 secs
⠋ [Compustat API] : Cleaning past days cache✓ [Compustat API] : Cleaning past days cache done in 0.0 secs
auto caching이 활성화 되었습니다. /home/sronly/sr-storage/kirin_cache/kirin_api_cache/2.19.17_2021-11-23에 캐쉬를 저장합니다.
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.149 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠹ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading masking data✓ [Compustat API] : Loading get_historic_universe from cache done in 2.006 secs
⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.306 secs
setting tensorflow random seed failed
load_data: r_s
load_data: r_a
load_data: fred_ff
load_data: t3y
load_data: t5y
load_data: t7y
load_data: t1y_ff
load_data: t5y_ff
load_data: t10y_t2y
load_data: export_growth
load_data: import_growth
load_data: real_gig
load_data: gpa
load_data: rev_surp
load_data: cash_at
load_data: retail_mfr
load_data: m1
load_data: m2
load_data: res_mom_12m_1m_0m
load_data: res_vol_6m_3m_0m
load_data: at
load_data: op_lev
load_data: roe
load_data: std_u_e
load_data: baa_ff
load_data: core_cpi
load_data: core_pce
load_data: corporate_tg
load_data: industrial_prod
load_data: home_pr
load_data: ae_m
load_data: ia_ta
load_data: rc_a
load_data: aaa_t10y
load_data: baa_t10y
load_data: aaa_ff
load_data: real_pig
load_data: federal_tg
load_data: real_gdp
load_data: pr_1m_0m
load_data: mv
load_data: btm
load_data: snp500_pr
load_data: wilshire500_pr
load_data: ted
load_data: ppi
load_data: trimmed_pce
load_data: unemploy
load_data: wti
load_data: capa_util
load_data: snp500_pe
load_data: mom_12m_1m
load_data: ram_12m_0m
load_data: vol_3m
load_data: t3m
load_data: t6m
load_data: t2y
load_data: t10y
load_data: aaa
load_data: baa
load_data: core_ppi
load_data: cpi
load_data: pce
load_data: snp500_vol
load_data: sector_values
load_data: mv
load_data: pr_1m_0m
load_data: mv
making sample. (inference: True, date:2015-12-31 00:00:00, date_number: 348)
making sample. (inference: False, date:2012-12-31 00:00:00, date_number: 312)
making sample. (inference: False, date:2013-01-31 00:00:00, date_number: 313)
making sample. (inference: False, date:2013-02-28 00:00:00, date_number: 314)
making sample. (inference: False, date:2013-03-31 00:00:00, date_number: 315)
making sample. (inference: False, date:2013-04-30 00:00:00, date_number: 316)
making sample. (inference: False, date:2013-05-31 00:00:00, date_number: 317)
making sample. (inference: False, date:2013-06-30 00:00:00, date_number: 318)
making sample. (inference: False, date:2013-07-31 00:00:00, date_number: 319)
making sample. (inference: False, date:2013-08-31 00:00:00, date_number: 320)
making sample. (inference: False, date:2013-09-30 00:00:00, date_number: 321)
making sample. (inference: False, date:2013-10-31 00:00:00, date_number: 322)
making sample. (inference: False, date:2013-11-30 00:00:00, date_number: 323)
making sample. (inference: False, date:2013-12-31 00:00:00, date_number: 324)
making sample. (inference: False, date:2014-01-31 00:00:00, date_number: 325)
making sample. (inference: False, date:2014-02-28 00:00:00, date_number: 326)
making sample. (inference: False, date:2014-03-31 00:00:00, date_number: 327)
making sample. (inference: False, date:2014-04-30 00:00:00, date_number: 328)
making sample. (inference: False, date:2014-05-31 00:00:00, date_number: 329)
making sample. (inference: False, date:2014-06-30 00:00:00, date_number: 330)
making sample. (inference: False, date:2014-07-31 00:00:00, date_number: 331)
making sample. (inference: False, date:2014-08-31 00:00:00, date_number: 332)
making sample. (inference: False, date:2014-09-30 00:00:00, date_number: 333)
making sample. (inference: False, date:2014-10-31 00:00:00, date_number: 334)
making sample. (inference: False, date:2014-11-30 00:00:00, date_number: 335)
making sample. (inference: False, date:2014-12-31 00:00:00, date_number: 336)
making sample. (inference: False, date:2015-01-31 00:00:00, date_number: 337)
making sample. (inference: False, date:2015-02-28 00:00:00, date_number: 338)
making sample. (inference: False, date:2015-03-31 00:00:00, date_number: 339)
making sample. (inference: False, date:2015-04-30 00:00:00, date_number: 340)
making sample. (inference: False, date:2015-05-31 00:00:00, date_number: 341)
making sample. (inference: False, date:2015-06-30 00:00:00, date_number: 342)
making sample. (inference: False, date:2015-07-31 00:00:00, date_number: 343)
making sample. (inference: False, date:2015-08-31 00:00:00, date_number: 344)
making sample. (inference: False, date:2015-09-30 00:00:00, date_number: 345)
making sample. (inference: False, date:2015-10-31 00:00:00, date_number: 346)
making sample. (inference: False, date:2015-11-30 00:00:00, date_number: 347)
[AttentionModel/2015-12-31 | get_logger | INFO]: ====================AttentionModel/2015-12-31 start====================
making sample. (inference: True, date:2016-01-31 00:00:00, date_number: 349)
making sample. (inference: False, date:2015-12-31 00:00:00, date_number: 348)
[AttentionModel/2016-01-31 | get_logger | INFO]: ====================AttentionModel/2016-01-31 start====================
[strategy_integration.py] set_seed(2015-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5729)training: 2/10 (0.5729)validation : 2/10 (0.5693)training: 3/10 (0.5693)validation : 3/10 (0.5669)training: 4/10 (0.5669)validation : 4/10 (0.5669)training: 5/10 (0.5669)validation : 5/10 (0.5669)training: 6/10 (0.5669)validation : 6/10 (0.5669)early stopping at 6 with loss 0.5669
AttentionModel-training is done: 6/10
2015-12-31 | reset count: 0 | final loss: 0.5669 at epoch 3
making sample. (inference: True, date:2016-02-29 00:00:00, date_number: 350)
making sample. (inference: False, date:2016-01-31 00:00:00, date_number: 349)
[strategy_integration.py] set_seed(2016-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5788)training: 2/10 (0.5788)validation : 2/10 (0.5788)training: 3/10 (0.5788)validation : 3/10 (0.5788)training: 4/10 (0.5788)validation : 4/10 (0.5750)training: 5/10 (0.5750)validation : 5/10 (0.5750)training: 6/10 (0.5750)validation : 6/10 (0.5750)training: 7/10 (0.5750)validation : 7/10 (0.5750)early stopping at 7 with loss 0.5750
AttentionModel-training is done: 7/10
2016-01-31 | reset count: 0 | final loss: 0.5750 at epoch 4
[AttentionModel/2016-02-29 | get_logger | INFO]: ====================AttentionModel/2016-02-29 start====================
making sample. (inference: True, date:2016-03-31 00:00:00, date_number: 351)
making sample. (inference: False, date:2016-02-29 00:00:00, date_number: 350)
[AttentionModel/2016-03-31 | get_logger | INFO]: ====================AttentionModel/2016-03-31 start====================
[strategy_integration.py] set_seed(2016-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5702)training: 2/10 (0.5702)validation : 2/10 (0.5692)training: 3/10 (0.5692)validation : 3/10 (0.5658)training: 4/10 (0.5658)validation : 4/10 (0.5658)training: 5/10 (0.5658)validation : 5/10 (0.5658)training: 6/10 (0.5658)validation : 6/10 (0.5658)early stopping at 6 with loss 0.5658
AttentionModel-training is done: 6/10
2016-02-29 | reset count: 0 | final loss: 0.5658 at epoch 3
[strategy_integration.py] set_seed(2016-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5687)training: 2/10 (0.5687)validation : 2/10 (0.5678)training: 3/10 (0.5678)validation : 3/10 (0.5678)training: 4/10 (0.5678)validation : 4/10 (0.5678)training: 5/10 (0.5678)validation : 5/10 (0.5668)training: 6/10 (0.5668)validation : 6/10 (0.5668)early stopping at 6 with loss 0.5668
AttentionModel-training is done: 6/10
2016-03-31 | reset count: 0 | final loss: 0.5668 at epoch 5
making sample. (inference: True, date:2016-04-30 00:00:00, date_number: 352)
making sample. (inference: False, date:2016-03-31 00:00:00, date_number: 351)
[AttentionModel/2016-04-30 | get_logger | INFO]: ====================AttentionModel/2016-04-30 start====================
making sample. (inference: True, date:2016-05-31 00:00:00, date_number: 353)
making sample. (inference: False, date:2016-04-30 00:00:00, date_number: 352)
[AttentionModel/2016-05-31 | get_logger | INFO]: ====================AttentionModel/2016-05-31 start====================
[strategy_integration.py] set_seed(2016-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5705)training: 2/10 (0.5705)validation : 2/10 (0.5705)training: 3/10 (0.5705)validation : 3/10 (0.5705)training: 4/10 (0.5705)validation : 4/10 (0.5697)training: 5/10 (0.5697)validation : 5/10 (0.5697)early stopping at 5 with loss 0.5697
AttentionModel-training is done: 5/10
2016-04-30 | reset count: 0 | final loss: 0.5697 at epoch 4
making sample. (inference: True, date:2016-06-30 00:00:00, date_number: 354)
making sample. (inference: False, date:2016-05-31 00:00:00, date_number: 353)
[AttentionModel/2016-06-30 | get_logger | INFO]: ====================AttentionModel/2016-06-30 start====================
[strategy_integration.py] set_seed(2016-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5807)training: 2/10 (0.5807)validation : 2/10 (0.5774)training: 3/10 (0.5774)validation : 3/10 (0.5774)training: 4/10 (0.5774)validation : 4/10 (0.5774)training: 5/10 (0.5774)validation : 5/10 (0.5756)training: 6/10 (0.5756)validation : 6/10 (0.5756)training: 7/10 (0.5756)validation : 7/10 (0.5756)early stopping at 7 with loss 0.5756
AttentionModel-training is done: 7/10
2016-05-31 | reset count: 0 | final loss: 0.5756 at epoch 5
[strategy_integration.py] set_seed(2016-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5735)training: 2/10 (0.5735)validation : 2/10 (0.5735)training: 3/10 (0.5735)validation : 3/10 (0.5722)training: 4/10 (0.5722)validation : 4/10 (0.5722)training: 5/10 (0.5722)validation : 5/10 (0.5722)early stopping at 5 with loss 0.5722
AttentionModel-training is done: 5/10
2016-06-30 | reset count: 0 | final loss: 0.5722 at epoch 3
making sample. (inference: True, date:2016-07-31 00:00:00, date_number: 355)
making sample. (inference: False, date:2016-06-30 00:00:00, date_number: 354)
[AttentionModel/2016-07-31 | get_logger | INFO]: ====================AttentionModel/2016-07-31 start====================
making sample. (inference: True, date:2016-08-31 00:00:00, date_number: 356)
making sample. (inference: False, date:2016-07-31 00:00:00, date_number: 355)
[AttentionModel/2016-08-31 | get_logger | INFO]: ====================AttentionModel/2016-08-31 start====================
[strategy_integration.py] set_seed(2016-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5785)training: 2/10 (0.5785)validation : 2/10 (0.5758)training: 3/10 (0.5758)validation : 3/10 (0.5758)training: 4/10 (0.5758)validation : 4/10 (0.5758)training: 5/10 (0.5758)validation : 5/10 (0.5729)training: 6/10 (0.5729)validation : 6/10 (0.5729)training: 7/10 (0.5729)validation : 7/10 (0.5729)early stopping at 7 with loss 0.5729
AttentionModel-training is done: 7/10
2016-07-31 | reset count: 0 | final loss: 0.5729 at epoch 5
[strategy_integration.py] set_seed(2016-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5737)training: 2/10 (0.5737)validation : 2/10 (0.5698)training: 3/10 (0.5698)validation : 3/10 (0.5646)training: 4/10 (0.5646)validation : 4/10 (0.5646)training: 5/10 (0.5646)validation : 5/10 (0.5641)training: 6/10 (0.5641)validation : 6/10 (0.5641)early stopping at 6 with loss 0.5641
AttentionModel-training is done: 6/10
2016-08-31 | reset count: 0 | final loss: 0.5641 at epoch 5
making sample. (inference: True, date:2016-09-30 00:00:00, date_number: 357)
making sample. (inference: False, date:2016-08-31 00:00:00, date_number: 356)
[AttentionModel/2016-09-30 | get_logger | INFO]: ====================AttentionModel/2016-09-30 start====================
making sample. (inference: True, date:2016-10-31 00:00:00, date_number: 358)
making sample. (inference: False, date:2016-09-30 00:00:00, date_number: 357)
[AttentionModel/2016-10-31 | get_logger | INFO]: ====================AttentionModel/2016-10-31 start====================
[strategy_integration.py] set_seed(2016-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5674)training: 2/10 (0.5674)validation : 2/10 (0.5674)training: 3/10 (0.5674)validation : 3/10 (0.5663)training: 4/10 (0.5663)validation : 4/10 (0.5663)training: 5/10 (0.5663)validation : 5/10 (0.5632)training: 6/10 (0.5632)validation : 6/10 (0.5632)early stopping at 6 with loss 0.5632
AttentionModel-training is done: 6/10
2016-09-30 | reset count: 0 | final loss: 0.5632 at epoch 5
making sample. (inference: True, date:2016-11-30 00:00:00, date_number: 359)
making sample. (inference: False, date:2016-10-31 00:00:00, date_number: 358)
[AttentionModel/2016-11-30 | get_logger | INFO]: ====================AttentionModel/2016-11-30 start====================
[strategy_integration.py] set_seed(2016-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5788)training: 2/10 (0.5788)validation : 2/10 (0.5772)training: 3/10 (0.5772)validation : 3/10 (0.5758)training: 4/10 (0.5758)validation : 4/10 (0.5748)training: 5/10 (0.5748)validation : 5/10 (0.5748)training: 6/10 (0.5748)validation : 6/10 (0.5748)early stopping at 6 with loss 0.5748
AttentionModel-training is done: 6/10
2016-10-31 | reset count: 0 | final loss: 0.5748 at epoch 4
making sample. (inference: True, date:2016-12-31 00:00:00, date_number: 360)
making sample. (inference: False, date:2016-11-30 00:00:00, date_number: 359)
[AttentionModel/2016-12-31 | get_logger | INFO]: ====================AttentionModel/2016-12-31 start====================
[strategy_integration.py] set_seed(2016-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5737)training: 2/10 (0.5737)validation : 2/10 (0.5737)training: 3/10 (0.5737)validation : 3/10 (0.5737)training: 4/10 (0.5737)validation : 4/10 (0.5715)training: 5/10 (0.5715)validation : 5/10 (0.5715)training: 6/10 (0.5715)validation : 6/10 (0.5715)training: 7/10 (0.5715)validation : 7/10 (0.5715)early stopping at 7 with loss 0.5715
AttentionModel-training is done: 7/10
2016-11-30 | reset count: 0 | final loss: 0.5715 at epoch 4
[strategy_integration.py] set_seed(2016-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5667)training: 2/10 (0.5667)validation : 2/10 (0.5667)training: 3/10 (0.5667)validation : 3/10 (0.5623)training: 4/10 (0.5623)validation : 4/10 (0.5623)training: 5/10 (0.5623)validation : 5/10 (0.5608)training: 6/10 (0.5608)validation : 6/10 (0.5608)early stopping at 6 with loss 0.5608
AttentionModel-training is done: 6/10
2016-12-31 | reset count: 0 | final loss: 0.5608 at epoch 5
making sample. (inference: True, date:2017-01-31 00:00:00, date_number: 361)
making sample. (inference: False, date:2016-12-31 00:00:00, date_number: 360)
[AttentionModel/2017-01-31 | get_logger | INFO]: ====================AttentionModel/2017-01-31 start====================
making sample. (inference: True, date:2017-02-28 00:00:00, date_number: 362)
making sample. (inference: False, date:2017-01-31 00:00:00, date_number: 361)
[AttentionModel/2017-02-28 | get_logger | INFO]: ====================AttentionModel/2017-02-28 start====================
[strategy_integration.py] set_seed(2017-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5815)training: 2/10 (0.5815)validation : 2/10 (0.5781)training: 3/10 (0.5781)validation : 3/10 (0.5764)training: 4/10 (0.5764)validation : 4/10 (0.5764)training: 5/10 (0.5764)validation : 5/10 (0.5764)early stopping at 5 with loss 0.5764
AttentionModel-training is done: 5/10
2017-01-31 | reset count: 0 | final loss: 0.5764 at epoch 3
making sample. (inference: True, date:2017-03-31 00:00:00, date_number: 363)
making sample. (inference: False, date:2017-02-28 00:00:00, date_number: 362)
[AttentionModel/2017-03-31 | get_logger | INFO]: ====================AttentionModel/2017-03-31 start====================
[strategy_integration.py] set_seed(2017-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5771)training: 2/10 (0.5771)validation : 2/10 (0.5750)training: 3/10 (0.5750)validation : 3/10 (0.5726)training: 4/10 (0.5726)validation : 4/10 (0.5726)training: 5/10 (0.5726)validation : 5/10 (0.5726)training: 6/10 (0.5726)validation : 6/10 (0.5726)early stopping at 6 with loss 0.5726
AttentionModel-training is done: 6/10
2017-02-28 | reset count: 0 | final loss: 0.5726 at epoch 3
making sample. (inference: True, date:2017-04-30 00:00:00, date_number: 364)
making sample. (inference: False, date:2017-03-31 00:00:00, date_number: 363)
[AttentionModel/2017-04-30 | get_logger | INFO]: ====================AttentionModel/2017-04-30 start====================
[strategy_integration.py] set_seed(2017-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5698)training: 2/10 (0.5698)validation : 2/10 (0.5698)training: 3/10 (0.5698)validation : 3/10 (0.5698)training: 4/10 (0.5698)validation : 4/10 (0.5698)training: 5/10 (0.5698)validation : 5/10 (0.5698)early stopping at 5 with loss 0.5698
AttentionModel-training is done: 5/10
2017-03-31 | reset count: 0 | final loss: 0.5698 at epoch 1
making sample. (inference: True, date:2017-05-31 00:00:00, date_number: 365)
making sample. (inference: False, date:2017-04-30 00:00:00, date_number: 364)
[AttentionModel/2017-05-31 | get_logger | INFO]: ====================AttentionModel/2017-05-31 start====================
[strategy_integration.py] set_seed(2017-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5662)training: 2/10 (0.5662)validation : 2/10 (0.5649)training: 3/10 (0.5649)validation : 3/10 (0.5649)training: 4/10 (0.5649)validation : 4/10 (0.5649)training: 5/10 (0.5649)validation : 5/10 (0.5649)early stopping at 5 with loss 0.5649
AttentionModel-training is done: 5/10
2017-04-30 | reset count: 0 | final loss: 0.5649 at epoch 2
making sample. (inference: True, date:2017-06-30 00:00:00, date_number: 366)
making sample. (inference: False, date:2017-05-31 00:00:00, date_number: 365)
[AttentionModel/2017-06-30 | get_logger | INFO]: ====================AttentionModel/2017-06-30 start====================
[strategy_integration.py] set_seed(2017-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5555)training: 2/10 (0.5555)validation : 2/10 (0.5555)training: 3/10 (0.5555)validation : 3/10 (0.5555)training: 4/10 (0.5555)validation : 4/10 (0.5514)training: 5/10 (0.5514)validation : 5/10 (0.5514)training: 6/10 (0.5514)validation : 6/10 (0.5514)training: 7/10 (0.5514)validation : 7/10 (0.5514)early stopping at 7 with loss 0.5514
AttentionModel-training is done: 7/10
2017-05-31 | reset count: 0 | final loss: 0.5514 at epoch 4
[strategy_integration.py] set_seed(2017-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5687)training: 2/10 (0.5687)validation : 2/10 (0.5687)training: 3/10 (0.5687)validation : 3/10 (0.5687)training: 4/10 (0.5687)validation : 4/10 (0.5674)training: 5/10 (0.5674)validation : 5/10 (0.5674)training: 6/10 (0.5674)validation : 6/10 (0.5674)early stopping at 6 with loss 0.5674
AttentionModel-training is done: 6/10
2017-06-30 | reset count: 0 | final loss: 0.5674 at epoch 4
making sample. (inference: True, date:2017-07-31 00:00:00, date_number: 367)
making sample. (inference: False, date:2017-06-30 00:00:00, date_number: 366)
[AttentionModel/2017-07-31 | get_logger | INFO]: ====================AttentionModel/2017-07-31 start====================
making sample. (inference: True, date:2017-08-31 00:00:00, date_number: 368)
making sample. (inference: False, date:2017-07-31 00:00:00, date_number: 367)
[AttentionModel/2017-08-31 | get_logger | INFO]: ====================AttentionModel/2017-08-31 start====================
[strategy_integration.py] set_seed(2017-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5640)training: 2/10 (0.5640)validation : 2/10 (0.5640)training: 3/10 (0.5640)validation : 3/10 (0.5640)training: 4/10 (0.5640)validation : 4/10 (0.5640)training: 5/10 (0.5640)validation : 5/10 (0.5640)training: 6/10 (0.5640)validation : 6/10 (0.5587)training: 7/10 (0.5587)validation : 7/10 (0.5587)training: 8/10 (0.5587)validation : 8/10 (0.5587)training: 9/10 (0.5587)validation : 9/10 (0.5587)early stopping at 9 with loss 0.5587
AttentionModel-training is done: 9/10
2017-07-31 | reset count: 0 | final loss: 0.5587 at epoch 6
[strategy_integration.py] set_seed(2017-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5647)training: 2/10 (0.5647)validation : 2/10 (0.5647)training: 3/10 (0.5647)validation : 3/10 (0.5613)training: 4/10 (0.5613)validation : 4/10 (0.5613)training: 5/10 (0.5613)validation : 5/10 (0.5613)training: 6/10 (0.5613)validation : 6/10 (0.5613)early stopping at 6 with loss 0.5613
AttentionModel-training is done: 6/10
2017-08-31 | reset count: 0 | final loss: 0.5613 at epoch 3
making sample. (inference: True, date:2017-09-30 00:00:00, date_number: 369)
making sample. (inference: False, date:2017-08-31 00:00:00, date_number: 368)
[AttentionModel/2017-09-30 | get_logger | INFO]: ====================AttentionModel/2017-09-30 start====================
making sample. (inference: True, date:2017-10-31 00:00:00, date_number: 370)
making sample. (inference: False, date:2017-09-30 00:00:00, date_number: 369)
[AttentionModel/2017-10-31 | get_logger | INFO]: ====================AttentionModel/2017-10-31 start====================
[strategy_integration.py] set_seed(2017-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5615)training: 2/10 (0.5615)validation : 2/10 (0.5615)training: 3/10 (0.5615)validation : 3/10 (0.5615)training: 4/10 (0.5615)validation : 4/10 (0.5594)training: 5/10 (0.5594)validation : 5/10 (0.5594)training: 6/10 (0.5594)validation : 6/10 (0.5594)training: 7/10 (0.5594)validation : 7/10 (0.5594)early stopping at 7 with loss 0.5594
AttentionModel-training is done: 7/10
2017-09-30 | reset count: 0 | final loss: 0.5594 at epoch 4
[strategy_integration.py] set_seed(2017-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5559)training: 2/10 (0.5559)validation : 2/10 (0.5511)training: 3/10 (0.5511)validation : 3/10 (0.5511)training: 4/10 (0.5511)validation : 4/10 (0.5511)training: 5/10 (0.5511)validation : 5/10 (0.5505)training: 6/10 (0.5505)validation : 6/10 (0.5505)training: 7/10 (0.5505)validation : 7/10 (0.5505)early stopping at 7 with loss 0.5505
AttentionModel-training is done: 7/10
2017-10-31 | reset count: 0 | final loss: 0.5505 at epoch 5
making sample. (inference: True, date:2017-11-30 00:00:00, date_number: 371)
making sample. (inference: False, date:2017-10-31 00:00:00, date_number: 370)
[AttentionModel/2017-11-30 | get_logger | INFO]: ====================AttentionModel/2017-11-30 start====================
making sample. (inference: True, date:2017-12-31 00:00:00, date_number: 372)
making sample. (inference: False, date:2017-11-30 00:00:00, date_number: 371)
[AttentionModel/2017-12-31 | get_logger | INFO]: ====================AttentionModel/2017-12-31 start====================
[strategy_integration.py] set_seed(2017-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5617)training: 2/10 (0.5617)validation : 2/10 (0.5617)training: 3/10 (0.5617)validation : 3/10 (0.5573)training: 4/10 (0.5573)validation : 4/10 (0.5573)training: 5/10 (0.5573)validation : 5/10 (0.5559)training: 6/10 (0.5559)validation : 6/10 (0.5559)early stopping at 6 with loss 0.5559
AttentionModel-training is done: 6/10
2017-11-30 | reset count: 0 | final loss: 0.5559 at epoch 5
[strategy_integration.py] set_seed(2017-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5641)training: 2/10 (0.5641)validation : 2/10 (0.5631)training: 3/10 (0.5631)validation : 3/10 (0.5602)training: 4/10 (0.5602)validation : 4/10 (0.5602)training: 5/10 (0.5602)validation : 5/10 (0.5602)early stopping at 5 with loss 0.5602
AttentionModel-training is done: 5/10
2017-12-31 | reset count: 0 | final loss: 0.5602 at epoch 3
making sample. (inference: True, date:2018-01-31 00:00:00, date_number: 373)
making sample. (inference: False, date:2017-12-31 00:00:00, date_number: 372)
[AttentionModel/2018-01-31 | get_logger | INFO]: ====================AttentionModel/2018-01-31 start====================
making sample. (inference: True, date:2018-02-28 00:00:00, date_number: 374)
making sample. (inference: False, date:2018-01-31 00:00:00, date_number: 373)
[AttentionModel/2018-02-28 | get_logger | INFO]: ====================AttentionModel/2018-02-28 start====================
[strategy_integration.py] set_seed(2018-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5674)training: 2/10 (0.5674)validation : 2/10 (0.5674)training: 3/10 (0.5674)validation : 3/10 (0.5674)training: 4/10 (0.5674)validation : 4/10 (0.5674)training: 5/10 (0.5674)validation : 5/10 (0.5674)training: 6/10 (0.5674)validation : 6/10 (0.5674)training: 7/10 (0.5674)validation : 7/10 (0.5674)early stopping at 7 with loss 0.5674
AttentionModel-training is done: 7/10
2018-01-31 | reset count: 0 | final loss: 0.5674 at epoch 1
[strategy_integration.py] set_seed(2018-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5579)training: 2/10 (0.5579)validation : 2/10 (0.5544)training: 3/10 (0.5544)validation : 3/10 (0.5517)training: 4/10 (0.5517)validation : 4/10 (0.5517)training: 5/10 (0.5517)validation : 5/10 (0.5517)training: 6/10 (0.5517)validation : 6/10 (0.5517)early stopping at 6 with loss 0.5517
AttentionModel-training is done: 6/10
2018-02-28 | reset count: 0 | final loss: 0.5517 at epoch 3
making sample. (inference: True, date:2018-03-31 00:00:00, date_number: 375)
making sample. (inference: False, date:2018-02-28 00:00:00, date_number: 374)
[AttentionModel/2018-03-31 | get_logger | INFO]: ====================AttentionModel/2018-03-31 start====================
making sample. (inference: True, date:2018-04-30 00:00:00, date_number: 376)
making sample. (inference: False, date:2018-03-31 00:00:00, date_number: 375)
[AttentionModel/2018-04-30 | get_logger | INFO]: ====================AttentionModel/2018-04-30 start====================
[strategy_integration.py] set_seed(2018-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5605)training: 2/10 (0.5605)validation : 2/10 (0.5589)training: 3/10 (0.5589)validation : 3/10 (0.5584)training: 4/10 (0.5584)validation : 4/10 (0.5544)training: 5/10 (0.5544)validation : 5/10 (0.5544)training: 6/10 (0.5544)validation : 6/10 (0.5544)training: 7/10 (0.5544)validation : 7/10 (0.5544)early stopping at 7 with loss 0.5544
AttentionModel-training is done: 7/10
2018-03-31 | reset count: 0 | final loss: 0.5544 at epoch 4
making sample. (inference: True, date:2018-05-31 00:00:00, date_number: 377)
making sample. (inference: False, date:2018-04-30 00:00:00, date_number: 376)
[AttentionModel/2018-05-31 | get_logger | INFO]: ====================AttentionModel/2018-05-31 start====================
[strategy_integration.py] set_seed(2018-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5748)training: 2/10 (0.5748)validation : 2/10 (0.5704)training: 3/10 (0.5704)validation : 3/10 (0.5704)training: 4/10 (0.5704)validation : 4/10 (0.5678)training: 5/10 (0.5678)validation : 5/10 (0.5663)training: 6/10 (0.5663)validation : 6/10 (0.5663)training: 7/10 (0.5663)validation : 7/10 (0.5663)training: 8/10 (0.5663)validation : 8/10 (0.5663)early stopping at 8 with loss 0.5663
AttentionModel-training is done: 8/10
2018-04-30 | reset count: 0 | final loss: 0.5663 at epoch 5
making sample. (inference: True, date:2018-06-30 00:00:00, date_number: 378)
making sample. (inference: False, date:2018-05-31 00:00:00, date_number: 377)
[AttentionModel/2018-06-30 | get_logger | INFO]: ====================AttentionModel/2018-06-30 start====================
[strategy_integration.py] set_seed(2018-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5692)training: 2/10 (0.5692)validation : 2/10 (0.5682)training: 3/10 (0.5682)validation : 3/10 (0.5677)training: 4/10 (0.5677)validation : 4/10 (0.5670)training: 5/10 (0.5670)validation : 5/10 (0.5659)training: 6/10 (0.5659)validation : 6/10 (0.5659)training: 7/10 (0.5659)validation : 7/10 (0.5637)training: 8/10 (0.5637)validation : 8/10 (0.5637)training: 9/10 (0.5637)validation : 9/10 (0.5637)early stopping at 9 with loss 0.5637
AttentionModel-training is done: 9/10
2018-05-31 | reset count: 0 | final loss: 0.5637 at epoch 7
[strategy_integration.py] set_seed(2018-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5680)training: 2/10 (0.5680)validation : 2/10 (0.5607)training: 3/10 (0.5607)validation : 3/10 (0.5607)training: 4/10 (0.5607)validation : 4/10 (0.5607)training: 5/10 (0.5607)validation : 5/10 (0.5607)training: 6/10 (0.5607)validation : 6/10 (0.5607)early stopping at 6 with loss 0.5607
AttentionModel-training is done: 6/10
2018-06-30 | reset count: 0 | final loss: 0.5607 at epoch 2
making sample. (inference: True, date:2018-07-31 00:00:00, date_number: 379)
making sample. (inference: False, date:2018-06-30 00:00:00, date_number: 378)
[AttentionModel/2018-07-31 | get_logger | INFO]: ====================AttentionModel/2018-07-31 start====================
making sample. (inference: True, date:2018-08-31 00:00:00, date_number: 380)
making sample. (inference: False, date:2018-07-31 00:00:00, date_number: 379)
[AttentionModel/2018-08-31 | get_logger | INFO]: ====================AttentionModel/2018-08-31 start====================
[strategy_integration.py] set_seed(2018-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5807)training: 2/10 (0.5807)validation : 2/10 (0.5807)training: 3/10 (0.5807)validation : 3/10 (0.5800)training: 4/10 (0.5800)validation : 4/10 (0.5800)training: 5/10 (0.5800)validation : 5/10 (0.5800)early stopping at 5 with loss 0.5800
AttentionModel-training is done: 5/10
2018-07-31 | reset count: 0 | final loss: 0.5800 at epoch 3
making sample. (inference: True, date:2018-09-30 00:00:00, date_number: 381)
making sample. (inference: False, date:2018-08-31 00:00:00, date_number: 380)
[AttentionModel/2018-09-30 | get_logger | INFO]: ====================AttentionModel/2018-09-30 start====================
[strategy_integration.py] set_seed(2018-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5732)training: 2/10 (0.5732)validation : 2/10 (0.5700)training: 3/10 (0.5700)validation : 3/10 (0.5681)training: 4/10 (0.5681)validation : 4/10 (0.5681)training: 5/10 (0.5681)validation : 5/10 (0.5681)training: 6/10 (0.5681)validation : 6/10 (0.5669)training: 7/10 (0.5669)validation : 7/10 (0.5669)training: 8/10 (0.5669)validation : 8/10 (0.5669)early stopping at 8 with loss 0.5669
AttentionModel-training is done: 8/10
2018-08-31 | reset count: 0 | final loss: 0.5669 at epoch 6
[strategy_integration.py] set_seed(2018-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5700)training: 2/10 (0.5700)validation : 2/10 (0.5700)training: 3/10 (0.5700)validation : 3/10 (0.5700)training: 4/10 (0.5700)validation : 4/10 (0.5700)training: 5/10 (0.5700)validation : 5/10 (0.5700)early stopping at 5 with loss 0.5700
AttentionModel-training is done: 5/10
2018-09-30 | reset count: 0 | final loss: 0.5700 at epoch 1
making sample. (inference: True, date:2018-10-31 00:00:00, date_number: 382)
making sample. (inference: False, date:2018-09-30 00:00:00, date_number: 381)
[AttentionModel/2018-10-31 | get_logger | INFO]: ====================AttentionModel/2018-10-31 start====================
making sample. (inference: True, date:2018-11-30 00:00:00, date_number: 383)
making sample. (inference: False, date:2018-10-31 00:00:00, date_number: 382)
[AttentionModel/2018-11-30 | get_logger | INFO]: ====================AttentionModel/2018-11-30 start====================
[strategy_integration.py] set_seed(2018-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5913)training: 2/10 (0.5913)validation : 2/10 (0.5835)training: 3/10 (0.5835)validation : 3/10 (0.5806)training: 4/10 (0.5806)validation : 4/10 (0.5806)training: 5/10 (0.5806)validation : 5/10 (0.5806)training: 6/10 (0.5806)validation : 6/10 (0.5806)early stopping at 6 with loss 0.5806
AttentionModel-training is done: 6/10
2018-10-31 | reset count: 0 | final loss: 0.5806 at epoch 3
[strategy_integration.py] set_seed(2018-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5771)training: 2/10 (0.5771)validation : 2/10 (0.5771)training: 3/10 (0.5771)validation : 3/10 (0.5760)training: 4/10 (0.5760)validation : 4/10 (0.5760)training: 5/10 (0.5760)validation : 5/10 (0.5760)early stopping at 5 with loss 0.5760
AttentionModel-training is done: 5/10
2018-11-30 | reset count: 0 | final loss: 0.5760 at epoch 3
making sample. (inference: True, date:2018-12-31 00:00:00, date_number: 384)
making sample. (inference: False, date:2018-11-30 00:00:00, date_number: 383)
[AttentionModel/2018-12-31 | get_logger | INFO]: ====================AttentionModel/2018-12-31 start====================
making sample. (inference: True, date:2019-01-31 00:00:00, date_number: 385)
making sample. (inference: False, date:2018-12-31 00:00:00, date_number: 384)
[AttentionModel/2019-01-31 | get_logger | INFO]: ====================AttentionModel/2019-01-31 start====================
[strategy_integration.py] set_seed(2018-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5758)training: 2/10 (0.5758)validation : 2/10 (0.5758)training: 3/10 (0.5758)validation : 3/10 (0.5741)training: 4/10 (0.5741)validation : 4/10 (0.5735)training: 5/10 (0.5735)validation : 5/10 (0.5727)training: 6/10 (0.5727)validation : 6/10 (0.5727)early stopping at 6 with loss 0.5727
AttentionModel-training is done: 6/10
2018-12-31 | reset count: 0 | final loss: 0.5727 at epoch 5
[strategy_integration.py] set_seed(2019-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5901)training: 2/10 (0.5901)validation : 2/10 (0.5879)training: 3/10 (0.5879)validation : 3/10 (0.5876)training: 4/10 (0.5876)validation : 4/10 (0.5876)training: 5/10 (0.5876)validation : 5/10 (0.5847)training: 6/10 (0.5847)validation : 6/10 (0.5847)early stopping at 6 with loss 0.5847
AttentionModel-training is done: 6/10
2019-01-31 | reset count: 0 | final loss: 0.5847 at epoch 5
making sample. (inference: True, date:2019-02-28 00:00:00, date_number: 386)
making sample. (inference: False, date:2019-01-31 00:00:00, date_number: 385)
[AttentionModel/2019-02-28 | get_logger | INFO]: ====================AttentionModel/2019-02-28 start====================
making sample. (inference: True, date:2019-03-31 00:00:00, date_number: 387)
making sample. (inference: False, date:2019-02-28 00:00:00, date_number: 386)
[AttentionModel/2019-03-31 | get_logger | INFO]: ====================AttentionModel/2019-03-31 start====================
[strategy_integration.py] set_seed(2019-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5796)training: 2/10 (0.5796)validation : 2/10 (0.5776)training: 3/10 (0.5776)validation : 3/10 (0.5757)training: 4/10 (0.5757)validation : 4/10 (0.5757)training: 5/10 (0.5757)validation : 5/10 (0.5757)training: 6/10 (0.5757)validation : 6/10 (0.5757)early stopping at 6 with loss 0.5757
AttentionModel-training is done: 6/10
2019-02-28 | reset count: 0 | final loss: 0.5757 at epoch 3
making sample. (inference: True, date:2019-04-30 00:00:00, date_number: 388)
making sample. (inference: False, date:2019-03-31 00:00:00, date_number: 387)
[AttentionModel/2019-04-30 | get_logger | INFO]: ====================AttentionModel/2019-04-30 start====================
[strategy_integration.py] set_seed(2019-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5791)training: 2/10 (0.5791)validation : 2/10 (0.5791)training: 3/10 (0.5791)validation : 3/10 (0.5779)training: 4/10 (0.5779)validation : 4/10 (0.5779)training: 5/10 (0.5779)validation : 5/10 (0.5779)training: 6/10 (0.5779)validation : 6/10 (0.5744)training: 7/10 (0.5744)validation : 7/10 (0.5744)training: 8/10 (0.5744)validation : 8/10 (0.5744)early stopping at 8 with loss 0.5744
AttentionModel-training is done: 8/10
2019-03-31 | reset count: 0 | final loss: 0.5744 at epoch 6
making sample. (inference: True, date:2019-05-31 00:00:00, date_number: 389)
making sample. (inference: False, date:2019-04-30 00:00:00, date_number: 388)
[AttentionModel/2019-05-31 | get_logger | INFO]: ====================AttentionModel/2019-05-31 start====================
[strategy_integration.py] set_seed(2019-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5863)training: 2/10 (0.5863)validation : 2/10 (0.5854)training: 3/10 (0.5854)validation : 3/10 (0.5854)training: 4/10 (0.5854)validation : 4/10 (0.5839)training: 5/10 (0.5839)validation : 5/10 (0.5839)training: 6/10 (0.5839)validation : 6/10 (0.5810)training: 7/10 (0.5810)validation : 7/10 (0.5794)training: 8/10 (0.5794)validation : 8/10 (0.5794)early stopping at 8 with loss 0.5794
AttentionModel-training is done: 8/10
2019-04-30 | reset count: 0 | final loss: 0.5794 at epoch 7
[strategy_integration.py] set_seed(2019-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5923)training: 2/10 (0.5923)validation : 2/10 (0.5889)training: 3/10 (0.5889)validation : 3/10 (0.5866)training: 4/10 (0.5866)validation : 4/10 (0.5866)training: 5/10 (0.5866)validation : 5/10 (0.5866)training: 6/10 (0.5866)validation : 6/10 (0.5857)training: 7/10 (0.5857)validation : 7/10 (0.5857)early stopping at 7 with loss 0.5857
AttentionModel-training is done: 7/10
2019-05-31 | reset count: 0 | final loss: 0.5857 at epoch 6
making sample. (inference: True, date:2019-06-30 00:00:00, date_number: 390)
making sample. (inference: False, date:2019-05-31 00:00:00, date_number: 389)
[AttentionModel/2019-06-30 | get_logger | INFO]: ====================AttentionModel/2019-06-30 start====================
making sample. (inference: True, date:2019-07-31 00:00:00, date_number: 391)
making sample. (inference: False, date:2019-06-30 00:00:00, date_number: 390)
[AttentionModel/2019-07-31 | get_logger | INFO]: ====================AttentionModel/2019-07-31 start====================
[strategy_integration.py] set_seed(2019-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5773)training: 2/10 (0.5773)validation : 2/10 (0.5762)training: 3/10 (0.5762)validation : 3/10 (0.5762)training: 4/10 (0.5762)validation : 4/10 (0.5748)training: 5/10 (0.5748)validation : 5/10 (0.5748)training: 6/10 (0.5748)validation : 6/10 (0.5748)early stopping at 6 with loss 0.5748
AttentionModel-training is done: 6/10
2019-06-30 | reset count: 0 | final loss: 0.5748 at epoch 4
[strategy_integration.py] set_seed(2019-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5866)training: 2/10 (0.5866)validation : 2/10 (0.5839)training: 3/10 (0.5839)validation : 3/10 (0.5818)training: 4/10 (0.5818)validation : 4/10 (0.5813)training: 5/10 (0.5813)validation : 5/10 (0.5813)training: 6/10 (0.5813)validation : 6/10 (0.5813)early stopping at 6 with loss 0.5813
AttentionModel-training is done: 6/10
2019-07-31 | reset count: 0 | final loss: 0.5813 at epoch 4
making sample. (inference: True, date:2019-08-31 00:00:00, date_number: 392)
making sample. (inference: False, date:2019-07-31 00:00:00, date_number: 391)
[AttentionModel/2019-08-31 | get_logger | INFO]: ====================AttentionModel/2019-08-31 start====================
making sample. (inference: True, date:2019-09-30 00:00:00, date_number: 393)
making sample. (inference: False, date:2019-08-31 00:00:00, date_number: 392)
[AttentionModel/2019-09-30 | get_logger | INFO]: ====================AttentionModel/2019-09-30 start====================
[strategy_integration.py] set_seed(2019-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5858)training: 2/10 (0.5858)validation : 2/10 (0.5850)training: 3/10 (0.5850)validation : 3/10 (0.5850)training: 4/10 (0.5850)validation : 4/10 (0.5826)training: 5/10 (0.5826)validation : 5/10 (0.5826)training: 6/10 (0.5826)validation : 6/10 (0.5826)training: 7/10 (0.5826)validation : 7/10 (0.5826)early stopping at 7 with loss 0.5826
AttentionModel-training is done: 7/10
2019-08-31 | reset count: 0 | final loss: 0.5826 at epoch 4
[strategy_integration.py] set_seed(2019-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5861)training: 2/10 (0.5861)validation : 2/10 (0.5861)training: 3/10 (0.5861)validation : 3/10 (0.5827)training: 4/10 (0.5827)validation : 4/10 (0.5827)training: 5/10 (0.5827)validation : 5/10 (0.5827)early stopping at 5 with loss 0.5827
AttentionModel-training is done: 5/10
2019-09-30 | reset count: 0 | final loss: 0.5827 at epoch 3
making sample. (inference: True, date:2019-10-31 00:00:00, date_number: 394)
making sample. (inference: False, date:2019-09-30 00:00:00, date_number: 393)
[AttentionModel/2019-10-31 | get_logger | INFO]: ====================AttentionModel/2019-10-31 start====================
making sample. (inference: True, date:2019-11-30 00:00:00, date_number: 395)
making sample. (inference: False, date:2019-10-31 00:00:00, date_number: 394)
[AttentionModel/2019-11-30 | get_logger | INFO]: ====================AttentionModel/2019-11-30 start====================
[strategy_integration.py] set_seed(2019-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5889)training: 2/10 (0.5889)validation : 2/10 (0.5879)training: 3/10 (0.5879)validation : 3/10 (0.5857)training: 4/10 (0.5857)validation : 4/10 (0.5797)training: 5/10 (0.5797)validation : 5/10 (0.5797)training: 6/10 (0.5797)validation : 6/10 (0.5797)training: 7/10 (0.5797)validation : 7/10 (0.5797)early stopping at 7 with loss 0.5797
AttentionModel-training is done: 7/10
2019-10-31 | reset count: 0 | final loss: 0.5797 at epoch 4
[strategy_integration.py] set_seed(2019-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5925)training: 2/10 (0.5925)validation : 2/10 (0.5914)training: 3/10 (0.5914)validation : 3/10 (0.5914)training: 4/10 (0.5914)validation : 4/10 (0.5871)training: 5/10 (0.5871)validation : 5/10 (0.5871)training: 6/10 (0.5871)validation : 6/10 (0.5871)training: 7/10 (0.5871)validation : 7/10 (0.5871)early stopping at 7 with loss 0.5871
AttentionModel-training is done: 7/10
2019-11-30 | reset count: 0 | final loss: 0.5871 at epoch 4
making sample. (inference: True, date:2019-12-31 00:00:00, date_number: 396)
making sample. (inference: False, date:2019-11-30 00:00:00, date_number: 395)
[AttentionModel/2019-12-31 | get_logger | INFO]: ====================AttentionModel/2019-12-31 start====================
making sample. (inference: True, date:2020-01-31 00:00:00, date_number: 397)
making sample. (inference: False, date:2019-12-31 00:00:00, date_number: 396)
[AttentionModel/2020-01-31 | get_logger | INFO]: ====================AttentionModel/2020-01-31 start====================
[strategy_integration.py] set_seed(2019-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5842)training: 2/10 (0.5842)validation : 2/10 (0.5802)training: 3/10 (0.5802)validation : 3/10 (0.5802)training: 4/10 (0.5802)validation : 4/10 (0.5802)training: 5/10 (0.5802)validation : 5/10 (0.5802)early stopping at 5 with loss 0.5802
AttentionModel-training is done: 5/10
2019-12-31 | reset count: 0 | final loss: 0.5802 at epoch 2
making sample. (inference: True, date:2020-02-29 00:00:00, date_number: 398)
making sample. (inference: False, date:2020-01-31 00:00:00, date_number: 397)
[AttentionModel/2020-02-29 | get_logger | INFO]: ====================AttentionModel/2020-02-29 start====================
[strategy_integration.py] set_seed(2020-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5884)training: 2/10 (0.5884)validation : 2/10 (0.5859)training: 3/10 (0.5859)validation : 3/10 (0.5859)training: 4/10 (0.5859)validation : 4/10 (0.5859)training: 5/10 (0.5859)validation : 5/10 (0.5813)training: 6/10 (0.5813)validation : 6/10 (0.5813)training: 7/10 (0.5813)validation : 7/10 (0.5813)training: 8/10 (0.5813)validation : 8/10 (0.5813)early stopping at 8 with loss 0.5813
AttentionModel-training is done: 8/10
2020-01-31 | reset count: 0 | final loss: 0.5813 at epoch 5
[strategy_integration.py] set_seed(2020-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5795)training: 2/10 (0.5795)validation : 2/10 (0.5776)training: 3/10 (0.5776)validation : 3/10 (0.5776)training: 4/10 (0.5776)validation : 4/10 (0.5776)training: 5/10 (0.5776)validation : 5/10 (0.5776)early stopping at 5 with loss 0.5776
AttentionModel-training is done: 5/10
2020-02-29 | reset count: 0 | final loss: 0.5776 at epoch 2
making sample. (inference: True, date:2020-03-31 00:00:00, date_number: 399)
making sample. (inference: False, date:2020-02-29 00:00:00, date_number: 398)
[AttentionModel/2020-03-31 | get_logger | INFO]: ====================AttentionModel/2020-03-31 start====================
making sample. (inference: True, date:2020-04-30 00:00:00, date_number: 400)
making sample. (inference: False, date:2020-03-31 00:00:00, date_number: 399)
[AttentionModel/2020-04-30 | get_logger | INFO]: ====================AttentionModel/2020-04-30 start====================
[strategy_integration.py] set_seed(2020-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5865)training: 2/10 (0.5865)validation : 2/10 (0.5828)training: 3/10 (0.5828)validation : 3/10 (0.5816)training: 4/10 (0.5816)validation : 4/10 (0.5816)training: 5/10 (0.5816)validation : 5/10 (0.5816)training: 6/10 (0.5816)validation : 6/10 (0.5816)early stopping at 6 with loss 0.5816
AttentionModel-training is done: 6/10
2020-03-31 | reset count: 0 | final loss: 0.5816 at epoch 3
[strategy_integration.py] set_seed(2020-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5715)training: 2/10 (0.5715)validation : 2/10 (0.5715)training: 3/10 (0.5715)validation : 3/10 (0.5681)training: 4/10 (0.5681)validation : 4/10 (0.5681)training: 5/10 (0.5681)validation : 5/10 (0.5681)training: 6/10 (0.5681)validation : 6/10 (0.5681)early stopping at 6 with loss 0.5681
AttentionModel-training is done: 6/10
2020-04-30 | reset count: 0 | final loss: 0.5681 at epoch 3
making sample. (inference: True, date:2020-05-31 00:00:00, date_number: 401)
making sample. (inference: False, date:2020-04-30 00:00:00, date_number: 400)
[AttentionModel/2020-05-31 | get_logger | INFO]: ====================AttentionModel/2020-05-31 start====================
making sample. (inference: True, date:2020-06-30 00:00:00, date_number: 402)
making sample. (inference: False, date:2020-05-31 00:00:00, date_number: 401)
[AttentionModel/2020-06-30 | get_logger | INFO]: ====================AttentionModel/2020-06-30 start====================
[strategy_integration.py] set_seed(2020-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5800)training: 2/10 (0.5800)validation : 2/10 (0.5759)training: 3/10 (0.5759)validation : 3/10 (0.5751)training: 4/10 (0.5751)validation : 4/10 (0.5751)training: 5/10 (0.5751)validation : 5/10 (0.5751)training: 6/10 (0.5751)validation : 6/10 (0.5751)early stopping at 6 with loss 0.5751
AttentionModel-training is done: 6/10
2020-05-31 | reset count: 0 | final loss: 0.5751 at epoch 3
[strategy_integration.py] set_seed(2020-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5889)training: 2/10 (0.5889)validation : 2/10 (0.5861)training: 3/10 (0.5861)validation : 3/10 (0.5844)training: 4/10 (0.5844)validation : 4/10 (0.5844)training: 5/10 (0.5844)validation : 5/10 (0.5844)training: 6/10 (0.5844)validation : 6/10 (0.5844)early stopping at 6 with loss 0.5844
AttentionModel-training is done: 6/10
2020-06-30 | reset count: 0 | final loss: 0.5844 at epoch 3
making sample. (inference: True, date:2020-07-31 00:00:00, date_number: 403)
making sample. (inference: False, date:2020-06-30 00:00:00, date_number: 402)
[AttentionModel/2020-07-31 | get_logger | INFO]: ====================AttentionModel/2020-07-31 start====================
making sample. (inference: True, date:2020-08-31 00:00:00, date_number: 404)
making sample. (inference: False, date:2020-07-31 00:00:00, date_number: 403)
[AttentionModel/2020-08-31 | get_logger | INFO]: ====================AttentionModel/2020-08-31 start====================
[strategy_integration.py] set_seed(2020-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5757)training: 2/10 (0.5757)validation : 2/10 (0.5711)training: 3/10 (0.5711)validation : 3/10 (0.5682)training: 4/10 (0.5682)validation : 4/10 (0.5682)training: 5/10 (0.5682)validation : 5/10 (0.5682)training: 6/10 (0.5682)validation : 6/10 (0.5682)early stopping at 6 with loss 0.5682
AttentionModel-training is done: 6/10
2020-07-31 | reset count: 0 | final loss: 0.5682 at epoch 3
[strategy_integration.py] set_seed(2020-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5701)training: 2/10 (0.5701)validation : 2/10 (0.5685)training: 3/10 (0.5685)validation : 3/10 (0.5685)training: 4/10 (0.5685)validation : 4/10 (0.5685)training: 5/10 (0.5685)validation : 5/10 (0.5685)early stopping at 5 with loss 0.5685
AttentionModel-training is done: 5/10
2020-08-31 | reset count: 0 | final loss: 0.5685 at epoch 2
making sample. (inference: True, date:2020-09-30 00:00:00, date_number: 405)
making sample. (inference: False, date:2020-08-31 00:00:00, date_number: 404)
[AttentionModel/2020-09-30 | get_logger | INFO]: ====================AttentionModel/2020-09-30 start====================
making sample. (inference: True, date:2020-10-31 00:00:00, date_number: 406)
making sample. (inference: False, date:2020-09-30 00:00:00, date_number: 405)
[AttentionModel/2020-10-31 | get_logger | INFO]: ====================AttentionModel/2020-10-31 start====================
[strategy_integration.py] set_seed(2020-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5750)training: 2/10 (0.5750)validation : 2/10 (0.5691)training: 3/10 (0.5691)validation : 3/10 (0.5691)training: 4/10 (0.5691)validation : 4/10 (0.5691)training: 5/10 (0.5691)validation : 5/10 (0.5691)early stopping at 5 with loss 0.5691
AttentionModel-training is done: 5/10
2020-09-30 | reset count: 0 | final loss: 0.5691 at epoch 2
[strategy_integration.py] set_seed(2020-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5776)training: 2/10 (0.5776)validation : 2/10 (0.5741)training: 3/10 (0.5741)validation : 3/10 (0.5741)training: 4/10 (0.5741)validation : 4/10 (0.5741)training: 5/10 (0.5741)validation : 5/10 (0.5741)early stopping at 5 with loss 0.5741
AttentionModel-training is done: 5/10
2020-10-31 | reset count: 0 | final loss: 0.5741 at epoch 2
making sample. (inference: True, date:2020-11-30 00:00:00, date_number: 407)
making sample. (inference: False, date:2020-10-31 00:00:00, date_number: 406)
[AttentionModel/2020-11-30 | get_logger | INFO]: ====================AttentionModel/2020-11-30 start====================
making sample. (inference: True, date:2020-12-31 00:00:00, date_number: 408)
making sample. (inference: False, date:2020-11-30 00:00:00, date_number: 407)
[AttentionModel/2020-12-31 | get_logger | INFO]: ====================AttentionModel/2020-12-31 start====================
[strategy_integration.py] set_seed(2020-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5722)training: 2/10 (0.5722)validation : 2/10 (0.5722)training: 3/10 (0.5722)validation : 3/10 (0.5722)training: 4/10 (0.5722)validation : 4/10 (0.5722)training: 5/10 (0.5722)validation : 5/10 (0.5705)training: 6/10 (0.5705)validation : 6/10 (0.5682)training: 7/10 (0.5682)validation : 7/10 (0.5682)training: 8/10 (0.5682)validation : 8/10 (0.5682)training: 9/10 (0.5682)validation : 9/10 (0.5682)early stopping at 9 with loss 0.5682
AttentionModel-training is done: 9/10
2020-11-30 | reset count: 0 | final loss: 0.5682 at epoch 6
[strategy_integration.py] set_seed(2020-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5738)training: 2/10 (0.5738)validation : 2/10 (0.5692)training: 3/10 (0.5692)validation : 3/10 (0.5692)training: 4/10 (0.5692)validation : 4/10 (0.5670)training: 5/10 (0.5670)validation : 5/10 (0.5670)training: 6/10 (0.5670)validation : 6/10 (0.5642)training: 7/10 (0.5642)validation : 7/10 (0.5642)training: 8/10 (0.5642)validation : 8/10 (0.5642)early stopping at 8 with loss 0.5642
AttentionModel-training is done: 8/10
2020-12-31 | reset count: 0 | final loss: 0.5642 at epoch 6
making sample. (inference: True, date:2021-01-31 00:00:00, date_number: 409)
making sample. (inference: False, date:2020-12-31 00:00:00, date_number: 408)
[AttentionModel/2021-01-31 | get_logger | INFO]: ====================AttentionModel/2021-01-31 start====================
making sample. (inference: True, date:2021-02-28 00:00:00, date_number: 410)
making sample. (inference: False, date:2021-01-31 00:00:00, date_number: 409)
[AttentionModel/2021-02-28 | get_logger | INFO]: ====================AttentionModel/2021-02-28 start====================
[strategy_integration.py] set_seed(2021-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5560)training: 2/10 (0.5560)validation : 2/10 (0.5520)training: 3/10 (0.5520)validation : 3/10 (0.5520)training: 4/10 (0.5520)validation : 4/10 (0.5520)training: 5/10 (0.5520)validation : 5/10 (0.5520)training: 6/10 (0.5520)validation : 6/10 (0.5520)training: 7/10 (0.5520)validation : 7/10 (0.5520)early stopping at 7 with loss 0.5520
AttentionModel-training is done: 7/10
2021-01-31 | reset count: 0 | final loss: 0.5520 at epoch 2
[strategy_integration.py] set_seed(2021-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5611)training: 2/10 (0.5611)validation : 2/10 (0.5605)training: 3/10 (0.5605)validation : 3/10 (0.5587)training: 4/10 (0.5587)validation : 4/10 (0.5587)training: 5/10 (0.5587)validation : 5/10 (0.5587)early stopping at 5 with loss 0.5587
AttentionModel-training is done: 5/10
2021-02-28 | reset count: 0 | final loss: 0.5587 at epoch 3
making sample. (inference: True, date:2021-03-31 00:00:00, date_number: 411)
making sample. (inference: False, date:2021-02-28 00:00:00, date_number: 410)
[AttentionModel/2021-03-31 | get_logger | INFO]: ====================AttentionModel/2021-03-31 start====================
making sample. (inference: True, date:2021-04-30 00:00:00, date_number: 412)
making sample. (inference: False, date:2021-03-31 00:00:00, date_number: 411)
[AttentionModel/2021-04-30 | get_logger | INFO]: ====================AttentionModel/2021-04-30 start====================
[strategy_integration.py] set_seed(2021-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5786)training: 2/10 (0.5786)validation : 2/10 (0.5717)training: 3/10 (0.5717)validation : 3/10 (0.5717)training: 4/10 (0.5717)validation : 4/10 (0.5717)training: 5/10 (0.5717)validation : 5/10 (0.5717)training: 6/10 (0.5717)validation : 6/10 (0.5717)early stopping at 6 with loss 0.5717
AttentionModel-training is done: 6/10
2021-03-31 | reset count: 0 | final loss: 0.5717 at epoch 2
making sample. (inference: True, date:2021-05-31 00:00:00, date_number: 413)
making sample. (inference: False, date:2021-04-30 00:00:00, date_number: 412)
[AttentionModel/2021-05-31 | get_logger | INFO]: ====================AttentionModel/2021-05-31 start====================
[strategy_integration.py] set_seed(2021-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5640)training: 2/10 (0.5640)validation : 2/10 (0.5623)training: 3/10 (0.5623)validation : 3/10 (0.5604)training: 4/10 (0.5604)validation : 4/10 (0.5603)training: 5/10 (0.5603)validation : 5/10 (0.5599)training: 6/10 (0.5599)validation : 6/10 (0.5599)early stopping at 6 with loss 0.5599
AttentionModel-training is done: 6/10
2021-04-30 | reset count: 0 | final loss: 0.5599 at epoch 5
making sample. (inference: True, date:2021-06-30 00:00:00, date_number: 414)
making sample. (inference: False, date:2021-05-31 00:00:00, date_number: 413)
[AttentionModel/2021-06-30 | get_logger | INFO]: ====================AttentionModel/2021-06-30 start====================
[strategy_integration.py] set_seed(2021-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5693)training: 2/10 (0.5693)validation : 2/10 (0.5632)training: 3/10 (0.5632)validation : 3/10 (0.5632)training: 4/10 (0.5632)validation : 4/10 (0.5632)training: 5/10 (0.5632)validation : 5/10 (0.5616)training: 6/10 (0.5616)validation : 6/10 (0.5616)training: 7/10 (0.5616)validation : 7/10 (0.5616)training: 8/10 (0.5616)validation : 8/10 (0.5616)early stopping at 8 with loss 0.5616
AttentionModel-training is done: 8/10
2021-05-31 | reset count: 0 | final loss: 0.5616 at epoch 5
[strategy_integration.py] set_seed(2021-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5672)training: 2/10 (0.5672)validation : 2/10 (0.5672)training: 3/10 (0.5672)validation : 3/10 (0.5672)training: 4/10 (0.5672)validation : 4/10 (0.5672)training: 5/10 (0.5672)validation : 5/10 (0.5672)early stopping at 5 with loss 0.5672
AttentionModel-training is done: 5/10
2021-06-30 | reset count: 0 | final loss: 0.5672 at epoch 1
making sample. (inference: True, date:2021-07-31 00:00:00, date_number: 415)
making sample. (inference: False, date:2021-06-30 00:00:00, date_number: 414)
[AttentionModel/2021-07-31 | get_logger | INFO]: ====================AttentionModel/2021-07-31 start====================
making sample. (inference: True, date:2021-08-31 00:00:00, date_number: 416)
making sample. (inference: False, date:2021-07-31 00:00:00, date_number: 415)
[AttentionModel/2021-08-31 | get_logger | INFO]: ====================AttentionModel/2021-08-31 start====================
[strategy_integration.py] set_seed(2021-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5750)training: 2/10 (0.5750)validation : 2/10 (0.5738)training: 3/10 (0.5738)validation : 3/10 (0.5722)training: 4/10 (0.5722)validation : 4/10 (0.5698)training: 5/10 (0.5698)validation : 5/10 (0.5698)training: 6/10 (0.5698)validation : 6/10 (0.5698)early stopping at 6 with loss 0.5698
AttentionModel-training is done: 6/10
2021-07-31 | reset count: 0 | final loss: 0.5698 at epoch 4
[strategy_integration.py] set_seed(2021-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5789)training: 2/10 (0.5789)validation : 2/10 (0.5776)training: 3/10 (0.5776)validation : 3/10 (0.5740)training: 4/10 (0.5740)validation : 4/10 (0.5740)training: 5/10 (0.5740)validation : 5/10 (0.5740)training: 6/10 (0.5740)validation : 6/10 (0.5740)early stopping at 6 with loss 0.5740
AttentionModel-training is done: 6/10
2021-08-31 | reset count: 0 | final loss: 0.5740 at epoch 3
making sample. (inference: True, date:2021-09-30 00:00:00, date_number: 417)
making sample. (inference: False, date:2021-08-31 00:00:00, date_number: 416)
[AttentionModel/2021-09-30 | get_logger | INFO]: ====================AttentionModel/2021-09-30 start====================
making sample. (inference: True, date:2021-10-31 00:00:00, date_number: 418)
making sample. (inference: False, date:2021-09-30 00:00:00, date_number: 417)
[AttentionModel/2021-10-31 | get_logger | INFO]: ====================AttentionModel/2021-10-31 start====================
[strategy_integration.py] set_seed(2021-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5765)training: 2/10 (0.5765)validation : 2/10 (0.5675)training: 3/10 (0.5675)validation : 3/10 (0.5675)training: 4/10 (0.5675)validation : 4/10 (0.5675)training: 5/10 (0.5675)validation : 5/10 (0.5659)training: 6/10 (0.5659)validation : 6/10 (0.5659)early stopping at 6 with loss 0.5659
AttentionModel-training is done: 6/10
2021-09-30 | reset count: 0 | final loss: 0.5659 at epoch 5
[strategy_integration.py] set_seed(2021-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5896)training: 2/10 (0.5896)validation : 2/10 (0.5889)training: 3/10 (0.5889)validation : 3/10 (0.5813)training: 4/10 (0.5813)validation : 4/10 (0.5813)training: 5/10 (0.5813)validation : 5/10 (0.5813)training: 6/10 (0.5813)validation : 6/10 (0.5813)early stopping at 6 with loss 0.5813
AttentionModel-training is done: 6/10
2021-10-31 | reset count: 0 | final loss: 0.5813 at epoch 3
[strategy | get_logger | INFO]: ====================strategy start====================
[strategy | _get_infer_dirs | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick108_test03/infer 이 strategy 대상에 추가됨
[strategy | _backtest | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick108_test03/infer is starting
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.152 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading masking data✓ [Compustat API] : Loading get_historic_universe from cache done in 1.919 secs
⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.22 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.062 secs
⠋ [Compustat API] : Loading get_monthly_total_return from cache✓ [Compustat API] : Loading get_monthly_total_return from cache done in 0.061 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.077 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.071 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.069 secs
⠋ [Compustat API] : Loading get_monthly_volume_data from cache✓ [Compustat API] : Loading get_monthly_volume_data from cache done in 0.08 secs
[strategy | _allocating | INFO]: [STRATEGY] allocating start
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_weight_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] performance.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] combined.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] weight.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] universe.csv is saved.
[comparison | get_logger | INFO]: ====================comparison start====================
[comparison | compare | INFO]: [5. COMPARISON] comparison is saved.
[comparison | compare | INFO]:                        Harvest_Green_concept001_pick108_test03_price_return  ...  SPCOMP_total_win
2016-01-31_2021-04-30                                           0.247157     ...          0.765625

[1 rows x 27 columns]
2015-12-31
2016-01-31
2016-02-29
2016-03-31
2016-04-30
2016-05-31
2016-06-30
2016-07-31
2016-08-31
2016-09-30
2016-10-31
2016-11-30
2016-12-31
2017-01-31
2017-02-28
2017-03-31
2017-04-30
2017-05-31
2017-06-30
2017-07-31
2017-08-31
2017-09-30
2017-10-31
2017-11-30
2017-12-31
2018-01-31
2018-02-28
2018-03-31
2018-04-30
2018-05-31
2018-06-30
2018-07-31
2018-08-31
2018-09-30
2018-10-31
2018-11-30
2018-12-31
2019-01-31
2019-02-28
2019-03-31
2019-04-30
2019-05-31
2019-06-30
2019-07-31
2019-08-31
2019-09-30
2019-10-31
2019-11-30
2019-12-31
2020-01-31
2020-02-29
2020-03-31
2020-04-30
2020-05-31
2020-06-30
2020-07-31
2020-08-31
2020-09-30
2020-10-31
2020-11-30
2020-12-31
2021-01-31
2021-02-28
2021-03-31
2021-04-30
2021-05-31
2021-06-30
2021-07-31
2021-08-31
2021-09-30
2021-10-31
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.053 secs
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.049 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.068 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.084 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.078 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.671 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.687 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.689 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.688 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.69 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.08 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.677 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.675 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.677 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.671 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.668 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.083 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.691 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.077 secs
⠋ [Compustat API] : Cleaning past days cache✓ [Compustat API] : Cleaning past days cache done in 0.001 secs
auto caching이 활성화 되었습니다. /home/sronly/sr-storage/kirin_cache/kirin_api_cache/2.19.17_2021-11-23에 캐쉬를 저장합니다.
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.15 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache✓ [Compustat API] : Loading get_historic_universe from cache done in 2.031 secs
⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.332 secs
setting tensorflow random seed failed
load_data: mom_12m_1m
load_data: ram_12m_0m
load_data: vol_3m
load_data: t3y
load_data: t5y
load_data: t7y
load_data: aaa_t10y
load_data: baa_t10y
load_data: aaa_ff
load_data: real_pig
load_data: federal_tg
load_data: real_gdp
load_data: gpa
load_data: rev_surp
load_data: cash_at
load_data: retail_mfr
load_data: m1
load_data: m2
load_data: ae_m
load_data: ia_ta
load_data: rc_a
load_data: t1y_ff
load_data: t5y_ff
load_data: t10y_t2y
load_data: export_growth
load_data: import_growth
load_data: real_gig
load_data: op_lev
load_data: roe
load_data: std_u_e
load_data: baa_ff
load_data: core_cpi
load_data: core_pce
load_data: corporate_tg
load_data: industrial_prod
load_data: home_pr
load_data: res_mom_12m_1m_0m
load_data: res_vol_6m_3m_0m
load_data: at
load_data: ret_noa
load_data: etm
load_data: ia_mv
load_data: pr_1m_0m
load_data: mv
load_data: btm
load_data: snp500_pr
load_data: wilshire500_pr
load_data: ted
load_data: ppi
load_data: trimmed_pce
load_data: unemploy
load_data: wti
load_data: capa_util
load_data: snp500_pe
load_data: r_s
load_data: r_a
load_data: fred_ff
load_data: t3m
load_data: t6m
load_data: t2y
load_data: t10y
load_data: aaa
load_data: baa
load_data: core_ppi
load_data: cpi
load_data: pce
load_data: snp500_vol
load_data: sector_values
load_data: mv
load_data: pr_1m_0m
load_data: mv
making sample. (inference: True, date:2015-12-31 00:00:00, date_number: 348)
making sample. (inference: False, date:2012-12-31 00:00:00, date_number: 312)
making sample. (inference: False, date:2013-01-31 00:00:00, date_number: 313)
making sample. (inference: False, date:2013-02-28 00:00:00, date_number: 314)
making sample. (inference: False, date:2013-03-31 00:00:00, date_number: 315)
making sample. (inference: False, date:2013-04-30 00:00:00, date_number: 316)
making sample. (inference: False, date:2013-05-31 00:00:00, date_number: 317)
making sample. (inference: False, date:2013-06-30 00:00:00, date_number: 318)
making sample. (inference: False, date:2013-07-31 00:00:00, date_number: 319)
making sample. (inference: False, date:2013-08-31 00:00:00, date_number: 320)
making sample. (inference: False, date:2013-09-30 00:00:00, date_number: 321)
making sample. (inference: False, date:2013-10-31 00:00:00, date_number: 322)
making sample. (inference: False, date:2013-11-30 00:00:00, date_number: 323)
making sample. (inference: False, date:2013-12-31 00:00:00, date_number: 324)
making sample. (inference: False, date:2014-01-31 00:00:00, date_number: 325)
making sample. (inference: False, date:2014-02-28 00:00:00, date_number: 326)
making sample. (inference: False, date:2014-03-31 00:00:00, date_number: 327)
making sample. (inference: False, date:2014-04-30 00:00:00, date_number: 328)
making sample. (inference: False, date:2014-05-31 00:00:00, date_number: 329)
making sample. (inference: False, date:2014-06-30 00:00:00, date_number: 330)
making sample. (inference: False, date:2014-07-31 00:00:00, date_number: 331)
making sample. (inference: False, date:2014-08-31 00:00:00, date_number: 332)
making sample. (inference: False, date:2014-09-30 00:00:00, date_number: 333)
making sample. (inference: False, date:2014-10-31 00:00:00, date_number: 334)
making sample. (inference: False, date:2014-11-30 00:00:00, date_number: 335)
making sample. (inference: False, date:2014-12-31 00:00:00, date_number: 336)
making sample. (inference: False, date:2015-01-31 00:00:00, date_number: 337)
making sample. (inference: False, date:2015-02-28 00:00:00, date_number: 338)
making sample. (inference: False, date:2015-03-31 00:00:00, date_number: 339)
making sample. (inference: False, date:2015-04-30 00:00:00, date_number: 340)
making sample. (inference: False, date:2015-05-31 00:00:00, date_number: 341)
making sample. (inference: False, date:2015-06-30 00:00:00, date_number: 342)
making sample. (inference: False, date:2015-07-31 00:00:00, date_number: 343)
making sample. (inference: False, date:2015-08-31 00:00:00, date_number: 344)
making sample. (inference: False, date:2015-09-30 00:00:00, date_number: 345)
making sample. (inference: False, date:2015-10-31 00:00:00, date_number: 346)
making sample. (inference: False, date:2015-11-30 00:00:00, date_number: 347)
[AttentionModel/2015-12-31 | get_logger | INFO]: ====================AttentionModel/2015-12-31 start====================
making sample. (inference: True, date:2016-01-31 00:00:00, date_number: 349)
making sample. (inference: False, date:2015-12-31 00:00:00, date_number: 348)
[AttentionModel/2016-01-31 | get_logger | INFO]: ====================AttentionModel/2016-01-31 start====================
[strategy_integration.py] set_seed(2015-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5796)training: 2/10 (0.5796)validation : 2/10 (0.5796)training: 3/10 (0.5796)validation : 3/10 (0.5756)training: 4/10 (0.5756)validation : 4/10 (0.5756)training: 5/10 (0.5756)validation : 5/10 (0.5756)training: 6/10 (0.5756)validation : 6/10 (0.5737)training: 7/10 (0.5737)validation : 7/10 (0.5737)training: 8/10 (0.5737)validation : 8/10 (0.5737)training: 9/10 (0.5737)validation : 9/10 (0.5737)early stopping at 9 with loss 0.5737
AttentionModel-training is done: 9/10
2015-12-31 | reset count: 0 | final loss: 0.5737 at epoch 6
[strategy_integration.py] set_seed(2016-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5830)training: 2/10 (0.5830)validation : 2/10 (0.5830)training: 3/10 (0.5830)validation : 3/10 (0.5830)training: 4/10 (0.5830)validation : 4/10 (0.5830)training: 5/10 (0.5830)validation : 5/10 (0.5830)early stopping at 5 with loss 0.5830
AttentionModel-training is done: 5/10
2016-01-31 | reset count: 0 | final loss: 0.5830 at epoch 1
making sample. (inference: True, date:2016-02-29 00:00:00, date_number: 350)
making sample. (inference: False, date:2016-01-31 00:00:00, date_number: 349)
[AttentionModel/2016-02-29 | get_logger | INFO]: ====================AttentionModel/2016-02-29 start====================
making sample. (inference: True, date:2016-03-31 00:00:00, date_number: 351)
making sample. (inference: False, date:2016-02-29 00:00:00, date_number: 350)
[AttentionModel/2016-03-31 | get_logger | INFO]: ====================AttentionModel/2016-03-31 start====================
[strategy_integration.py] set_seed(2016-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5845)training: 2/10 (0.5845)validation : 2/10 (0.5817)training: 3/10 (0.5817)validation : 3/10 (0.5817)training: 4/10 (0.5817)validation : 4/10 (0.5817)training: 5/10 (0.5817)validation : 5/10 (0.5786)training: 6/10 (0.5786)validation : 6/10 (0.5786)training: 7/10 (0.5786)validation : 7/10 (0.5786)training: 8/10 (0.5786)validation : 8/10 (0.5786)early stopping at 8 with loss 0.5786
AttentionModel-training is done: 8/10
2016-02-29 | reset count: 0 | final loss: 0.5786 at epoch 5
[strategy_integration.py] set_seed(2016-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5799)training: 2/10 (0.5799)validation : 2/10 (0.5775)training: 3/10 (0.5775)validation : 3/10 (0.5775)training: 4/10 (0.5775)validation : 4/10 (0.5775)training: 5/10 (0.5775)validation : 5/10 (0.5775)early stopping at 5 with loss 0.5775
AttentionModel-training is done: 5/10
2016-03-31 | reset count: 0 | final loss: 0.5775 at epoch 2
making sample. (inference: True, date:2016-04-30 00:00:00, date_number: 352)
making sample. (inference: False, date:2016-03-31 00:00:00, date_number: 351)
[AttentionModel/2016-04-30 | get_logger | INFO]: ====================AttentionModel/2016-04-30 start====================
making sample. (inference: True, date:2016-05-31 00:00:00, date_number: 353)
making sample. (inference: False, date:2016-04-30 00:00:00, date_number: 352)
[AttentionModel/2016-05-31 | get_logger | INFO]: ====================AttentionModel/2016-05-31 start====================
[strategy_integration.py] set_seed(2016-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5720)training: 2/10 (0.5720)validation : 2/10 (0.5705)training: 3/10 (0.5705)validation : 3/10 (0.5705)training: 4/10 (0.5705)validation : 4/10 (0.5666)training: 5/10 (0.5666)validation : 5/10 (0.5666)training: 6/10 (0.5666)validation : 6/10 (0.5666)training: 7/10 (0.5666)validation : 7/10 (0.5666)early stopping at 7 with loss 0.5666
AttentionModel-training is done: 7/10
2016-04-30 | reset count: 0 | final loss: 0.5666 at epoch 4
making sample. (inference: True, date:2016-06-30 00:00:00, date_number: 354)
making sample. (inference: False, date:2016-05-31 00:00:00, date_number: 353)
[AttentionModel/2016-06-30 | get_logger | INFO]: ====================AttentionModel/2016-06-30 start====================
[strategy_integration.py] set_seed(2016-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5702)training: 2/10 (0.5702)validation : 2/10 (0.5702)training: 3/10 (0.5702)validation : 3/10 (0.5702)training: 4/10 (0.5702)validation : 4/10 (0.5702)training: 5/10 (0.5702)validation : 5/10 (0.5683)training: 6/10 (0.5683)validation : 6/10 (0.5683)training: 7/10 (0.5683)validation : 7/10 (0.5683)training: 8/10 (0.5683)validation : 8/10 (0.5683)early stopping at 8 with loss 0.5683
AttentionModel-training is done: 8/10
2016-05-31 | reset count: 0 | final loss: 0.5683 at epoch 5
making sample. (inference: True, date:2016-07-31 00:00:00, date_number: 355)
making sample. (inference: False, date:2016-06-30 00:00:00, date_number: 354)
[AttentionModel/2016-07-31 | get_logger | INFO]: ====================AttentionModel/2016-07-31 start====================
[strategy_integration.py] set_seed(2016-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5836)training: 2/10 (0.5836)validation : 2/10 (0.5812)training: 3/10 (0.5812)validation : 3/10 (0.5755)training: 4/10 (0.5755)validation : 4/10 (0.5755)training: 5/10 (0.5755)validation : 5/10 (0.5755)training: 6/10 (0.5755)validation : 6/10 (0.5755)early stopping at 6 with loss 0.5755
AttentionModel-training is done: 6/10
2016-06-30 | reset count: 0 | final loss: 0.5755 at epoch 3
making sample. (inference: True, date:2016-08-31 00:00:00, date_number: 356)
making sample. (inference: False, date:2016-07-31 00:00:00, date_number: 355)
[AttentionModel/2016-08-31 | get_logger | INFO]: ====================AttentionModel/2016-08-31 start====================
[strategy_integration.py] set_seed(2016-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5709)training: 2/10 (0.5709)validation : 2/10 (0.5709)training: 3/10 (0.5709)validation : 3/10 (0.5673)training: 4/10 (0.5673)validation : 4/10 (0.5673)training: 5/10 (0.5673)validation : 5/10 (0.5673)training: 6/10 (0.5673)validation : 6/10 (0.5673)early stopping at 6 with loss 0.5673
AttentionModel-training is done: 6/10
2016-07-31 | reset count: 0 | final loss: 0.5673 at epoch 3
making sample. (inference: True, date:2016-09-30 00:00:00, date_number: 357)
making sample. (inference: False, date:2016-08-31 00:00:00, date_number: 356)
[AttentionModel/2016-09-30 | get_logger | INFO]: ====================AttentionModel/2016-09-30 start====================
[strategy_integration.py] set_seed(2016-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5724)training: 2/10 (0.5724)validation : 2/10 (0.5692)training: 3/10 (0.5692)validation : 3/10 (0.5692)training: 4/10 (0.5692)validation : 4/10 (0.5692)training: 5/10 (0.5692)validation : 5/10 (0.5692)training: 6/10 (0.5692)validation : 6/10 (0.5692)early stopping at 6 with loss 0.5692
AttentionModel-training is done: 6/10
2016-08-31 | reset count: 0 | final loss: 0.5692 at epoch 3
making sample. (inference: True, date:2016-10-31 00:00:00, date_number: 358)
making sample. (inference: False, date:2016-09-30 00:00:00, date_number: 357)
[AttentionModel/2016-10-31 | get_logger | INFO]: ====================AttentionModel/2016-10-31 start====================
[strategy_integration.py] set_seed(2016-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5818)training: 2/10 (0.5818)validation : 2/10 (0.5789)training: 3/10 (0.5789)validation : 3/10 (0.5775)training: 4/10 (0.5775)validation : 4/10 (0.5775)training: 5/10 (0.5775)validation : 5/10 (0.5775)training: 6/10 (0.5775)validation : 6/10 (0.5775)early stopping at 6 with loss 0.5775
AttentionModel-training is done: 6/10
2016-09-30 | reset count: 0 | final loss: 0.5775 at epoch 3
[strategy_integration.py] set_seed(2016-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5813)training: 2/10 (0.5813)validation : 2/10 (0.5797)training: 3/10 (0.5797)validation : 3/10 (0.5797)training: 4/10 (0.5797)validation : 4/10 (0.5788)training: 5/10 (0.5788)validation : 5/10 (0.5788)early stopping at 5 with loss 0.5788
AttentionModel-training is done: 5/10
2016-10-31 | reset count: 0 | final loss: 0.5788 at epoch 4
making sample. (inference: True, date:2016-11-30 00:00:00, date_number: 359)
making sample. (inference: False, date:2016-10-31 00:00:00, date_number: 358)
[AttentionModel/2016-11-30 | get_logger | INFO]: ====================AttentionModel/2016-11-30 start====================
making sample. (inference: True, date:2016-12-31 00:00:00, date_number: 360)
making sample. (inference: False, date:2016-11-30 00:00:00, date_number: 359)
[AttentionModel/2016-12-31 | get_logger | INFO]: ====================AttentionModel/2016-12-31 start====================
[strategy_integration.py] set_seed(2016-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5641)training: 2/10 (0.5641)validation : 2/10 (0.5601)training: 3/10 (0.5601)validation : 3/10 (0.5601)training: 4/10 (0.5601)validation : 4/10 (0.5601)training: 5/10 (0.5601)validation : 5/10 (0.5601)early stopping at 5 with loss 0.5601
AttentionModel-training is done: 5/10
2016-11-30 | reset count: 0 | final loss: 0.5601 at epoch 2
making sample. (inference: True, date:2017-01-31 00:00:00, date_number: 361)
making sample. (inference: False, date:2016-12-31 00:00:00, date_number: 360)
[AttentionModel/2017-01-31 | get_logger | INFO]: ====================AttentionModel/2017-01-31 start====================
[strategy_integration.py] set_seed(2016-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5757)training: 2/10 (0.5757)validation : 2/10 (0.5705)training: 3/10 (0.5705)validation : 3/10 (0.5705)training: 4/10 (0.5705)validation : 4/10 (0.5698)training: 5/10 (0.5698)validation : 5/10 (0.5698)training: 6/10 (0.5698)validation : 6/10 (0.5655)training: 7/10 (0.5655)validation : 7/10 (0.5655)training: 8/10 (0.5655)validation : 8/10 (0.5655)training: 9/10 (0.5655)validation : 9/10 (0.5655)early stopping at 9 with loss 0.5655
AttentionModel-training is done: 9/10
2016-12-31 | reset count: 0 | final loss: 0.5655 at epoch 6
[strategy_integration.py] set_seed(2017-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5664)training: 2/10 (0.5664)validation : 2/10 (0.5664)training: 3/10 (0.5664)validation : 3/10 (0.5664)training: 4/10 (0.5664)validation : 4/10 (0.5664)training: 5/10 (0.5664)validation : 5/10 (0.5645)training: 6/10 (0.5645)validation : 6/10 (0.5645)training: 7/10 (0.5645)validation : 7/10 (0.5645)early stopping at 7 with loss 0.5645
AttentionModel-training is done: 7/10
2017-01-31 | reset count: 0 | final loss: 0.5645 at epoch 5
making sample. (inference: True, date:2017-02-28 00:00:00, date_number: 362)
making sample. (inference: False, date:2017-01-31 00:00:00, date_number: 361)
[AttentionModel/2017-02-28 | get_logger | INFO]: ====================AttentionModel/2017-02-28 start====================
making sample. (inference: True, date:2017-03-31 00:00:00, date_number: 363)
making sample. (inference: False, date:2017-02-28 00:00:00, date_number: 362)
[AttentionModel/2017-03-31 | get_logger | INFO]: ====================AttentionModel/2017-03-31 start====================
[strategy_integration.py] set_seed(2017-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5804)training: 2/10 (0.5804)validation : 2/10 (0.5797)training: 3/10 (0.5797)validation : 3/10 (0.5718)training: 4/10 (0.5718)validation : 4/10 (0.5718)training: 5/10 (0.5718)validation : 5/10 (0.5718)training: 6/10 (0.5718)validation : 6/10 (0.5718)early stopping at 6 with loss 0.5718
AttentionModel-training is done: 6/10
2017-02-28 | reset count: 0 | final loss: 0.5718 at epoch 3
making sample. (inference: True, date:2017-04-30 00:00:00, date_number: 364)
making sample. (inference: False, date:2017-03-31 00:00:00, date_number: 363)
[AttentionModel/2017-04-30 | get_logger | INFO]: ====================AttentionModel/2017-04-30 start====================
[strategy_integration.py] set_seed(2017-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5732)training: 2/10 (0.5732)validation : 2/10 (0.5683)training: 3/10 (0.5683)validation : 3/10 (0.5683)training: 4/10 (0.5683)validation : 4/10 (0.5683)training: 5/10 (0.5683)validation : 5/10 (0.5683)training: 6/10 (0.5683)validation : 6/10 (0.5683)training: 7/10 (0.5683)validation : 7/10 (0.5683)early stopping at 7 with loss 0.5683
AttentionModel-training is done: 7/10
2017-03-31 | reset count: 0 | final loss: 0.5683 at epoch 2
making sample. (inference: True, date:2017-05-31 00:00:00, date_number: 365)
making sample. (inference: False, date:2017-04-30 00:00:00, date_number: 364)
[AttentionModel/2017-05-31 | get_logger | INFO]: ====================AttentionModel/2017-05-31 start====================
[strategy_integration.py] set_seed(2017-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5721)training: 2/10 (0.5721)validation : 2/10 (0.5677)training: 3/10 (0.5677)validation : 3/10 (0.5677)training: 4/10 (0.5677)validation : 4/10 (0.5645)training: 5/10 (0.5645)validation : 5/10 (0.5645)training: 6/10 (0.5645)validation : 6/10 (0.5645)early stopping at 6 with loss 0.5645
AttentionModel-training is done: 6/10
2017-04-30 | reset count: 0 | final loss: 0.5645 at epoch 4
making sample. (inference: True, date:2017-06-30 00:00:00, date_number: 366)
making sample. (inference: False, date:2017-05-31 00:00:00, date_number: 365)
[AttentionModel/2017-06-30 | get_logger | INFO]: ====================AttentionModel/2017-06-30 start====================
[strategy_integration.py] set_seed(2017-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5719)training: 2/10 (0.5719)validation : 2/10 (0.5714)training: 3/10 (0.5714)validation : 3/10 (0.5671)training: 4/10 (0.5671)validation : 4/10 (0.5671)training: 5/10 (0.5671)validation : 5/10 (0.5670)training: 6/10 (0.5670)validation : 6/10 (0.5670)early stopping at 6 with loss 0.5670
AttentionModel-training is done: 6/10
2017-05-31 | reset count: 0 | final loss: 0.5670 at epoch 5
making sample. (inference: True, date:2017-07-31 00:00:00, date_number: 367)
making sample. (inference: False, date:2017-06-30 00:00:00, date_number: 366)
[AttentionModel/2017-07-31 | get_logger | INFO]: ====================AttentionModel/2017-07-31 start====================
[strategy_integration.py] set_seed(2017-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5730)training: 2/10 (0.5730)validation : 2/10 (0.5690)training: 3/10 (0.5690)validation : 3/10 (0.5690)training: 4/10 (0.5690)validation : 4/10 (0.5676)training: 5/10 (0.5676)validation : 5/10 (0.5669)training: 6/10 (0.5669)validation : 6/10 (0.5646)training: 7/10 (0.5646)validation : 7/10 (0.5646)training: 8/10 (0.5646)validation : 8/10 (0.5646)early stopping at 8 with loss 0.5646
AttentionModel-training is done: 8/10
2017-06-30 | reset count: 0 | final loss: 0.5646 at epoch 6
[strategy_integration.py] set_seed(2017-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5666)training: 2/10 (0.5666)validation : 2/10 (0.5666)training: 3/10 (0.5666)validation : 3/10 (0.5666)training: 4/10 (0.5666)validation : 4/10 (0.5666)training: 5/10 (0.5666)validation : 5/10 (0.5655)training: 6/10 (0.5655)validation : 6/10 (0.5655)early stopping at 6 with loss 0.5655
AttentionModel-training is done: 6/10
2017-07-31 | reset count: 0 | final loss: 0.5655 at epoch 5
making sample. (inference: True, date:2017-08-31 00:00:00, date_number: 368)
making sample. (inference: False, date:2017-07-31 00:00:00, date_number: 367)
[AttentionModel/2017-08-31 | get_logger | INFO]: ====================AttentionModel/2017-08-31 start====================
making sample. (inference: True, date:2017-09-30 00:00:00, date_number: 369)
making sample. (inference: False, date:2017-08-31 00:00:00, date_number: 368)
[AttentionModel/2017-09-30 | get_logger | INFO]: ====================AttentionModel/2017-09-30 start====================
[strategy_integration.py] set_seed(2017-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5686)training: 2/10 (0.5686)validation : 2/10 (0.5662)training: 3/10 (0.5662)validation : 3/10 (0.5627)training: 4/10 (0.5627)validation : 4/10 (0.5627)training: 5/10 (0.5627)validation : 5/10 (0.5627)training: 6/10 (0.5627)validation : 6/10 (0.5627)early stopping at 6 with loss 0.5627
AttentionModel-training is done: 6/10
2017-08-31 | reset count: 0 | final loss: 0.5627 at epoch 3
[strategy_integration.py] set_seed(2017-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5614)training: 2/10 (0.5614)validation : 2/10 (0.5537)training: 3/10 (0.5537)validation : 3/10 (0.5523)training: 4/10 (0.5523)validation : 4/10 (0.5523)training: 5/10 (0.5523)validation : 5/10 (0.5523)training: 6/10 (0.5523)validation : 6/10 (0.5523)early stopping at 6 with loss 0.5523
AttentionModel-training is done: 6/10
2017-09-30 | reset count: 0 | final loss: 0.5523 at epoch 3
making sample. (inference: True, date:2017-10-31 00:00:00, date_number: 370)
making sample. (inference: False, date:2017-09-30 00:00:00, date_number: 369)
[AttentionModel/2017-10-31 | get_logger | INFO]: ====================AttentionModel/2017-10-31 start====================
making sample. (inference: True, date:2017-11-30 00:00:00, date_number: 371)
making sample. (inference: False, date:2017-10-31 00:00:00, date_number: 370)
[AttentionModel/2017-11-30 | get_logger | INFO]: ====================AttentionModel/2017-11-30 start====================
[strategy_integration.py] set_seed(2017-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5616)training: 2/10 (0.5616)validation : 2/10 (0.5616)training: 3/10 (0.5616)validation : 3/10 (0.5609)training: 4/10 (0.5609)validation : 4/10 (0.5595)training: 5/10 (0.5595)validation : 5/10 (0.5595)training: 6/10 (0.5595)validation : 6/10 (0.5595)early stopping at 6 with loss 0.5595
AttentionModel-training is done: 6/10
2017-10-31 | reset count: 0 | final loss: 0.5595 at epoch 4
making sample. (inference: True, date:2017-12-31 00:00:00, date_number: 372)
making sample. (inference: False, date:2017-11-30 00:00:00, date_number: 371)
[strategy_integration.py] set_seed(2017-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5738)training: 2/10 (0.5738)validation : 2/10 (0.5738)training: 3/10 (0.5738)validation : 3/10 (0.5679)training: 4/10 (0.5679)validation : 4/10 (0.5679)training: 5/10 (0.5679)validation : 5/10 (0.5679)training: 6/10 (0.5679)validation : 6/10 (0.5679)early stopping at 6 with loss 0.5679
AttentionModel-training is done: 6/10
2017-11-30 | reset count: 0 | final loss: 0.5679 at epoch 3
[AttentionModel/2017-12-31 | get_logger | INFO]: ====================AttentionModel/2017-12-31 start====================
making sample. (inference: True, date:2018-01-31 00:00:00, date_number: 373)
making sample. (inference: False, date:2017-12-31 00:00:00, date_number: 372)
[AttentionModel/2018-01-31 | get_logger | INFO]: ====================AttentionModel/2018-01-31 start====================
[strategy_integration.py] set_seed(2017-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5684)training: 2/10 (0.5684)validation : 2/10 (0.5675)training: 3/10 (0.5675)validation : 3/10 (0.5675)training: 4/10 (0.5675)validation : 4/10 (0.5654)training: 5/10 (0.5654)validation : 5/10 (0.5654)training: 6/10 (0.5654)validation : 6/10 (0.5643)training: 7/10 (0.5643)validation : 7/10 (0.5643)early stopping at 7 with loss 0.5643
AttentionModel-training is done: 7/10
2017-12-31 | reset count: 0 | final loss: 0.5643 at epoch 6
[strategy_integration.py] set_seed(2018-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5718)training: 2/10 (0.5718)validation : 2/10 (0.5718)training: 3/10 (0.5718)validation : 3/10 (0.5718)training: 4/10 (0.5718)validation : 4/10 (0.5718)training: 5/10 (0.5718)validation : 5/10 (0.5718)early stopping at 5 with loss 0.5718
AttentionModel-training is done: 5/10
2018-01-31 | reset count: 0 | final loss: 0.5718 at epoch 1
making sample. (inference: True, date:2018-02-28 00:00:00, date_number: 374)
making sample. (inference: False, date:2018-01-31 00:00:00, date_number: 373)
[AttentionModel/2018-02-28 | get_logger | INFO]: ====================AttentionModel/2018-02-28 start====================
making sample. (inference: True, date:2018-03-31 00:00:00, date_number: 375)
making sample. (inference: False, date:2018-02-28 00:00:00, date_number: 374)
[AttentionModel/2018-03-31 | get_logger | INFO]: ====================AttentionModel/2018-03-31 start====================
[strategy_integration.py] set_seed(2018-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5686)training: 2/10 (0.5686)validation : 2/10 (0.5644)training: 3/10 (0.5644)validation : 3/10 (0.5639)training: 4/10 (0.5639)validation : 4/10 (0.5639)training: 5/10 (0.5639)validation : 5/10 (0.5639)early stopping at 5 with loss 0.5639
AttentionModel-training is done: 5/10
2018-02-28 | reset count: 0 | final loss: 0.5639 at epoch 3
[strategy_integration.py] set_seed(2018-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5684)training: 2/10 (0.5684)validation : 2/10 (0.5684)training: 3/10 (0.5684)validation : 3/10 (0.5684)training: 4/10 (0.5684)validation : 4/10 (0.5651)training: 5/10 (0.5651)validation : 5/10 (0.5651)early stopping at 5 with loss 0.5651
AttentionModel-training is done: 5/10
2018-03-31 | reset count: 0 | final loss: 0.5651 at epoch 4
making sample. (inference: True, date:2018-04-30 00:00:00, date_number: 376)
making sample. (inference: False, date:2018-03-31 00:00:00, date_number: 375)
[AttentionModel/2018-04-30 | get_logger | INFO]: ====================AttentionModel/2018-04-30 start====================
making sample. (inference: True, date:2018-05-31 00:00:00, date_number: 377)
making sample. (inference: False, date:2018-04-30 00:00:00, date_number: 376)
[AttentionModel/2018-05-31 | get_logger | INFO]: ====================AttentionModel/2018-05-31 start====================
[strategy_integration.py] set_seed(2018-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5725)training: 2/10 (0.5725)validation : 2/10 (0.5660)training: 3/10 (0.5660)validation : 3/10 (0.5660)training: 4/10 (0.5660)validation : 4/10 (0.5660)training: 5/10 (0.5660)validation : 5/10 (0.5660)training: 6/10 (0.5660)validation : 6/10 (0.5660)early stopping at 6 with loss 0.5660
AttentionModel-training is done: 6/10
2018-04-30 | reset count: 0 | final loss: 0.5660 at epoch 2
[strategy_integration.py] set_seed(2018-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5659)training: 2/10 (0.5659)validation : 2/10 (0.5659)training: 3/10 (0.5659)validation : 3/10 (0.5641)training: 4/10 (0.5641)validation : 4/10 (0.5641)training: 5/10 (0.5641)validation : 5/10 (0.5641)early stopping at 5 with loss 0.5641
AttentionModel-training is done: 5/10
2018-05-31 | reset count: 0 | final loss: 0.5641 at epoch 3
making sample. (inference: True, date:2018-06-30 00:00:00, date_number: 378)
making sample. (inference: False, date:2018-05-31 00:00:00, date_number: 377)
[AttentionModel/2018-06-30 | get_logger | INFO]: ====================AttentionModel/2018-06-30 start====================
making sample. (inference: True, date:2018-07-31 00:00:00, date_number: 379)
making sample. (inference: False, date:2018-06-30 00:00:00, date_number: 378)
[AttentionModel/2018-07-31 | get_logger | INFO]: ====================AttentionModel/2018-07-31 start====================
[strategy_integration.py] set_seed(2018-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5697)training: 2/10 (0.5697)validation : 2/10 (0.5602)training: 3/10 (0.5602)validation : 3/10 (0.5602)training: 4/10 (0.5602)validation : 4/10 (0.5602)training: 5/10 (0.5602)validation : 5/10 (0.5602)training: 6/10 (0.5602)validation : 6/10 (0.5602)early stopping at 6 with loss 0.5602
AttentionModel-training is done: 6/10
2018-06-30 | reset count: 0 | final loss: 0.5602 at epoch 2
[strategy_integration.py] set_seed(2018-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5765)training: 2/10 (0.5765)validation : 2/10 (0.5764)training: 3/10 (0.5764)validation : 3/10 (0.5737)training: 4/10 (0.5737)validation : 4/10 (0.5737)training: 5/10 (0.5737)validation : 5/10 (0.5737)early stopping at 5 with loss 0.5737
AttentionModel-training is done: 5/10
2018-07-31 | reset count: 0 | final loss: 0.5737 at epoch 3
making sample. (inference: True, date:2018-08-31 00:00:00, date_number: 380)
making sample. (inference: False, date:2018-07-31 00:00:00, date_number: 379)
[AttentionModel/2018-08-31 | get_logger | INFO]: ====================AttentionModel/2018-08-31 start====================
making sample. (inference: True, date:2018-09-30 00:00:00, date_number: 381)
making sample. (inference: False, date:2018-08-31 00:00:00, date_number: 380)
[AttentionModel/2018-09-30 | get_logger | INFO]: ====================AttentionModel/2018-09-30 start====================
[strategy_integration.py] set_seed(2018-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5714)training: 2/10 (0.5714)validation : 2/10 (0.5709)training: 3/10 (0.5709)validation : 3/10 (0.5693)training: 4/10 (0.5693)validation : 4/10 (0.5693)training: 5/10 (0.5693)validation : 5/10 (0.5693)training: 6/10 (0.5693)validation : 6/10 (0.5668)training: 7/10 (0.5668)validation : 7/10 (0.5668)early stopping at 7 with loss 0.5668
AttentionModel-training is done: 7/10
2018-08-31 | reset count: 0 | final loss: 0.5668 at epoch 6
[strategy_integration.py] set_seed(2018-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5745)training: 2/10 (0.5745)validation : 2/10 (0.5730)training: 3/10 (0.5730)validation : 3/10 (0.5728)training: 4/10 (0.5728)validation : 4/10 (0.5695)training: 5/10 (0.5695)validation : 5/10 (0.5695)training: 6/10 (0.5695)validation : 6/10 (0.5695)training: 7/10 (0.5695)validation : 7/10 (0.5695)early stopping at 7 with loss 0.5695
AttentionModel-training is done: 7/10
2018-09-30 | reset count: 0 | final loss: 0.5695 at epoch 4
making sample. (inference: True, date:2018-10-31 00:00:00, date_number: 382)
making sample. (inference: False, date:2018-09-30 00:00:00, date_number: 381)
[AttentionModel/2018-10-31 | get_logger | INFO]: ====================AttentionModel/2018-10-31 start====================
making sample. (inference: True, date:2018-11-30 00:00:00, date_number: 383)
making sample. (inference: False, date:2018-10-31 00:00:00, date_number: 382)
[AttentionModel/2018-11-30 | get_logger | INFO]: ====================AttentionModel/2018-11-30 start====================
[strategy_integration.py] set_seed(2018-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5900)training: 2/10 (0.5900)validation : 2/10 (0.5880)training: 3/10 (0.5880)validation : 3/10 (0.5863)training: 4/10 (0.5863)validation : 4/10 (0.5838)training: 5/10 (0.5838)validation : 5/10 (0.5838)training: 6/10 (0.5838)validation : 6/10 (0.5838)training: 7/10 (0.5838)validation : 7/10 (0.5838)early stopping at 7 with loss 0.5838
AttentionModel-training is done: 7/10
2018-10-31 | reset count: 0 | final loss: 0.5838 at epoch 4
[strategy_integration.py] set_seed(2018-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5878)training: 2/10 (0.5878)validation : 2/10 (0.5776)training: 3/10 (0.5776)validation : 3/10 (0.5776)training: 4/10 (0.5776)validation : 4/10 (0.5776)training: 5/10 (0.5776)validation : 5/10 (0.5776)training: 6/10 (0.5776)validation : 6/10 (0.5776)early stopping at 6 with loss 0.5776
AttentionModel-training is done: 6/10
2018-11-30 | reset count: 0 | final loss: 0.5776 at epoch 2
making sample. (inference: True, date:2018-12-31 00:00:00, date_number: 384)
making sample. (inference: False, date:2018-11-30 00:00:00, date_number: 383)
[AttentionModel/2018-12-31 | get_logger | INFO]: ====================AttentionModel/2018-12-31 start====================
making sample. (inference: True, date:2019-01-31 00:00:00, date_number: 385)
making sample. (inference: False, date:2018-12-31 00:00:00, date_number: 384)
[AttentionModel/2019-01-31 | get_logger | INFO]: ====================AttentionModel/2019-01-31 start====================
[strategy_integration.py] set_seed(2018-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5855)training: 2/10 (0.5855)validation : 2/10 (0.5790)training: 3/10 (0.5790)validation : 3/10 (0.5790)training: 4/10 (0.5790)validation : 4/10 (0.5786)training: 5/10 (0.5786)validation : 5/10 (0.5786)training: 6/10 (0.5786)validation : 6/10 (0.5786)training: 7/10 (0.5786)validation : 7/10 (0.5786)early stopping at 7 with loss 0.5786
AttentionModel-training is done: 7/10
2018-12-31 | reset count: 0 | final loss: 0.5786 at epoch 4
[strategy_integration.py] set_seed(2019-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5911)training: 2/10 (0.5911)validation : 2/10 (0.5894)training: 3/10 (0.5894)validation : 3/10 (0.5894)training: 4/10 (0.5894)validation : 4/10 (0.5874)training: 5/10 (0.5874)validation : 5/10 (0.5874)training: 6/10 (0.5874)validation : 6/10 (0.5874)early stopping at 6 with loss 0.5874
AttentionModel-training is done: 6/10
2019-01-31 | reset count: 0 | final loss: 0.5874 at epoch 4
making sample. (inference: True, date:2019-02-28 00:00:00, date_number: 386)
making sample. (inference: False, date:2019-01-31 00:00:00, date_number: 385)
[AttentionModel/2019-02-28 | get_logger | INFO]: ====================AttentionModel/2019-02-28 start====================
making sample. (inference: True, date:2019-03-31 00:00:00, date_number: 387)
making sample. (inference: False, date:2019-02-28 00:00:00, date_number: 386)
[AttentionModel/2019-03-31 | get_logger | INFO]: ====================AttentionModel/2019-03-31 start====================
[strategy_integration.py] set_seed(2019-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5844)training: 2/10 (0.5844)validation : 2/10 (0.5844)training: 3/10 (0.5844)validation : 3/10 (0.5828)training: 4/10 (0.5828)validation : 4/10 (0.5828)training: 5/10 (0.5828)validation : 5/10 (0.5826)training: 6/10 (0.5826)validation : 6/10 (0.5826)early stopping at 6 with loss 0.5826
AttentionModel-training is done: 6/10
2019-02-28 | reset count: 0 | final loss: 0.5826 at epoch 5
making sample. (inference: True, date:2019-04-30 00:00:00, date_number: 388)
making sample. (inference: False, date:2019-03-31 00:00:00, date_number: 387)
[AttentionModel/2019-04-30 | get_logger | INFO]: ====================AttentionModel/2019-04-30 start====================
[strategy_integration.py] set_seed(2019-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5812)training: 2/10 (0.5812)validation : 2/10 (0.5763)training: 3/10 (0.5763)validation : 3/10 (0.5746)training: 4/10 (0.5746)validation : 4/10 (0.5746)training: 5/10 (0.5746)validation : 5/10 (0.5723)training: 6/10 (0.5723)validation : 6/10 (0.5723)training: 7/10 (0.5723)validation : 7/10 (0.5723)training: 8/10 (0.5723)validation : 8/10 (0.5723)early stopping at 8 with loss 0.5723
AttentionModel-training is done: 8/10
2019-03-31 | reset count: 0 | final loss: 0.5723 at epoch 5
[strategy_integration.py] set_seed(2019-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5924)training: 2/10 (0.5924)validation : 2/10 (0.5903)training: 3/10 (0.5903)validation : 3/10 (0.5903)training: 4/10 (0.5903)validation : 4/10 (0.5903)training: 5/10 (0.5903)validation : 5/10 (0.5903)early stopping at 5 with loss 0.5903
AttentionModel-training is done: 5/10
2019-04-30 | reset count: 0 | final loss: 0.5903 at epoch 2
making sample. (inference: True, date:2019-05-31 00:00:00, date_number: 389)
making sample. (inference: False, date:2019-04-30 00:00:00, date_number: 388)
[AttentionModel/2019-05-31 | get_logger | INFO]: ====================AttentionModel/2019-05-31 start====================
making sample. (inference: True, date:2019-06-30 00:00:00, date_number: 390)
making sample. (inference: False, date:2019-05-31 00:00:00, date_number: 389)
[AttentionModel/2019-06-30 | get_logger | INFO]: ====================AttentionModel/2019-06-30 start====================
[strategy_integration.py] set_seed(2019-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5877)training: 2/10 (0.5877)validation : 2/10 (0.5877)training: 3/10 (0.5877)validation : 3/10 (0.5872)training: 4/10 (0.5872)validation : 4/10 (0.5851)training: 5/10 (0.5851)validation : 5/10 (0.5846)training: 6/10 (0.5846)validation : 6/10 (0.5835)training: 7/10 (0.5835)validation : 7/10 (0.5835)early stopping at 7 with loss 0.5835
AttentionModel-training is done: 7/10
2019-05-31 | reset count: 0 | final loss: 0.5835 at epoch 6
[strategy_integration.py] set_seed(2019-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5851)training: 2/10 (0.5851)validation : 2/10 (0.5851)training: 3/10 (0.5851)validation : 3/10 (0.5825)training: 4/10 (0.5825)validation : 4/10 (0.5825)training: 5/10 (0.5825)validation : 5/10 (0.5825)training: 6/10 (0.5825)validation : 6/10 (0.5825)early stopping at 6 with loss 0.5825
AttentionModel-training is done: 6/10
2019-06-30 | reset count: 0 | final loss: 0.5825 at epoch 3
making sample. (inference: True, date:2019-07-31 00:00:00, date_number: 391)
making sample. (inference: False, date:2019-06-30 00:00:00, date_number: 390)
[AttentionModel/2019-07-31 | get_logger | INFO]: ====================AttentionModel/2019-07-31 start====================
making sample. (inference: True, date:2019-08-31 00:00:00, date_number: 392)
making sample. (inference: False, date:2019-07-31 00:00:00, date_number: 391)
[AttentionModel/2019-08-31 | get_logger | INFO]: ====================AttentionModel/2019-08-31 start====================
[strategy_integration.py] set_seed(2019-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5872)training: 2/10 (0.5872)validation : 2/10 (0.5863)training: 3/10 (0.5863)validation : 3/10 (0.5831)training: 4/10 (0.5831)validation : 4/10 (0.5831)training: 5/10 (0.5831)validation : 5/10 (0.5816)training: 6/10 (0.5816)validation : 6/10 (0.5816)training: 7/10 (0.5816)validation : 7/10 (0.5816)training: 8/10 (0.5816)validation : 8/10 (0.5816)early stopping at 8 with loss 0.5816
AttentionModel-training is done: 8/10
2019-07-31 | reset count: 0 | final loss: 0.5816 at epoch 5
[strategy_integration.py] set_seed(2019-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5961)training: 2/10 (0.5961)validation : 2/10 (0.5915)training: 3/10 (0.5915)validation : 3/10 (0.5898)training: 4/10 (0.5898)validation : 4/10 (0.5898)training: 5/10 (0.5898)validation : 5/10 (0.5898)training: 6/10 (0.5898)validation : 6/10 (0.5898)early stopping at 6 with loss 0.5898
AttentionModel-training is done: 6/10
2019-08-31 | reset count: 0 | final loss: 0.5898 at epoch 3
making sample. (inference: True, date:2019-09-30 00:00:00, date_number: 393)
making sample. (inference: False, date:2019-08-31 00:00:00, date_number: 392)
[AttentionModel/2019-09-30 | get_logger | INFO]: ====================AttentionModel/2019-09-30 start====================
making sample. (inference: True, date:2019-10-31 00:00:00, date_number: 394)
making sample. (inference: False, date:2019-09-30 00:00:00, date_number: 393)
[AttentionModel/2019-10-31 | get_logger | INFO]: ====================AttentionModel/2019-10-31 start====================
[strategy_integration.py] set_seed(2019-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5860)training: 2/10 (0.5860)validation : 2/10 (0.5815)training: 3/10 (0.5815)validation : 3/10 (0.5804)training: 4/10 (0.5804)validation : 4/10 (0.5804)training: 5/10 (0.5804)validation : 5/10 (0.5795)training: 6/10 (0.5795)validation : 6/10 (0.5795)training: 7/10 (0.5795)validation : 7/10 (0.5795)early stopping at 7 with loss 0.5795
AttentionModel-training is done: 7/10
2019-09-30 | reset count: 0 | final loss: 0.5795 at epoch 6
[strategy_integration.py] set_seed(2019-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5882)training: 2/10 (0.5882)validation : 2/10 (0.5874)training: 3/10 (0.5874)validation : 3/10 (0.5854)training: 4/10 (0.5854)validation : 4/10 (0.5848)training: 5/10 (0.5848)validation : 5/10 (0.5848)training: 6/10 (0.5848)validation : 6/10 (0.5848)training: 7/10 (0.5848)validation : 7/10 (0.5848)early stopping at 7 with loss 0.5848
AttentionModel-training is done: 7/10
2019-10-31 | reset count: 0 | final loss: 0.5848 at epoch 4
making sample. (inference: True, date:2019-11-30 00:00:00, date_number: 395)
making sample. (inference: False, date:2019-10-31 00:00:00, date_number: 394)
[AttentionModel/2019-11-30 | get_logger | INFO]: ====================AttentionModel/2019-11-30 start====================
making sample. (inference: True, date:2019-12-31 00:00:00, date_number: 396)
making sample. (inference: False, date:2019-11-30 00:00:00, date_number: 395)
[AttentionModel/2019-12-31 | get_logger | INFO]: ====================AttentionModel/2019-12-31 start====================
[strategy_integration.py] set_seed(2019-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5909)training: 2/10 (0.5909)validation : 2/10 (0.5867)training: 3/10 (0.5867)validation : 3/10 (0.5858)training: 4/10 (0.5858)validation : 4/10 (0.5852)training: 5/10 (0.5852)validation : 5/10 (0.5852)training: 6/10 (0.5852)validation : 6/10 (0.5852)early stopping at 6 with loss 0.5852
AttentionModel-training is done: 6/10
2019-11-30 | reset count: 0 | final loss: 0.5852 at epoch 4
[strategy_integration.py] set_seed(2019-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5796)training: 2/10 (0.5796)validation : 2/10 (0.5796)training: 3/10 (0.5796)validation : 3/10 (0.5796)training: 4/10 (0.5796)validation : 4/10 (0.5796)training: 5/10 (0.5796)validation : 5/10 (0.5796)early stopping at 5 with loss 0.5796
AttentionModel-training is done: 5/10
2019-12-31 | reset count: 0 | final loss: 0.5796 at epoch 1
making sample. (inference: True, date:2020-01-31 00:00:00, date_number: 397)
making sample. (inference: False, date:2019-12-31 00:00:00, date_number: 396)
[AttentionModel/2020-01-31 | get_logger | INFO]: ====================AttentionModel/2020-01-31 start====================
making sample. (inference: True, date:2020-02-29 00:00:00, date_number: 398)
making sample. (inference: False, date:2020-01-31 00:00:00, date_number: 397)
[AttentionModel/2020-02-29 | get_logger | INFO]: ====================AttentionModel/2020-02-29 start====================
[strategy_integration.py] set_seed(2020-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5990)training: 2/10 (0.5990)validation : 2/10 (0.5969)training: 3/10 (0.5969)validation : 3/10 (0.5948)training: 4/10 (0.5948)validation : 4/10 (0.5939)training: 5/10 (0.5939)validation : 5/10 (0.5926)training: 6/10 (0.5926)validation : 6/10 (0.5926)early stopping at 6 with loss 0.5926
AttentionModel-training is done: 6/10
2020-01-31 | reset count: 0 | final loss: 0.5926 at epoch 5
[strategy_integration.py] set_seed(2020-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5813)training: 2/10 (0.5813)validation : 2/10 (0.5792)training: 3/10 (0.5792)validation : 3/10 (0.5792)training: 4/10 (0.5792)validation : 4/10 (0.5792)training: 5/10 (0.5792)validation : 5/10 (0.5792)early stopping at 5 with loss 0.5792
AttentionModel-training is done: 5/10
2020-02-29 | reset count: 0 | final loss: 0.5792 at epoch 2
making sample. (inference: True, date:2020-03-31 00:00:00, date_number: 399)
making sample. (inference: False, date:2020-02-29 00:00:00, date_number: 398)
[AttentionModel/2020-03-31 | get_logger | INFO]: ====================AttentionModel/2020-03-31 start====================
making sample. (inference: True, date:2020-04-30 00:00:00, date_number: 400)
making sample. (inference: False, date:2020-03-31 00:00:00, date_number: 399)
[AttentionModel/2020-04-30 | get_logger | INFO]: ====================AttentionModel/2020-04-30 start====================
[strategy_integration.py] set_seed(2020-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5792)training: 2/10 (0.5792)validation : 2/10 (0.5781)training: 3/10 (0.5781)validation : 3/10 (0.5781)training: 4/10 (0.5781)validation : 4/10 (0.5771)training: 5/10 (0.5771)validation : 5/10 (0.5771)training: 6/10 (0.5771)validation : 6/10 (0.5764)training: 7/10 (0.5764)validation : 7/10 (0.5764)early stopping at 7 with loss 0.5764
AttentionModel-training is done: 7/10
2020-03-31 | reset count: 0 | final loss: 0.5764 at epoch 6
[strategy_integration.py] set_seed(2020-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5720)training: 2/10 (0.5720)validation : 2/10 (0.5701)training: 3/10 (0.5701)validation : 3/10 (0.5701)training: 4/10 (0.5701)validation : 4/10 (0.5689)training: 5/10 (0.5689)validation : 5/10 (0.5689)training: 6/10 (0.5689)validation : 6/10 (0.5689)training: 7/10 (0.5689)validation : 7/10 (0.5689)early stopping at 7 with loss 0.5689
AttentionModel-training is done: 7/10
2020-04-30 | reset count: 0 | final loss: 0.5689 at epoch 4
making sample. (inference: True, date:2020-05-31 00:00:00, date_number: 401)
making sample. (inference: False, date:2020-04-30 00:00:00, date_number: 400)
[AttentionModel/2020-05-31 | get_logger | INFO]: ====================AttentionModel/2020-05-31 start====================
making sample. (inference: True, date:2020-06-30 00:00:00, date_number: 402)
making sample. (inference: False, date:2020-05-31 00:00:00, date_number: 401)
[AttentionModel/2020-06-30 | get_logger | INFO]: ====================AttentionModel/2020-06-30 start====================
[strategy_integration.py] set_seed(2020-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5816)training: 2/10 (0.5816)validation : 2/10 (0.5816)training: 3/10 (0.5816)validation : 3/10 (0.5753)training: 4/10 (0.5753)validation : 4/10 (0.5753)training: 5/10 (0.5753)validation : 5/10 (0.5753)early stopping at 5 with loss 0.5753
AttentionModel-training is done: 5/10
2020-05-31 | reset count: 0 | final loss: 0.5753 at epoch 3
making sample. (inference: True, date:2020-07-31 00:00:00, date_number: 403)
making sample. (inference: False, date:2020-06-30 00:00:00, date_number: 402)
[AttentionModel/2020-07-31 | get_logger | INFO]: ====================AttentionModel/2020-07-31 start====================
[strategy_integration.py] set_seed(2020-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5916)training: 2/10 (0.5916)validation : 2/10 (0.5870)training: 3/10 (0.5870)validation : 3/10 (0.5842)training: 4/10 (0.5842)validation : 4/10 (0.5833)training: 5/10 (0.5833)validation : 5/10 (0.5833)training: 6/10 (0.5833)validation : 6/10 (0.5833)early stopping at 6 with loss 0.5833
AttentionModel-training is done: 6/10
2020-06-30 | reset count: 0 | final loss: 0.5833 at epoch 4
[strategy_integration.py] set_seed(2020-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5789)training: 2/10 (0.5789)validation : 2/10 (0.5789)training: 3/10 (0.5789)validation : 3/10 (0.5789)training: 4/10 (0.5789)validation : 4/10 (0.5745)training: 5/10 (0.5745)validation : 5/10 (0.5745)training: 6/10 (0.5745)validation : 6/10 (0.5745)early stopping at 6 with loss 0.5745
AttentionModel-training is done: 6/10
2020-07-31 | reset count: 0 | final loss: 0.5745 at epoch 4
making sample. (inference: True, date:2020-08-31 00:00:00, date_number: 404)
making sample. (inference: False, date:2020-07-31 00:00:00, date_number: 403)
[AttentionModel/2020-08-31 | get_logger | INFO]: ====================AttentionModel/2020-08-31 start====================
making sample. (inference: True, date:2020-09-30 00:00:00, date_number: 405)
making sample. (inference: False, date:2020-08-31 00:00:00, date_number: 404)
[AttentionModel/2020-09-30 | get_logger | INFO]: ====================AttentionModel/2020-09-30 start====================
[strategy_integration.py] set_seed(2020-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5794)training: 2/10 (0.5794)validation : 2/10 (0.5743)training: 3/10 (0.5743)validation : 3/10 (0.5743)training: 4/10 (0.5743)validation : 4/10 (0.5739)training: 5/10 (0.5739)validation : 5/10 (0.5739)training: 6/10 (0.5739)validation : 6/10 (0.5739)training: 7/10 (0.5739)validation : 7/10 (0.5695)training: 8/10 (0.5695)validation : 8/10 (0.5695)early stopping at 8 with loss 0.5695
AttentionModel-training is done: 8/10
2020-08-31 | reset count: 0 | final loss: 0.5695 at epoch 7
[strategy_integration.py] set_seed(2020-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5682)training: 2/10 (0.5682)validation : 2/10 (0.5670)training: 3/10 (0.5670)validation : 3/10 (0.5670)training: 4/10 (0.5670)validation : 4/10 (0.5670)training: 5/10 (0.5670)validation : 5/10 (0.5670)early stopping at 5 with loss 0.5670
AttentionModel-training is done: 5/10
2020-09-30 | reset count: 0 | final loss: 0.5670 at epoch 2
making sample. (inference: True, date:2020-10-31 00:00:00, date_number: 406)
making sample. (inference: False, date:2020-09-30 00:00:00, date_number: 405)
[AttentionModel/2020-10-31 | get_logger | INFO]: ====================AttentionModel/2020-10-31 start====================
making sample. (inference: True, date:2020-11-30 00:00:00, date_number: 407)
making sample. (inference: False, date:2020-10-31 00:00:00, date_number: 406)
[AttentionModel/2020-11-30 | get_logger | INFO]: ====================AttentionModel/2020-11-30 start====================
[strategy_integration.py] set_seed(2020-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5819)training: 2/10 (0.5819)validation : 2/10 (0.5793)training: 3/10 (0.5793)validation : 3/10 (0.5793)training: 4/10 (0.5793)validation : 4/10 (0.5769)training: 5/10 (0.5769)validation : 5/10 (0.5769)training: 6/10 (0.5769)validation : 6/10 (0.5762)training: 7/10 (0.5762)validation : 7/10 (0.5762)early stopping at 7 with loss 0.5762
AttentionModel-training is done: 7/10
2020-10-31 | reset count: 0 | final loss: 0.5762 at epoch 6
[strategy_integration.py] set_seed(2020-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5700)training: 2/10 (0.5700)validation : 2/10 (0.5700)training: 3/10 (0.5700)validation : 3/10 (0.5674)training: 4/10 (0.5674)validation : 4/10 (0.5665)training: 5/10 (0.5665)validation : 5/10 (0.5665)training: 6/10 (0.5665)validation : 6/10 (0.5665)training: 7/10 (0.5665)validation : 7/10 (0.5665)early stopping at 7 with loss 0.5665
AttentionModel-training is done: 7/10
2020-11-30 | reset count: 0 | final loss: 0.5665 at epoch 4
making sample. (inference: True, date:2020-12-31 00:00:00, date_number: 408)
making sample. (inference: False, date:2020-11-30 00:00:00, date_number: 407)
[AttentionModel/2020-12-31 | get_logger | INFO]: ====================AttentionModel/2020-12-31 start====================
making sample. (inference: True, date:2021-01-31 00:00:00, date_number: 409)
making sample. (inference: False, date:2020-12-31 00:00:00, date_number: 408)
[AttentionModel/2021-01-31 | get_logger | INFO]: ====================AttentionModel/2021-01-31 start====================
[strategy_integration.py] set_seed(2020-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5733)training: 2/10 (0.5733)validation : 2/10 (0.5669)training: 3/10 (0.5669)validation : 3/10 (0.5649)training: 4/10 (0.5649)validation : 4/10 (0.5649)training: 5/10 (0.5649)validation : 5/10 (0.5649)training: 6/10 (0.5649)validation : 6/10 (0.5649)early stopping at 6 with loss 0.5649
AttentionModel-training is done: 6/10
2020-12-31 | reset count: 0 | final loss: 0.5649 at epoch 3
making sample. (inference: True, date:2021-02-28 00:00:00, date_number: 410)
making sample. (inference: False, date:2021-01-31 00:00:00, date_number: 409)
[AttentionModel/2021-02-28 | get_logger | INFO]: ====================AttentionModel/2021-02-28 start====================
[strategy_integration.py] set_seed(2021-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5571)training: 2/10 (0.5571)validation : 2/10 (0.5571)training: 3/10 (0.5571)validation : 3/10 (0.5532)training: 4/10 (0.5532)validation : 4/10 (0.5532)training: 5/10 (0.5532)validation : 5/10 (0.5504)training: 6/10 (0.5504)validation : 6/10 (0.5503)training: 7/10 (0.5503)validation : 7/10 (0.5503)early stopping at 7 with loss 0.5503
AttentionModel-training is done: 7/10
2021-01-31 | reset count: 0 | final loss: 0.5503 at epoch 6
making sample. (inference: True, date:2021-03-31 00:00:00, date_number: 411)
making sample. (inference: False, date:2021-02-28 00:00:00, date_number: 410)
[AttentionModel/2021-03-31 | get_logger | INFO]: ====================AttentionModel/2021-03-31 start====================
[strategy_integration.py] set_seed(2021-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5735)training: 2/10 (0.5735)validation : 2/10 (0.5688)training: 3/10 (0.5688)validation : 3/10 (0.5679)training: 4/10 (0.5679)validation : 4/10 (0.5679)training: 5/10 (0.5679)validation : 5/10 (0.5679)training: 6/10 (0.5679)validation : 6/10 (0.5661)training: 7/10 (0.5661)validation : 7/10 (0.5661)training: 8/10 (0.5661)validation : 8/10 (0.5661)training: 9/10 (0.5661)validation : 9/10 (0.5661)early stopping at 9 with loss 0.5661
AttentionModel-training is done: 9/10
2021-02-28 | reset count: 0 | final loss: 0.5661 at epoch 6
making sample. (inference: True, date:2021-04-30 00:00:00, date_number: 412)
making sample. (inference: False, date:2021-03-31 00:00:00, date_number: 411)
[AttentionModel/2021-04-30 | get_logger | INFO]: ====================AttentionModel/2021-04-30 start====================
[strategy_integration.py] set_seed(2021-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5685)training: 2/10 (0.5685)validation : 2/10 (0.5672)training: 3/10 (0.5672)validation : 3/10 (0.5666)training: 4/10 (0.5666)validation : 4/10 (0.5666)training: 5/10 (0.5666)validation : 5/10 (0.5651)training: 6/10 (0.5651)validation : 6/10 (0.5651)training: 7/10 (0.5651)validation : 7/10 (0.5595)training: 8/10 (0.5595)validation : 8/10 (0.5595)training: 9/10 (0.5595)validation : 9/10 (0.5595)early stopping at 9 with loss 0.5595
AttentionModel-training is done: 9/10
2021-03-31 | reset count: 0 | final loss: 0.5595 at epoch 7
making sample. (inference: True, date:2021-05-31 00:00:00, date_number: 413)
making sample. (inference: False, date:2021-04-30 00:00:00, date_number: 412)
[AttentionModel/2021-05-31 | get_logger | INFO]: ====================AttentionModel/2021-05-31 start====================
[strategy_integration.py] set_seed(2021-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5680)training: 2/10 (0.5680)validation : 2/10 (0.5662)training: 3/10 (0.5662)validation : 3/10 (0.5662)training: 4/10 (0.5662)validation : 4/10 (0.5641)training: 5/10 (0.5641)validation : 5/10 (0.5632)training: 6/10 (0.5632)validation : 6/10 (0.5632)early stopping at 6 with loss 0.5632
AttentionModel-training is done: 6/10
2021-04-30 | reset count: 0 | final loss: 0.5632 at epoch 5
making sample. (inference: True, date:2021-06-30 00:00:00, date_number: 414)
making sample. (inference: False, date:2021-05-31 00:00:00, date_number: 413)
[AttentionModel/2021-06-30 | get_logger | INFO]: ====================AttentionModel/2021-06-30 start====================
[strategy_integration.py] set_seed(2021-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5718)training: 2/10 (0.5718)validation : 2/10 (0.5718)training: 3/10 (0.5718)validation : 3/10 (0.5718)training: 4/10 (0.5718)validation : 4/10 (0.5718)training: 5/10 (0.5718)validation : 5/10 (0.5716)early stopping at 5 with loss 0.5716
AttentionModel-training is done: 5/10
2021-05-31 | reset count: 0 | final loss: 0.5716 at epoch 5
[strategy_integration.py] set_seed(2021-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5837)training: 2/10 (0.5837)validation : 2/10 (0.5837)training: 3/10 (0.5837)validation : 3/10 (0.5837)training: 4/10 (0.5837)validation : 4/10 (0.5835)training: 5/10 (0.5835)validation : 5/10 (0.5835)early stopping at 5 with loss 0.5835
AttentionModel-training is done: 5/10
2021-06-30 | reset count: 0 | final loss: 0.5835 at epoch 4
making sample. (inference: True, date:2021-07-31 00:00:00, date_number: 415)
making sample. (inference: False, date:2021-06-30 00:00:00, date_number: 414)
[AttentionModel/2021-07-31 | get_logger | INFO]: ====================AttentionModel/2021-07-31 start====================
making sample. (inference: True, date:2021-08-31 00:00:00, date_number: 416)
making sample. (inference: False, date:2021-07-31 00:00:00, date_number: 415)
[AttentionModel/2021-08-31 | get_logger | INFO]: ====================AttentionModel/2021-08-31 start====================
[strategy_integration.py] set_seed(2021-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5781)training: 2/10 (0.5781)validation : 2/10 (0.5727)training: 3/10 (0.5727)validation : 3/10 (0.5694)training: 4/10 (0.5694)validation : 4/10 (0.5694)training: 5/10 (0.5694)validation : 5/10 (0.5694)training: 6/10 (0.5694)validation : 6/10 (0.5694)early stopping at 6 with loss 0.5694
AttentionModel-training is done: 6/10
2021-07-31 | reset count: 0 | final loss: 0.5694 at epoch 3
[strategy_integration.py] set_seed(2021-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5741)training: 2/10 (0.5741)validation : 2/10 (0.5731)training: 3/10 (0.5731)validation : 3/10 (0.5712)training: 4/10 (0.5712)validation : 4/10 (0.5712)training: 5/10 (0.5712)validation : 5/10 (0.5712)training: 6/10 (0.5712)validation : 6/10 (0.5712)early stopping at 6 with loss 0.5712
AttentionModel-training is done: 6/10
2021-08-31 | reset count: 0 | final loss: 0.5712 at epoch 5
making sample. (inference: True, date:2021-09-30 00:00:00, date_number: 417)
making sample. (inference: False, date:2021-08-31 00:00:00, date_number: 416)
[AttentionModel/2021-09-30 | get_logger | INFO]: ====================AttentionModel/2021-09-30 start====================
making sample. (inference: True, date:2021-10-31 00:00:00, date_number: 418)
making sample. (inference: False, date:2021-09-30 00:00:00, date_number: 417)
[AttentionModel/2021-10-31 | get_logger | INFO]: ====================AttentionModel/2021-10-31 start====================
[strategy_integration.py] set_seed(2021-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5776)training: 2/10 (0.5776)validation : 2/10 (0.5741)training: 3/10 (0.5741)validation : 3/10 (0.5741)training: 4/10 (0.5741)validation : 4/10 (0.5741)training: 5/10 (0.5741)validation : 5/10 (0.5741)early stopping at 5 with loss 0.5741
AttentionModel-training is done: 5/10
2021-09-30 | reset count: 0 | final loss: 0.5741 at epoch 2
[strategy_integration.py] set_seed(2021-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5942)training: 2/10 (0.5942)validation : 2/10 (0.5894)training: 3/10 (0.5894)validation : 3/10 (0.5879)training: 4/10 (0.5879)validation : 4/10 (0.5879)training: 5/10 (0.5879)validation : 5/10 (0.5879)training: 6/10 (0.5879)validation : 6/10 (0.5879)early stopping at 6 with loss 0.5879
AttentionModel-training is done: 6/10
2021-10-31 | reset count: 0 | final loss: 0.5879 at epoch 3
[strategy | get_logger | INFO]: ====================strategy start====================
[strategy | _get_infer_dirs | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick188_test01/infer 이 strategy 대상에 추가됨
[strategy | _backtest | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick188_test01/infer is starting
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.154 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠹ [Compustat API] : Loading masking data⠹ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading masking data⠦ [Compustat API] : Loading get_historic_universe from cache⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading get_historic_universe from cache done in 1.932 secs
⠧ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.233 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.065 secs
⠋ [Compustat API] : Loading get_monthly_total_return from cache✓ [Compustat API] : Loading get_monthly_total_return from cache done in 0.064 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.079 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.073 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.072 secs
⠋ [Compustat API] : Loading get_monthly_volume_data from cache✓ [Compustat API] : Loading get_monthly_volume_data from cache done in 0.082 secs
[strategy | _allocating | INFO]: [STRATEGY] allocating start
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_weight_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] performance.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] combined.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] weight.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] universe.csv is saved.
[comparison | get_logger | INFO]: ====================comparison start====================
[comparison | compare | INFO]: [5. COMPARISON] comparison is saved.
[comparison | compare | INFO]:                        Harvest_Green_concept001_pick188_test01_price_return  ...  SPCOMP_total_win
2016-01-31_2021-04-30                                           0.234969     ...          0.765625

[1 rows x 27 columns]
2015-12-31
2016-01-31
2016-02-29
2016-03-31
2016-04-30
2016-05-31
2016-06-30
2016-07-31
2016-08-31
2016-09-30
2016-10-31
2016-11-30
2016-12-31
2017-01-31
2017-02-28
2017-03-31
2017-04-30
2017-05-31
2017-06-30
2017-07-31
2017-08-31
2017-09-30
2017-10-31
2017-11-30
2017-12-31
2018-01-31
2018-02-28
2018-03-31
2018-04-30
2018-05-31
2018-06-30
2018-07-31
2018-08-31
2018-09-30
2018-10-31
2018-11-30
2018-12-31
2019-01-31
2019-02-28
2019-03-31
2019-04-30
2019-05-31
2019-06-30
2019-07-31
2019-08-31
2019-09-30
2019-10-31
2019-11-30
2019-12-31
2020-01-31
2020-02-29
2020-03-31
2020-04-30
2020-05-31
2020-06-30
2020-07-31
2020-08-31
2020-09-30
2020-10-31
2020-11-30
2020-12-31
2021-01-31
2021-02-28
2021-03-31
2021-04-30
2021-05-31
2021-06-30
2021-07-31
2021-08-31
2021-09-30
2021-10-31
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.053 secs
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.051 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.07 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.084 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.078 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.71 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.727 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.731 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.73 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.732 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.081 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.715 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.717 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.715 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.709 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.708 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.085 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.732 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.083 secs
⠋ [Compustat API] : Cleaning past days cache✓ [Compustat API] : Cleaning past days cache done in 0.0 secs
auto caching이 활성화 되었습니다. /home/sronly/sr-storage/kirin_cache/kirin_api_cache/2.19.17_2021-11-23에 캐쉬를 저장합니다.
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.149 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache✓ [Compustat API] : Loading get_historic_universe from cache done in 2.026 secs
⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.326 secs
setting tensorflow random seed failed
load_data: gpa
load_data: rev_surp
load_data: cash_at
load_data: retail_mfr
load_data: m1
load_data: m2
load_data: pr_1m_0m
load_data: mv
load_data: btm
load_data: t3y
load_data: t5y
load_data: t7y
load_data: t1y_ff
load_data: t5y_ff
load_data: t10y_t2y
load_data: export_growth
load_data: import_growth
load_data: real_gig
load_data: ae_m
load_data: ia_ta
load_data: rc_a
load_data: aaa_t10y
load_data: baa_t10y
load_data: aaa_ff
load_data: real_pig
load_data: federal_tg
load_data: real_gdp
load_data: res_mom_12m_1m_0m
load_data: res_vol_6m_3m_0m
load_data: at
load_data: mom_12m_1m
load_data: ram_12m_0m
load_data: vol_3m
load_data: snp500_pr
load_data: wilshire500_pr
load_data: ted
load_data: ppi
load_data: trimmed_pce
load_data: unemploy
load_data: snp500_vol
load_data: op_lev
load_data: roe
load_data: std_u_e
load_data: baa_ff
load_data: core_cpi
load_data: core_pce
load_data: corporate_tg
load_data: industrial_prod
load_data: home_pr
load_data: r_s
load_data: r_a
load_data: fred_ff
load_data: t3m
load_data: t6m
load_data: t2y
load_data: t10y
load_data: aaa
load_data: baa
load_data: core_ppi
load_data: cpi
load_data: pce
load_data: wti
load_data: capa_util
load_data: snp500_pe
load_data: ret_noa
load_data: etm
load_data: ia_mv
load_data: sector_values
load_data: mv
load_data: pr_1m_0m
load_data: mv
making sample. (inference: True, date:2015-12-31 00:00:00, date_number: 348)
making sample. (inference: False, date:2012-12-31 00:00:00, date_number: 312)
making sample. (inference: False, date:2013-01-31 00:00:00, date_number: 313)
making sample. (inference: False, date:2013-02-28 00:00:00, date_number: 314)
making sample. (inference: False, date:2013-03-31 00:00:00, date_number: 315)
making sample. (inference: False, date:2013-04-30 00:00:00, date_number: 316)
making sample. (inference: False, date:2013-05-31 00:00:00, date_number: 317)
making sample. (inference: False, date:2013-06-30 00:00:00, date_number: 318)
making sample. (inference: False, date:2013-07-31 00:00:00, date_number: 319)
making sample. (inference: False, date:2013-08-31 00:00:00, date_number: 320)
making sample. (inference: False, date:2013-09-30 00:00:00, date_number: 321)
making sample. (inference: False, date:2013-10-31 00:00:00, date_number: 322)
making sample. (inference: False, date:2013-11-30 00:00:00, date_number: 323)
making sample. (inference: False, date:2013-12-31 00:00:00, date_number: 324)
making sample. (inference: False, date:2014-01-31 00:00:00, date_number: 325)
making sample. (inference: False, date:2014-02-28 00:00:00, date_number: 326)
making sample. (inference: False, date:2014-03-31 00:00:00, date_number: 327)
making sample. (inference: False, date:2014-04-30 00:00:00, date_number: 328)
making sample. (inference: False, date:2014-05-31 00:00:00, date_number: 329)
making sample. (inference: False, date:2014-06-30 00:00:00, date_number: 330)
making sample. (inference: False, date:2014-07-31 00:00:00, date_number: 331)
making sample. (inference: False, date:2014-08-31 00:00:00, date_number: 332)
making sample. (inference: False, date:2014-09-30 00:00:00, date_number: 333)
making sample. (inference: False, date:2014-10-31 00:00:00, date_number: 334)
making sample. (inference: False, date:2014-11-30 00:00:00, date_number: 335)
making sample. (inference: False, date:2014-12-31 00:00:00, date_number: 336)
making sample. (inference: False, date:2015-01-31 00:00:00, date_number: 337)
making sample. (inference: False, date:2015-02-28 00:00:00, date_number: 338)
making sample. (inference: False, date:2015-03-31 00:00:00, date_number: 339)
making sample. (inference: False, date:2015-04-30 00:00:00, date_number: 340)
making sample. (inference: False, date:2015-05-31 00:00:00, date_number: 341)
making sample. (inference: False, date:2015-06-30 00:00:00, date_number: 342)
making sample. (inference: False, date:2015-07-31 00:00:00, date_number: 343)
making sample. (inference: False, date:2015-08-31 00:00:00, date_number: 344)
making sample. (inference: False, date:2015-09-30 00:00:00, date_number: 345)
making sample. (inference: False, date:2015-10-31 00:00:00, date_number: 346)
making sample. (inference: False, date:2015-11-30 00:00:00, date_number: 347)
[AttentionModel/2015-12-31 | get_logger | INFO]: ====================AttentionModel/2015-12-31 start====================
making sample. (inference: True, date:2016-01-31 00:00:00, date_number: 349)
making sample. (inference: False, date:2015-12-31 00:00:00, date_number: 348)
[AttentionModel/2016-01-31 | get_logger | INFO]: ====================AttentionModel/2016-01-31 start====================
[strategy_integration.py] set_seed(2015-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5795)training: 2/10 (0.5795)validation : 2/10 (0.5742)training: 3/10 (0.5742)validation : 3/10 (0.5742)training: 4/10 (0.5742)validation : 4/10 (0.5742)training: 5/10 (0.5742)validation : 5/10 (0.5739)training: 6/10 (0.5739)validation : 6/10 (0.5739)early stopping at 6 with loss 0.5739
AttentionModel-training is done: 6/10
2015-12-31 | reset count: 0 | final loss: 0.5739 at epoch 5
[strategy_integration.py] set_seed(2016-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5756)training: 2/10 (0.5756)validation : 2/10 (0.5756)training: 3/10 (0.5756)validation : 3/10 (0.5756)training: 4/10 (0.5756)validation : 4/10 (0.5733)training: 5/10 (0.5733)validation : 5/10 (0.5733)early stopping at 5 with loss 0.5733
AttentionModel-training is done: 5/10
2016-01-31 | reset count: 0 | final loss: 0.5733 at epoch 4
making sample. (inference: True, date:2016-02-29 00:00:00, date_number: 350)
making sample. (inference: False, date:2016-01-31 00:00:00, date_number: 349)
[AttentionModel/2016-02-29 | get_logger | INFO]: ====================AttentionModel/2016-02-29 start====================
making sample. (inference: True, date:2016-03-31 00:00:00, date_number: 351)
making sample. (inference: False, date:2016-02-29 00:00:00, date_number: 350)
[AttentionModel/2016-03-31 | get_logger | INFO]: ====================AttentionModel/2016-03-31 start====================
[strategy_integration.py] set_seed(2016-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5776)training: 2/10 (0.5776)validation : 2/10 (0.5742)training: 3/10 (0.5742)validation : 3/10 (0.5742)training: 4/10 (0.5742)validation : 4/10 (0.5742)training: 5/10 (0.5742)validation : 5/10 (0.5742)early stopping at 5 with loss 0.5742
AttentionModel-training is done: 5/10
2016-02-29 | reset count: 0 | final loss: 0.5742 at epoch 2
making sample. (inference: True, date:2016-04-30 00:00:00, date_number: 352)
making sample. (inference: False, date:2016-03-31 00:00:00, date_number: 351)
[AttentionModel/2016-04-30 | get_logger | INFO]: ====================AttentionModel/2016-04-30 start====================
[strategy_integration.py] set_seed(2016-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5897)training: 2/10 (0.5897)validation : 2/10 (0.5807)training: 3/10 (0.5807)validation : 3/10 (0.5807)training: 4/10 (0.5807)validation : 4/10 (0.5807)training: 5/10 (0.5807)validation : 5/10 (0.5807)training: 6/10 (0.5807)validation : 6/10 (0.5807)training: 7/10 (0.5807)validation : 7/10 (0.5807)early stopping at 7 with loss 0.5807
AttentionModel-training is done: 7/10
2016-03-31 | reset count: 0 | final loss: 0.5807 at epoch 2
making sample. (inference: True, date:2016-05-31 00:00:00, date_number: 353)
making sample. (inference: False, date:2016-04-30 00:00:00, date_number: 352)
[AttentionModel/2016-05-31 | get_logger | INFO]: ====================AttentionModel/2016-05-31 start====================
[strategy_integration.py] set_seed(2016-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5872)training: 2/10 (0.5872)validation : 2/10 (0.5819)training: 3/10 (0.5819)validation : 3/10 (0.5819)training: 4/10 (0.5819)validation : 4/10 (0.5819)training: 5/10 (0.5819)validation : 5/10 (0.5819)training: 6/10 (0.5819)validation : 6/10 (0.5819)early stopping at 6 with loss 0.5819
AttentionModel-training is done: 6/10
2016-04-30 | reset count: 0 | final loss: 0.5819 at epoch 2
making sample. (inference: True, date:2016-06-30 00:00:00, date_number: 354)
making sample. (inference: False, date:2016-05-31 00:00:00, date_number: 353)
[AttentionModel/2016-06-30 | get_logger | INFO]: ====================AttentionModel/2016-06-30 start====================
[strategy_integration.py] set_seed(2016-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5666)training: 2/10 (0.5666)validation : 2/10 (0.5639)training: 3/10 (0.5639)validation : 3/10 (0.5639)training: 4/10 (0.5639)validation : 4/10 (0.5639)training: 5/10 (0.5639)validation : 5/10 (0.5639)training: 6/10 (0.5639)validation : 6/10 (0.5639)training: 7/10 (0.5639)validation : 7/10 (0.5639)early stopping at 7 with loss 0.5639
AttentionModel-training is done: 7/10
2016-05-31 | reset count: 0 | final loss: 0.5639 at epoch 2
[strategy_integration.py] set_seed(2016-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5813)training: 2/10 (0.5813)validation : 2/10 (0.5808)training: 3/10 (0.5808)validation : 3/10 (0.5771)training: 4/10 (0.5771)validation : 4/10 (0.5771)training: 5/10 (0.5771)validation : 5/10 (0.5771)training: 6/10 (0.5771)validation : 6/10 (0.5771)early stopping at 6 with loss 0.5771
AttentionModel-training is done: 6/10
2016-06-30 | reset count: 0 | final loss: 0.5771 at epoch 3
making sample. (inference: True, date:2016-07-31 00:00:00, date_number: 355)
making sample. (inference: False, date:2016-06-30 00:00:00, date_number: 354)
[AttentionModel/2016-07-31 | get_logger | INFO]: ====================AttentionModel/2016-07-31 start====================
making sample. (inference: True, date:2016-08-31 00:00:00, date_number: 356)
making sample. (inference: False, date:2016-07-31 00:00:00, date_number: 355)
[AttentionModel/2016-08-31 | get_logger | INFO]: ====================AttentionModel/2016-08-31 start====================
[strategy_integration.py] set_seed(2016-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5780)training: 2/10 (0.5780)validation : 2/10 (0.5758)training: 3/10 (0.5758)validation : 3/10 (0.5740)training: 4/10 (0.5740)validation : 4/10 (0.5740)training: 5/10 (0.5740)validation : 5/10 (0.5740)training: 6/10 (0.5740)validation : 6/10 (0.5740)early stopping at 6 with loss 0.5740
AttentionModel-training is done: 6/10
2016-07-31 | reset count: 0 | final loss: 0.5740 at epoch 3
[strategy_integration.py] set_seed(2016-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5716)training: 2/10 (0.5716)validation : 2/10 (0.5632)training: 3/10 (0.5632)validation : 3/10 (0.5632)training: 4/10 (0.5632)validation : 4/10 (0.5632)training: 5/10 (0.5632)validation : 5/10 (0.5632)training: 6/10 (0.5632)validation : 6/10 (0.5632)early stopping at 6 with loss 0.5632
AttentionModel-training is done: 6/10
2016-08-31 | reset count: 0 | final loss: 0.5632 at epoch 2
making sample. (inference: True, date:2016-09-30 00:00:00, date_number: 357)
making sample. (inference: False, date:2016-08-31 00:00:00, date_number: 356)
[AttentionModel/2016-09-30 | get_logger | INFO]: ====================AttentionModel/2016-09-30 start====================
making sample. (inference: True, date:2016-10-31 00:00:00, date_number: 358)
making sample. (inference: False, date:2016-09-30 00:00:00, date_number: 357)
[AttentionModel/2016-10-31 | get_logger | INFO]: ====================AttentionModel/2016-10-31 start====================
[strategy_integration.py] set_seed(2016-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5654)training: 2/10 (0.5654)validation : 2/10 (0.5654)training: 3/10 (0.5654)validation : 3/10 (0.5654)training: 4/10 (0.5654)validation : 4/10 (0.5654)training: 5/10 (0.5654)validation : 5/10 (0.5654)early stopping at 5 with loss 0.5654
AttentionModel-training is done: 5/10
2016-09-30 | reset count: 0 | final loss: 0.5654 at epoch 1
making sample. (inference: True, date:2016-11-30 00:00:00, date_number: 359)
making sample. (inference: False, date:2016-10-31 00:00:00, date_number: 358)
[AttentionModel/2016-11-30 | get_logger | INFO]: ====================AttentionModel/2016-11-30 start====================
[strategy_integration.py] set_seed(2016-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5662)training: 2/10 (0.5662)validation : 2/10 (0.5642)training: 3/10 (0.5642)validation : 3/10 (0.5629)training: 4/10 (0.5629)validation : 4/10 (0.5629)training: 5/10 (0.5629)validation : 5/10 (0.5629)training: 6/10 (0.5629)validation : 6/10 (0.5629)early stopping at 6 with loss 0.5629
AttentionModel-training is done: 6/10
2016-10-31 | reset count: 0 | final loss: 0.5629 at epoch 3
making sample. (inference: True, date:2016-12-31 00:00:00, date_number: 360)
making sample. (inference: False, date:2016-11-30 00:00:00, date_number: 359)
[AttentionModel/2016-12-31 | get_logger | INFO]: ====================AttentionModel/2016-12-31 start====================
[strategy_integration.py] set_seed(2016-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5695)training: 2/10 (0.5695)validation : 2/10 (0.5660)training: 3/10 (0.5660)validation : 3/10 (0.5660)training: 4/10 (0.5660)validation : 4/10 (0.5660)training: 5/10 (0.5660)validation : 5/10 (0.5660)training: 6/10 (0.5660)validation : 6/10 (0.5660)early stopping at 6 with loss 0.5660
AttentionModel-training is done: 6/10
2016-11-30 | reset count: 0 | final loss: 0.5660 at epoch 2
making sample. (inference: True, date:2017-01-31 00:00:00, date_number: 361)
making sample. (inference: False, date:2016-12-31 00:00:00, date_number: 360)
[AttentionModel/2017-01-31 | get_logger | INFO]: ====================AttentionModel/2017-01-31 start====================
[strategy_integration.py] set_seed(2016-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5725)training: 2/10 (0.5725)validation : 2/10 (0.5721)training: 3/10 (0.5721)validation : 3/10 (0.5664)training: 4/10 (0.5664)validation : 4/10 (0.5664)training: 5/10 (0.5664)validation : 5/10 (0.5664)training: 6/10 (0.5664)validation : 6/10 (0.5664)early stopping at 6 with loss 0.5664
AttentionModel-training is done: 6/10
2016-12-31 | reset count: 0 | final loss: 0.5664 at epoch 3
making sample. (inference: True, date:2017-02-28 00:00:00, date_number: 362)
making sample. (inference: False, date:2017-01-31 00:00:00, date_number: 361)
[AttentionModel/2017-02-28 | get_logger | INFO]: ====================AttentionModel/2017-02-28 start====================
[strategy_integration.py] set_seed(2017-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5704)training: 2/10 (0.5704)validation : 2/10 (0.5695)training: 3/10 (0.5695)validation : 3/10 (0.5695)training: 4/10 (0.5695)validation : 4/10 (0.5690)training: 5/10 (0.5690)validation : 5/10 (0.5690)training: 6/10 (0.5690)validation : 6/10 (0.5690)early stopping at 6 with loss 0.5690
AttentionModel-training is done: 6/10
2017-01-31 | reset count: 0 | final loss: 0.5690 at epoch 4
making sample. (inference: True, date:2017-03-31 00:00:00, date_number: 363)
making sample. (inference: False, date:2017-02-28 00:00:00, date_number: 362)
[strategy_integration.py] set_seed(2017-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5733)training: 2/10 (0.5733)validation : 2/10 (0.5733)training: 3/10 (0.5733)validation : 3/10 (0.5721)training: 4/10 (0.5721)validation : 4/10 (0.5716)training: 5/10 (0.5716)validation : 5/10 (0.5716)early stopping at 5 with loss 0.5716
AttentionModel-training is done: 5/10
2017-02-28 | reset count: 0 | final loss: 0.5716 at epoch 4
[AttentionModel/2017-03-31 | get_logger | INFO]: ====================AttentionModel/2017-03-31 start====================
making sample. (inference: True, date:2017-04-30 00:00:00, date_number: 364)
making sample. (inference: False, date:2017-03-31 00:00:00, date_number: 363)
[AttentionModel/2017-04-30 | get_logger | INFO]: ====================AttentionModel/2017-04-30 start====================
[strategy_integration.py] set_seed(2017-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5613)training: 2/10 (0.5613)validation : 2/10 (0.5613)training: 3/10 (0.5613)validation : 3/10 (0.5613)training: 4/10 (0.5613)validation : 4/10 (0.5613)training: 5/10 (0.5613)validation : 5/10 (0.5613)early stopping at 5 with loss 0.5613
AttentionModel-training is done: 5/10
2017-03-31 | reset count: 0 | final loss: 0.5613 at epoch 1
making sample. (inference: True, date:2017-05-31 00:00:00, date_number: 365)
making sample. (inference: False, date:2017-04-30 00:00:00, date_number: 364)
[AttentionModel/2017-05-31 | get_logger | INFO]: ====================AttentionModel/2017-05-31 start====================
[strategy_integration.py] set_seed(2017-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5660)training: 2/10 (0.5660)validation : 2/10 (0.5637)training: 3/10 (0.5637)validation : 3/10 (0.5637)training: 4/10 (0.5637)validation : 4/10 (0.5637)training: 5/10 (0.5637)validation : 5/10 (0.5591)training: 6/10 (0.5591)validation : 6/10 (0.5591)training: 7/10 (0.5591)validation : 7/10 (0.5591)training: 8/10 (0.5591)validation : 8/10 (0.5586)training: 9/10 (0.5586)validation : 9/10 (0.5586)early stopping at 9 with loss 0.5586
AttentionModel-training is done: 9/10
2017-04-30 | reset count: 0 | final loss: 0.5586 at epoch 8
[strategy_integration.py] set_seed(2017-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5683)training: 2/10 (0.5683)validation : 2/10 (0.5652)training: 3/10 (0.5652)validation : 3/10 (0.5652)training: 4/10 (0.5652)validation : 4/10 (0.5652)training: 5/10 (0.5652)validation : 5/10 (0.5614)training: 6/10 (0.5614)validation : 6/10 (0.5614)early stopping at 6 with loss 0.5614
AttentionModel-training is done: 6/10
2017-05-31 | reset count: 0 | final loss: 0.5614 at epoch 5
making sample. (inference: True, date:2017-06-30 00:00:00, date_number: 366)
making sample. (inference: False, date:2017-05-31 00:00:00, date_number: 365)
[AttentionModel/2017-06-30 | get_logger | INFO]: ====================AttentionModel/2017-06-30 start====================
making sample. (inference: True, date:2017-07-31 00:00:00, date_number: 367)
making sample. (inference: False, date:2017-06-30 00:00:00, date_number: 366)
[AttentionModel/2017-07-31 | get_logger | INFO]: ====================AttentionModel/2017-07-31 start====================
[strategy_integration.py] set_seed(2017-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5574)training: 2/10 (0.5574)validation : 2/10 (0.5574)training: 3/10 (0.5574)validation : 3/10 (0.5574)training: 4/10 (0.5574)validation : 4/10 (0.5574)training: 5/10 (0.5574)validation : 5/10 (0.5574)early stopping at 5 with loss 0.5574
AttentionModel-training is done: 5/10
2017-06-30 | reset count: 0 | final loss: 0.5574 at epoch 1
making sample. (inference: True, date:2017-08-31 00:00:00, date_number: 368)
making sample. (inference: False, date:2017-07-31 00:00:00, date_number: 367)
[AttentionModel/2017-08-31 | get_logger | INFO]: ====================AttentionModel/2017-08-31 start====================
[strategy_integration.py] set_seed(2017-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5719)training: 2/10 (0.5719)validation : 2/10 (0.5719)training: 3/10 (0.5719)validation : 3/10 (0.5719)training: 4/10 (0.5719)validation : 4/10 (0.5679)training: 5/10 (0.5679)validation : 5/10 (0.5679)training: 6/10 (0.5679)validation : 6/10 (0.5679)training: 7/10 (0.5679)validation : 7/10 (0.5679)early stopping at 7 with loss 0.5679
AttentionModel-training is done: 7/10
2017-07-31 | reset count: 0 | final loss: 0.5679 at epoch 4
making sample. (inference: True, date:2017-09-30 00:00:00, date_number: 369)
making sample. (inference: False, date:2017-08-31 00:00:00, date_number: 368)
[AttentionModel/2017-09-30 | get_logger | INFO]: ====================AttentionModel/2017-09-30 start====================
[strategy_integration.py] set_seed(2017-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5702)training: 2/10 (0.5702)validation : 2/10 (0.5665)training: 3/10 (0.5665)validation : 3/10 (0.5623)training: 4/10 (0.5623)validation : 4/10 (0.5623)training: 5/10 (0.5623)validation : 5/10 (0.5623)training: 6/10 (0.5623)validation : 6/10 (0.5623)early stopping at 6 with loss 0.5623
AttentionModel-training is done: 6/10
2017-08-31 | reset count: 0 | final loss: 0.5623 at epoch 3
making sample. (inference: True, date:2017-10-31 00:00:00, date_number: 370)
making sample. (inference: False, date:2017-09-30 00:00:00, date_number: 369)
[AttentionModel/2017-10-31 | get_logger | INFO]: ====================AttentionModel/2017-10-31 start====================
[strategy_integration.py] set_seed(2017-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5732)training: 2/10 (0.5732)validation : 2/10 (0.5732)training: 3/10 (0.5732)validation : 3/10 (0.5703)training: 4/10 (0.5703)validation : 4/10 (0.5703)training: 5/10 (0.5703)validation : 5/10 (0.5703)training: 6/10 (0.5703)validation : 6/10 (0.5703)early stopping at 6 with loss 0.5703
AttentionModel-training is done: 6/10
2017-09-30 | reset count: 0 | final loss: 0.5703 at epoch 3
making sample. (inference: True, date:2017-11-30 00:00:00, date_number: 371)
making sample. (inference: False, date:2017-10-31 00:00:00, date_number: 370)
[AttentionModel/2017-11-30 | get_logger | INFO]: ====================AttentionModel/2017-11-30 start====================
[strategy_integration.py] set_seed(2017-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5657)training: 2/10 (0.5657)validation : 2/10 (0.5644)training: 3/10 (0.5644)validation : 3/10 (0.5629)training: 4/10 (0.5629)validation : 4/10 (0.5629)training: 5/10 (0.5629)validation : 5/10 (0.5629)training: 6/10 (0.5629)validation : 6/10 (0.5629)early stopping at 6 with loss 0.5629
AttentionModel-training is done: 6/10
2017-10-31 | reset count: 0 | final loss: 0.5629 at epoch 3
making sample. (inference: True, date:2017-12-31 00:00:00, date_number: 372)
making sample. (inference: False, date:2017-11-30 00:00:00, date_number: 371)
[AttentionModel/2017-12-31 | get_logger | INFO]: ====================AttentionModel/2017-12-31 start====================
[strategy_integration.py] set_seed(2017-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5596)training: 2/10 (0.5596)validation : 2/10 (0.5586)training: 3/10 (0.5586)validation : 3/10 (0.5586)training: 4/10 (0.5586)validation : 4/10 (0.5586)training: 5/10 (0.5586)validation : 5/10 (0.5586)early stopping at 5 with loss 0.5586
AttentionModel-training is done: 5/10
2017-11-30 | reset count: 0 | final loss: 0.5586 at epoch 2
[strategy_integration.py] set_seed(2017-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5647)training: 2/10 (0.5647)validation : 2/10 (0.5630)training: 3/10 (0.5630)validation : 3/10 (0.5630)training: 4/10 (0.5630)validation : 4/10 (0.5630)training: 5/10 (0.5630)validation : 5/10 (0.5622)early stopping at 5 with loss 0.5622
AttentionModel-training is done: 5/10
2017-12-31 | reset count: 0 | final loss: 0.5622 at epoch 5
making sample. (inference: True, date:2018-01-31 00:00:00, date_number: 373)
making sample. (inference: False, date:2017-12-31 00:00:00, date_number: 372)
[AttentionModel/2018-01-31 | get_logger | INFO]: ====================AttentionModel/2018-01-31 start====================
making sample. (inference: True, date:2018-02-28 00:00:00, date_number: 374)
making sample. (inference: False, date:2018-01-31 00:00:00, date_number: 373)
[AttentionModel/2018-02-28 | get_logger | INFO]: ====================AttentionModel/2018-02-28 start====================
[strategy_integration.py] set_seed(2018-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5660)training: 2/10 (0.5660)validation : 2/10 (0.5660)training: 3/10 (0.5660)validation : 3/10 (0.5656)training: 4/10 (0.5656)validation : 4/10 (0.5656)training: 5/10 (0.5656)validation : 5/10 (0.5656)training: 6/10 (0.5656)validation : 6/10 (0.5656)early stopping at 6 with loss 0.5656
AttentionModel-training is done: 6/10
2018-01-31 | reset count: 0 | final loss: 0.5656 at epoch 3
[strategy_integration.py] set_seed(2018-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5612)training: 2/10 (0.5612)validation : 2/10 (0.5612)training: 3/10 (0.5612)validation : 3/10 (0.5595)training: 4/10 (0.5595)validation : 4/10 (0.5595)training: 5/10 (0.5595)validation : 5/10 (0.5595)early stopping at 5 with loss 0.5595
AttentionModel-training is done: 5/10
2018-02-28 | reset count: 0 | final loss: 0.5595 at epoch 3
making sample. (inference: True, date:2018-03-31 00:00:00, date_number: 375)
making sample. (inference: False, date:2018-02-28 00:00:00, date_number: 374)
[AttentionModel/2018-03-31 | get_logger | INFO]: ====================AttentionModel/2018-03-31 start====================
making sample. (inference: True, date:2018-04-30 00:00:00, date_number: 376)
making sample. (inference: False, date:2018-03-31 00:00:00, date_number: 375)
[AttentionModel/2018-04-30 | get_logger | INFO]: ====================AttentionModel/2018-04-30 start====================
[strategy_integration.py] set_seed(2018-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5635)training: 2/10 (0.5635)validation : 2/10 (0.5635)training: 3/10 (0.5635)validation : 3/10 (0.5617)training: 4/10 (0.5617)validation : 4/10 (0.5617)training: 5/10 (0.5617)validation : 5/10 (0.5617)early stopping at 5 with loss 0.5617
AttentionModel-training is done: 5/10
2018-03-31 | reset count: 0 | final loss: 0.5617 at epoch 3
making sample. (inference: True, date:2018-05-31 00:00:00, date_number: 377)
making sample. (inference: False, date:2018-04-30 00:00:00, date_number: 376)
[AttentionModel/2018-05-31 | get_logger | INFO]: ====================AttentionModel/2018-05-31 start====================
[strategy_integration.py] set_seed(2018-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5723)training: 2/10 (0.5723)validation : 2/10 (0.5721)training: 3/10 (0.5721)validation : 3/10 (0.5705)training: 4/10 (0.5705)validation : 4/10 (0.5684)training: 5/10 (0.5684)validation : 5/10 (0.5684)training: 6/10 (0.5684)validation : 6/10 (0.5676)training: 7/10 (0.5676)validation : 7/10 (0.5676)early stopping at 7 with loss 0.5676
AttentionModel-training is done: 7/10
2018-04-30 | reset count: 0 | final loss: 0.5676 at epoch 6
[strategy_integration.py] set_seed(2018-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5783)training: 2/10 (0.5783)validation : 2/10 (0.5732)training: 3/10 (0.5732)validation : 3/10 (0.5732)training: 4/10 (0.5732)validation : 4/10 (0.5732)training: 5/10 (0.5732)validation : 5/10 (0.5732)early stopping at 5 with loss 0.5732
AttentionModel-training is done: 5/10
2018-05-31 | reset count: 0 | final loss: 0.5732 at epoch 2
making sample. (inference: True, date:2018-06-30 00:00:00, date_number: 378)
making sample. (inference: False, date:2018-05-31 00:00:00, date_number: 377)
[AttentionModel/2018-06-30 | get_logger | INFO]: ====================AttentionModel/2018-06-30 start====================
making sample. (inference: True, date:2018-07-31 00:00:00, date_number: 379)
making sample. (inference: False, date:2018-06-30 00:00:00, date_number: 378)
[AttentionModel/2018-07-31 | get_logger | INFO]: ====================AttentionModel/2018-07-31 start====================
[strategy_integration.py] set_seed(2018-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5675)training: 2/10 (0.5675)validation : 2/10 (0.5675)training: 3/10 (0.5675)validation : 3/10 (0.5654)training: 4/10 (0.5654)validation : 4/10 (0.5654)training: 5/10 (0.5654)validation : 5/10 (0.5654)early stopping at 5 with loss 0.5654
AttentionModel-training is done: 5/10
2018-06-30 | reset count: 0 | final loss: 0.5654 at epoch 3
[strategy_integration.py] set_seed(2018-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5789)training: 2/10 (0.5789)validation : 2/10 (0.5780)training: 3/10 (0.5780)validation : 3/10 (0.5750)training: 4/10 (0.5750)validation : 4/10 (0.5750)training: 5/10 (0.5750)validation : 5/10 (0.5750)training: 6/10 (0.5750)validation : 6/10 (0.5750)early stopping at 6 with loss 0.5750
AttentionModel-training is done: 6/10
2018-07-31 | reset count: 0 | final loss: 0.5750 at epoch 3
making sample. (inference: True, date:2018-08-31 00:00:00, date_number: 380)
making sample. (inference: False, date:2018-07-31 00:00:00, date_number: 379)
[AttentionModel/2018-08-31 | get_logger | INFO]: ====================AttentionModel/2018-08-31 start====================
making sample. (inference: True, date:2018-09-30 00:00:00, date_number: 381)
making sample. (inference: False, date:2018-08-31 00:00:00, date_number: 380)
[AttentionModel/2018-09-30 | get_logger | INFO]: ====================AttentionModel/2018-09-30 start====================
[strategy_integration.py] set_seed(2018-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5780)training: 2/10 (0.5780)validation : 2/10 (0.5754)training: 3/10 (0.5754)validation : 3/10 (0.5726)training: 4/10 (0.5726)validation : 4/10 (0.5726)training: 5/10 (0.5726)validation : 5/10 (0.5726)training: 6/10 (0.5726)validation : 6/10 (0.5726)early stopping at 6 with loss 0.5726
AttentionModel-training is done: 6/10
2018-08-31 | reset count: 0 | final loss: 0.5726 at epoch 3
[strategy_integration.py] set_seed(2018-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5720)training: 2/10 (0.5720)validation : 2/10 (0.5716)training: 3/10 (0.5716)validation : 3/10 (0.5716)training: 4/10 (0.5716)validation : 4/10 (0.5716)training: 5/10 (0.5716)validation : 5/10 (0.5712)early stopping at 5 with loss 0.5712
AttentionModel-training is done: 5/10
2018-09-30 | reset count: 0 | final loss: 0.5712 at epoch 5
making sample. (inference: True, date:2018-10-31 00:00:00, date_number: 382)
making sample. (inference: False, date:2018-09-30 00:00:00, date_number: 381)
[AttentionModel/2018-10-31 | get_logger | INFO]: ====================AttentionModel/2018-10-31 start====================
making sample. (inference: True, date:2018-11-30 00:00:00, date_number: 383)
making sample. (inference: False, date:2018-10-31 00:00:00, date_number: 382)
[AttentionModel/2018-11-30 | get_logger | INFO]: ====================AttentionModel/2018-11-30 start====================
[strategy_integration.py] set_seed(2018-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5842)training: 2/10 (0.5842)validation : 2/10 (0.5842)training: 3/10 (0.5842)validation : 3/10 (0.5818)training: 4/10 (0.5818)validation : 4/10 (0.5807)training: 5/10 (0.5807)validation : 5/10 (0.5807)training: 6/10 (0.5807)validation : 6/10 (0.5807)early stopping at 6 with loss 0.5807
AttentionModel-training is done: 6/10
2018-10-31 | reset count: 0 | final loss: 0.5807 at epoch 4
[strategy_integration.py] set_seed(2018-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5892)training: 2/10 (0.5892)validation : 2/10 (0.5874)training: 3/10 (0.5874)validation : 3/10 (0.5874)training: 4/10 (0.5874)validation : 4/10 (0.5874)training: 5/10 (0.5874)validation : 5/10 (0.5874)early stopping at 5 with loss 0.5874
AttentionModel-training is done: 5/10
2018-11-30 | reset count: 0 | final loss: 0.5874 at epoch 2
making sample. (inference: True, date:2018-12-31 00:00:00, date_number: 384)
making sample. (inference: False, date:2018-11-30 00:00:00, date_number: 383)
[AttentionModel/2018-12-31 | get_logger | INFO]: ====================AttentionModel/2018-12-31 start====================
making sample. (inference: True, date:2019-01-31 00:00:00, date_number: 385)
making sample. (inference: False, date:2018-12-31 00:00:00, date_number: 384)
[AttentionModel/2019-01-31 | get_logger | INFO]: ====================AttentionModel/2019-01-31 start====================
[strategy_integration.py] set_seed(2018-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5808)training: 2/10 (0.5808)validation : 2/10 (0.5808)training: 3/10 (0.5808)validation : 3/10 (0.5808)training: 4/10 (0.5808)validation : 4/10 (0.5804)training: 5/10 (0.5804)validation : 5/10 (0.5790)training: 6/10 (0.5790)validation : 6/10 (0.5775)training: 7/10 (0.5775)validation : 7/10 (0.5775)early stopping at 7 with loss 0.5775
AttentionModel-training is done: 7/10
2018-12-31 | reset count: 0 | final loss: 0.5775 at epoch 6
[strategy_integration.py] set_seed(2019-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5923)training: 2/10 (0.5923)validation : 2/10 (0.5873)training: 3/10 (0.5873)validation : 3/10 (0.5870)training: 4/10 (0.5870)validation : 4/10 (0.5870)training: 5/10 (0.5870)validation : 5/10 (0.5869)early stopping at 5 with loss 0.5869
AttentionModel-training is done: 5/10
2019-01-31 | reset count: 0 | final loss: 0.5869 at epoch 5
making sample. (inference: True, date:2019-02-28 00:00:00, date_number: 386)
making sample. (inference: False, date:2019-01-31 00:00:00, date_number: 385)
[AttentionModel/2019-02-28 | get_logger | INFO]: ====================AttentionModel/2019-02-28 start====================
making sample. (inference: True, date:2019-03-31 00:00:00, date_number: 387)
making sample. (inference: False, date:2019-02-28 00:00:00, date_number: 386)
[AttentionModel/2019-03-31 | get_logger | INFO]: ====================AttentionModel/2019-03-31 start====================
[strategy_integration.py] set_seed(2019-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5892)training: 2/10 (0.5892)validation : 2/10 (0.5855)training: 3/10 (0.5855)validation : 3/10 (0.5855)training: 4/10 (0.5855)validation : 4/10 (0.5855)training: 5/10 (0.5855)validation : 5/10 (0.5834)training: 6/10 (0.5834)validation : 6/10 (0.5834)training: 7/10 (0.5834)validation : 7/10 (0.5834)training: 8/10 (0.5834)validation : 8/10 (0.5834)early stopping at 8 with loss 0.5834
AttentionModel-training is done: 8/10
2019-02-28 | reset count: 0 | final loss: 0.5834 at epoch 5
making sample. (inference: True, date:2019-04-30 00:00:00, date_number: 388)
making sample. (inference: False, date:2019-03-31 00:00:00, date_number: 387)
[AttentionModel/2019-04-30 | get_logger | INFO]: ====================AttentionModel/2019-04-30 start====================
[strategy_integration.py] set_seed(2019-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5920)training: 2/10 (0.5920)validation : 2/10 (0.5920)training: 3/10 (0.5920)validation : 3/10 (0.5907)training: 4/10 (0.5907)validation : 4/10 (0.5889)training: 5/10 (0.5889)validation : 5/10 (0.5889)training: 6/10 (0.5889)validation : 6/10 (0.5889)training: 7/10 (0.5889)validation : 7/10 (0.5864)training: 8/10 (0.5864)validation : 8/10 (0.5864)training: 9/10 (0.5864)validation : 9/10 (0.5864)early stopping at 9 with loss 0.5864
AttentionModel-training is done: 9/10
2019-03-31 | reset count: 0 | final loss: 0.5864 at epoch 7
making sample. (inference: True, date:2019-05-31 00:00:00, date_number: 389)
making sample. (inference: False, date:2019-04-30 00:00:00, date_number: 388)
[AttentionModel/2019-05-31 | get_logger | INFO]: ====================AttentionModel/2019-05-31 start====================
[strategy_integration.py] set_seed(2019-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5911)training: 2/10 (0.5911)validation : 2/10 (0.5899)training: 3/10 (0.5899)validation : 3/10 (0.5854)training: 4/10 (0.5854)validation : 4/10 (0.5853)training: 5/10 (0.5853)validation : 5/10 (0.5853)training: 6/10 (0.5853)validation : 6/10 (0.5853)early stopping at 6 with loss 0.5853
AttentionModel-training is done: 6/10
2019-04-30 | reset count: 0 | final loss: 0.5853 at epoch 4
making sample. (inference: True, date:2019-06-30 00:00:00, date_number: 390)
making sample. (inference: False, date:2019-05-31 00:00:00, date_number: 389)
[AttentionModel/2019-06-30 | get_logger | INFO]: ====================AttentionModel/2019-06-30 start====================
[strategy_integration.py] set_seed(2019-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5832)training: 2/10 (0.5832)validation : 2/10 (0.5810)training: 3/10 (0.5810)validation : 3/10 (0.5810)training: 4/10 (0.5810)validation : 4/10 (0.5771)training: 5/10 (0.5771)validation : 5/10 (0.5771)training: 6/10 (0.5771)validation : 6/10 (0.5771)training: 7/10 (0.5771)validation : 7/10 (0.5771)early stopping at 7 with loss 0.5771
AttentionModel-training is done: 7/10
2019-05-31 | reset count: 0 | final loss: 0.5771 at epoch 4
making sample. (inference: True, date:2019-07-31 00:00:00, date_number: 391)
making sample. (inference: False, date:2019-06-30 00:00:00, date_number: 390)
[AttentionModel/2019-07-31 | get_logger | INFO]: ====================AttentionModel/2019-07-31 start====================
[strategy_integration.py] set_seed(2019-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5856)training: 2/10 (0.5856)validation : 2/10 (0.5856)training: 3/10 (0.5856)validation : 3/10 (0.5856)training: 4/10 (0.5856)validation : 4/10 (0.5827)training: 5/10 (0.5827)validation : 5/10 (0.5827)training: 6/10 (0.5827)validation : 6/10 (0.5827)early stopping at 6 with loss 0.5827
AttentionModel-training is done: 6/10
2019-06-30 | reset count: 0 | final loss: 0.5827 at epoch 4
making sample. (inference: True, date:2019-08-31 00:00:00, date_number: 392)
making sample. (inference: False, date:2019-07-31 00:00:00, date_number: 391)
[AttentionModel/2019-08-31 | get_logger | INFO]: ====================AttentionModel/2019-08-31 start====================
[strategy_integration.py] set_seed(2019-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5895)training: 2/10 (0.5895)validation : 2/10 (0.5881)training: 3/10 (0.5881)validation : 3/10 (0.5841)training: 4/10 (0.5841)validation : 4/10 (0.5841)training: 5/10 (0.5841)validation : 5/10 (0.5841)training: 6/10 (0.5841)validation : 6/10 (0.5841)early stopping at 6 with loss 0.5841
AttentionModel-training is done: 6/10
2019-07-31 | reset count: 0 | final loss: 0.5841 at epoch 3
[strategy_integration.py] set_seed(2019-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5854)training: 2/10 (0.5854)validation : 2/10 (0.5814)training: 3/10 (0.5814)validation : 3/10 (0.5814)training: 4/10 (0.5814)validation : 4/10 (0.5814)training: 5/10 (0.5814)validation : 5/10 (0.5814)early stopping at 5 with loss 0.5814
AttentionModel-training is done: 5/10
2019-08-31 | reset count: 0 | final loss: 0.5814 at epoch 2
making sample. (inference: True, date:2019-09-30 00:00:00, date_number: 393)
making sample. (inference: False, date:2019-08-31 00:00:00, date_number: 392)
[AttentionModel/2019-09-30 | get_logger | INFO]: ====================AttentionModel/2019-09-30 start====================
making sample. (inference: True, date:2019-10-31 00:00:00, date_number: 394)
making sample. (inference: False, date:2019-09-30 00:00:00, date_number: 393)
[AttentionModel/2019-10-31 | get_logger | INFO]: ====================AttentionModel/2019-10-31 start====================
[strategy_integration.py] set_seed(2019-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5908)training: 2/10 (0.5908)validation : 2/10 (0.5834)training: 3/10 (0.5834)validation : 3/10 (0.5834)training: 4/10 (0.5834)validation : 4/10 (0.5834)training: 5/10 (0.5834)validation : 5/10 (0.5834)early stopping at 5 with loss 0.5834
AttentionModel-training is done: 5/10
2019-09-30 | reset count: 0 | final loss: 0.5834 at epoch 2
making sample. (inference: True, date:2019-11-30 00:00:00, date_number: 395)
making sample. (inference: False, date:2019-10-31 00:00:00, date_number: 394)
[AttentionModel/2019-11-30 | get_logger | INFO]: ====================AttentionModel/2019-11-30 start====================
[strategy_integration.py] set_seed(2019-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5866)training: 2/10 (0.5866)validation : 2/10 (0.5866)training: 3/10 (0.5866)validation : 3/10 (0.5866)training: 4/10 (0.5866)validation : 4/10 (0.5866)training: 5/10 (0.5866)validation : 5/10 (0.5829)training: 6/10 (0.5829)validation : 6/10 (0.5829)training: 7/10 (0.5829)validation : 7/10 (0.5829)training: 8/10 (0.5829)validation : 8/10 (0.5829)early stopping at 8 with loss 0.5829
AttentionModel-training is done: 8/10
2019-10-31 | reset count: 0 | final loss: 0.5829 at epoch 5
making sample. (inference: True, date:2019-12-31 00:00:00, date_number: 396)
making sample. (inference: False, date:2019-11-30 00:00:00, date_number: 395)
[AttentionModel/2019-12-31 | get_logger | INFO]: ====================AttentionModel/2019-12-31 start====================
[strategy_integration.py] set_seed(2019-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5867)training: 2/10 (0.5867)validation : 2/10 (0.5861)training: 3/10 (0.5861)validation : 3/10 (0.5861)training: 4/10 (0.5861)validation : 4/10 (0.5861)training: 5/10 (0.5861)validation : 5/10 (0.5852)training: 6/10 (0.5852)validation : 6/10 (0.5852)early stopping at 6 with loss 0.5852
AttentionModel-training is done: 6/10
2019-11-30 | reset count: 0 | final loss: 0.5852 at epoch 5
making sample. (inference: True, date:2020-01-31 00:00:00, date_number: 397)
making sample. (inference: False, date:2019-12-31 00:00:00, date_number: 396)
[AttentionModel/2020-01-31 | get_logger | INFO]: ====================AttentionModel/2020-01-31 start====================
[strategy_integration.py] set_seed(2019-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5913)training: 2/10 (0.5913)validation : 2/10 (0.5882)training: 3/10 (0.5882)validation : 3/10 (0.5874)training: 4/10 (0.5874)validation : 4/10 (0.5874)training: 5/10 (0.5874)validation : 5/10 (0.5863)training: 6/10 (0.5863)validation : 6/10 (0.5863)early stopping at 6 with loss 0.5863
AttentionModel-training is done: 6/10
2019-12-31 | reset count: 0 | final loss: 0.5863 at epoch 5
[strategy_integration.py] set_seed(2020-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5832)training: 2/10 (0.5832)validation : 2/10 (0.5819)training: 3/10 (0.5819)validation : 3/10 (0.5807)training: 4/10 (0.5807)validation : 4/10 (0.5807)training: 5/10 (0.5807)validation : 5/10 (0.5807)early stopping at 5 with loss 0.5807
AttentionModel-training is done: 5/10
2020-01-31 | reset count: 0 | final loss: 0.5807 at epoch 3
making sample. (inference: True, date:2020-02-29 00:00:00, date_number: 398)
making sample. (inference: False, date:2020-01-31 00:00:00, date_number: 397)
[AttentionModel/2020-02-29 | get_logger | INFO]: ====================AttentionModel/2020-02-29 start====================
making sample. (inference: True, date:2020-03-31 00:00:00, date_number: 399)
making sample. (inference: False, date:2020-02-29 00:00:00, date_number: 398)
[AttentionModel/2020-03-31 | get_logger | INFO]: ====================AttentionModel/2020-03-31 start====================
[strategy_integration.py] set_seed(2020-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5858)training: 2/10 (0.5858)validation : 2/10 (0.5851)training: 3/10 (0.5851)validation : 3/10 (0.5825)training: 4/10 (0.5825)validation : 4/10 (0.5825)training: 5/10 (0.5825)validation : 5/10 (0.5825)early stopping at 5 with loss 0.5825
AttentionModel-training is done: 5/10
2020-02-29 | reset count: 0 | final loss: 0.5825 at epoch 3
making sample. (inference: True, date:2020-04-30 00:00:00, date_number: 400)
making sample. (inference: False, date:2020-03-31 00:00:00, date_number: 399)
[AttentionModel/2020-04-30 | get_logger | INFO]: ====================AttentionModel/2020-04-30 start====================
[strategy_integration.py] set_seed(2020-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5861)training: 2/10 (0.5861)validation : 2/10 (0.5836)training: 3/10 (0.5836)validation : 3/10 (0.5836)training: 4/10 (0.5836)validation : 4/10 (0.5836)training: 5/10 (0.5836)validation : 5/10 (0.5808)training: 6/10 (0.5808)validation : 6/10 (0.5808)training: 7/10 (0.5808)validation : 7/10 (0.5798)training: 8/10 (0.5798)validation : 8/10 (0.5798)early stopping at 8 with loss 0.5798
AttentionModel-training is done: 8/10
2020-03-31 | reset count: 0 | final loss: 0.5798 at epoch 7
making sample. (inference: True, date:2020-05-31 00:00:00, date_number: 401)
making sample. (inference: False, date:2020-04-30 00:00:00, date_number: 400)
[AttentionModel/2020-05-31 | get_logger | INFO]: ====================AttentionModel/2020-05-31 start====================
[strategy_integration.py] set_seed(2020-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5731)training: 2/10 (0.5731)validation : 2/10 (0.5731)training: 3/10 (0.5731)validation : 3/10 (0.5731)training: 4/10 (0.5731)validation : 4/10 (0.5731)training: 5/10 (0.5731)validation : 5/10 (0.5731)early stopping at 5 with loss 0.5731
AttentionModel-training is done: 5/10
2020-04-30 | reset count: 0 | final loss: 0.5731 at epoch 1
making sample. (inference: True, date:2020-06-30 00:00:00, date_number: 402)
making sample. (inference: False, date:2020-05-31 00:00:00, date_number: 401)
[AttentionModel/2020-06-30 | get_logger | INFO]: ====================AttentionModel/2020-06-30 start====================
[strategy_integration.py] set_seed(2020-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5808)training: 2/10 (0.5808)validation : 2/10 (0.5785)training: 3/10 (0.5785)validation : 3/10 (0.5740)training: 4/10 (0.5740)validation : 4/10 (0.5740)training: 5/10 (0.5740)validation : 5/10 (0.5740)training: 6/10 (0.5740)validation : 6/10 (0.5740)early stopping at 6 with loss 0.5740
AttentionModel-training is done: 6/10
2020-05-31 | reset count: 0 | final loss: 0.5740 at epoch 3
[strategy_integration.py] set_seed(2020-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5711)training: 2/10 (0.5711)validation : 2/10 (0.5690)training: 3/10 (0.5690)validation : 3/10 (0.5690)training: 4/10 (0.5690)validation : 4/10 (0.5690)training: 5/10 (0.5690)validation : 5/10 (0.5690)early stopping at 5 with loss 0.5690
AttentionModel-training is done: 5/10
2020-06-30 | reset count: 0 | final loss: 0.5690 at epoch 2
making sample. (inference: True, date:2020-07-31 00:00:00, date_number: 403)
making sample. (inference: False, date:2020-06-30 00:00:00, date_number: 402)
[AttentionModel/2020-07-31 | get_logger | INFO]: ====================AttentionModel/2020-07-31 start====================
making sample. (inference: True, date:2020-08-31 00:00:00, date_number: 404)
making sample. (inference: False, date:2020-07-31 00:00:00, date_number: 403)
[AttentionModel/2020-08-31 | get_logger | INFO]: ====================AttentionModel/2020-08-31 start====================
[strategy_integration.py] set_seed(2020-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5825)training: 2/10 (0.5825)validation : 2/10 (0.5805)training: 3/10 (0.5805)validation : 3/10 (0.5805)training: 4/10 (0.5805)validation : 4/10 (0.5805)training: 5/10 (0.5805)validation : 5/10 (0.5805)training: 6/10 (0.5805)validation : 6/10 (0.5780)training: 7/10 (0.5780)validation : 7/10 (0.5780)training: 8/10 (0.5780)validation : 8/10 (0.5780)early stopping at 8 with loss 0.5780
AttentionModel-training is done: 8/10
2020-07-31 | reset count: 0 | final loss: 0.5780 at epoch 6
[strategy_integration.py] set_seed(2020-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5714)training: 2/10 (0.5714)validation : 2/10 (0.5659)training: 3/10 (0.5659)validation : 3/10 (0.5659)training: 4/10 (0.5659)validation : 4/10 (0.5659)training: 5/10 (0.5659)validation : 5/10 (0.5645)training: 6/10 (0.5645)validation : 6/10 (0.5645)early stopping at 6 with loss 0.5645
AttentionModel-training is done: 6/10
2020-08-31 | reset count: 0 | final loss: 0.5645 at epoch 5
making sample. (inference: True, date:2020-09-30 00:00:00, date_number: 405)
making sample. (inference: False, date:2020-08-31 00:00:00, date_number: 404)
[AttentionModel/2020-09-30 | get_logger | INFO]: ====================AttentionModel/2020-09-30 start====================
making sample. (inference: True, date:2020-10-31 00:00:00, date_number: 406)
making sample. (inference: False, date:2020-09-30 00:00:00, date_number: 405)
[AttentionModel/2020-10-31 | get_logger | INFO]: ====================AttentionModel/2020-10-31 start====================
[strategy_integration.py] set_seed(2020-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5726)training: 2/10 (0.5726)validation : 2/10 (0.5698)training: 3/10 (0.5698)validation : 3/10 (0.5698)training: 4/10 (0.5698)validation : 4/10 (0.5698)training: 5/10 (0.5698)validation : 5/10 (0.5658)training: 6/10 (0.5658)validation : 6/10 (0.5658)training: 7/10 (0.5658)validation : 7/10 (0.5658)early stopping at 7 with loss 0.5658
AttentionModel-training is done: 7/10
2020-09-30 | reset count: 0 | final loss: 0.5658 at epoch 5
[strategy_integration.py] set_seed(2020-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5826)training: 2/10 (0.5826)validation : 2/10 (0.5820)training: 3/10 (0.5820)validation : 3/10 (0.5820)training: 4/10 (0.5820)validation : 4/10 (0.5820)training: 5/10 (0.5820)validation : 5/10 (0.5820)early stopping at 5 with loss 0.5820
AttentionModel-training is done: 5/10
2020-10-31 | reset count: 0 | final loss: 0.5820 at epoch 2
making sample. (inference: True, date:2020-11-30 00:00:00, date_number: 407)
making sample. (inference: False, date:2020-10-31 00:00:00, date_number: 406)
[AttentionModel/2020-11-30 | get_logger | INFO]: ====================AttentionModel/2020-11-30 start====================
making sample. (inference: True, date:2020-12-31 00:00:00, date_number: 408)
making sample. (inference: False, date:2020-11-30 00:00:00, date_number: 407)
[AttentionModel/2020-12-31 | get_logger | INFO]: ====================AttentionModel/2020-12-31 start====================
[strategy_integration.py] set_seed(2020-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5836)training: 2/10 (0.5836)validation : 2/10 (0.5762)training: 3/10 (0.5762)validation : 3/10 (0.5762)training: 4/10 (0.5762)validation : 4/10 (0.5750)training: 5/10 (0.5750)validation : 5/10 (0.5750)training: 6/10 (0.5750)validation : 6/10 (0.5750)early stopping at 6 with loss 0.5750
AttentionModel-training is done: 6/10
2020-11-30 | reset count: 0 | final loss: 0.5750 at epoch 4
making sample. (inference: True, date:2021-01-31 00:00:00, date_number: 409)
making sample. (inference: False, date:2020-12-31 00:00:00, date_number: 408)
[AttentionModel/2021-01-31 | get_logger | INFO]: ====================AttentionModel/2021-01-31 start====================
[strategy_integration.py] set_seed(2020-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5754)training: 2/10 (0.5754)validation : 2/10 (0.5712)training: 3/10 (0.5712)validation : 3/10 (0.5712)training: 4/10 (0.5712)validation : 4/10 (0.5712)training: 5/10 (0.5712)validation : 5/10 (0.5695)training: 6/10 (0.5695)validation : 6/10 (0.5695)early stopping at 6 with loss 0.5695
AttentionModel-training is done: 6/10
2020-12-31 | reset count: 0 | final loss: 0.5695 at epoch 5
making sample. (inference: True, date:2021-02-28 00:00:00, date_number: 410)
making sample. (inference: False, date:2021-01-31 00:00:00, date_number: 409)
[AttentionModel/2021-02-28 | get_logger | INFO]: ====================AttentionModel/2021-02-28 start====================
[strategy_integration.py] set_seed(2021-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5586)training: 2/10 (0.5586)validation : 2/10 (0.5578)training: 3/10 (0.5578)validation : 3/10 (0.5578)training: 4/10 (0.5578)validation : 4/10 (0.5578)training: 5/10 (0.5578)validation : 5/10 (0.5578)early stopping at 5 with loss 0.5578
AttentionModel-training is done: 5/10
2021-01-31 | reset count: 0 | final loss: 0.5578 at epoch 2
making sample. (inference: True, date:2021-03-31 00:00:00, date_number: 411)
making sample. (inference: False, date:2021-02-28 00:00:00, date_number: 410)
[AttentionModel/2021-03-31 | get_logger | INFO]: ====================AttentionModel/2021-03-31 start====================
[strategy_integration.py] set_seed(2021-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5613)training: 2/10 (0.5613)validation : 2/10 (0.5613)training: 3/10 (0.5613)validation : 3/10 (0.5590)training: 4/10 (0.5590)validation : 4/10 (0.5590)training: 5/10 (0.5590)validation : 5/10 (0.5590)early stopping at 5 with loss 0.5590
AttentionModel-training is done: 5/10
2021-02-28 | reset count: 0 | final loss: 0.5590 at epoch 3
making sample. (inference: True, date:2021-04-30 00:00:00, date_number: 412)
making sample. (inference: False, date:2021-03-31 00:00:00, date_number: 411)
[AttentionModel/2021-04-30 | get_logger | INFO]: ====================AttentionModel/2021-04-30 start====================
[strategy_integration.py] set_seed(2021-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5720)training: 2/10 (0.5720)validation : 2/10 (0.5720)training: 3/10 (0.5720)validation : 3/10 (0.5720)training: 4/10 (0.5720)validation : 4/10 (0.5720)training: 5/10 (0.5720)validation : 5/10 (0.5698)training: 6/10 (0.5698)validation : 6/10 (0.5698)training: 7/10 (0.5698)validation : 7/10 (0.5698)training: 8/10 (0.5698)validation : 8/10 (0.5698)early stopping at 8 with loss 0.5698
AttentionModel-training is done: 8/10
2021-03-31 | reset count: 0 | final loss: 0.5698 at epoch 5
[strategy_integration.py] set_seed(2021-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5728)training: 2/10 (0.5728)validation : 2/10 (0.5655)training: 3/10 (0.5655)validation : 3/10 (0.5655)training: 4/10 (0.5655)validation : 4/10 (0.5655)training: 5/10 (0.5655)validation : 5/10 (0.5655)training: 6/10 (0.5655)validation : 6/10 (0.5655)early stopping at 6 with loss 0.5655
AttentionModel-training is done: 6/10
2021-04-30 | reset count: 0 | final loss: 0.5655 at epoch 2
making sample. (inference: True, date:2021-05-31 00:00:00, date_number: 413)
making sample. (inference: False, date:2021-04-30 00:00:00, date_number: 412)
[AttentionModel/2021-05-31 | get_logger | INFO]: ====================AttentionModel/2021-05-31 start====================
making sample. (inference: True, date:2021-06-30 00:00:00, date_number: 414)
making sample. (inference: False, date:2021-05-31 00:00:00, date_number: 413)
[AttentionModel/2021-06-30 | get_logger | INFO]: ====================AttentionModel/2021-06-30 start====================
[strategy_integration.py] set_seed(2021-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5741)training: 2/10 (0.5741)validation : 2/10 (0.5741)training: 3/10 (0.5741)validation : 3/10 (0.5705)training: 4/10 (0.5705)validation : 4/10 (0.5705)training: 5/10 (0.5705)validation : 5/10 (0.5705)training: 6/10 (0.5705)validation : 6/10 (0.5705)early stopping at 6 with loss 0.5705
AttentionModel-training is done: 6/10
2021-05-31 | reset count: 0 | final loss: 0.5705 at epoch 3
[strategy_integration.py] set_seed(2021-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5721)training: 2/10 (0.5721)validation : 2/10 (0.5710)training: 3/10 (0.5710)validation : 3/10 (0.5710)training: 4/10 (0.5710)validation : 4/10 (0.5675)training: 5/10 (0.5675)validation : 5/10 (0.5675)training: 6/10 (0.5675)validation : 6/10 (0.5652)training: 7/10 (0.5652)validation : 7/10 (0.5652)early stopping at 7 with loss 0.5652
AttentionModel-training is done: 7/10
2021-06-30 | reset count: 0 | final loss: 0.5652 at epoch 6
making sample. (inference: True, date:2021-07-31 00:00:00, date_number: 415)
making sample. (inference: False, date:2021-06-30 00:00:00, date_number: 414)
[AttentionModel/2021-07-31 | get_logger | INFO]: ====================AttentionModel/2021-07-31 start====================
making sample. (inference: True, date:2021-08-31 00:00:00, date_number: 416)
making sample. (inference: False, date:2021-07-31 00:00:00, date_number: 415)
[AttentionModel/2021-08-31 | get_logger | INFO]: ====================AttentionModel/2021-08-31 start====================
[strategy_integration.py] set_seed(2021-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5821)training: 2/10 (0.5821)validation : 2/10 (0.5770)training: 3/10 (0.5770)validation : 3/10 (0.5770)training: 4/10 (0.5770)validation : 4/10 (0.5770)training: 5/10 (0.5770)validation : 5/10 (0.5770)early stopping at 5 with loss 0.5770
AttentionModel-training is done: 5/10
2021-07-31 | reset count: 0 | final loss: 0.5770 at epoch 2
[strategy_integration.py] set_seed(2021-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5777)training: 2/10 (0.5777)validation : 2/10 (0.5717)training: 3/10 (0.5717)validation : 3/10 (0.5717)training: 4/10 (0.5717)validation : 4/10 (0.5717)training: 5/10 (0.5717)validation : 5/10 (0.5717)early stopping at 5 with loss 0.5717
AttentionModel-training is done: 5/10
2021-08-31 | reset count: 0 | final loss: 0.5717 at epoch 2
making sample. (inference: True, date:2021-09-30 00:00:00, date_number: 417)
making sample. (inference: False, date:2021-08-31 00:00:00, date_number: 416)
[AttentionModel/2021-09-30 | get_logger | INFO]: ====================AttentionModel/2021-09-30 start====================
making sample. (inference: True, date:2021-10-31 00:00:00, date_number: 418)
making sample. (inference: False, date:2021-09-30 00:00:00, date_number: 417)
[AttentionModel/2021-10-31 | get_logger | INFO]: ====================AttentionModel/2021-10-31 start====================
[strategy_integration.py] set_seed(2021-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5748)training: 2/10 (0.5748)validation : 2/10 (0.5748)training: 3/10 (0.5748)validation : 3/10 (0.5748)training: 4/10 (0.5748)validation : 4/10 (0.5748)training: 5/10 (0.5748)validation : 5/10 (0.5748)early stopping at 5 with loss 0.5748
AttentionModel-training is done: 5/10
2021-09-30 | reset count: 0 | final loss: 0.5748 at epoch 1
[strategy_integration.py] set_seed(2021-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5936)training: 2/10 (0.5936)validation : 2/10 (0.5922)training: 3/10 (0.5922)validation : 3/10 (0.5901)training: 4/10 (0.5901)validation : 4/10 (0.5884)training: 5/10 (0.5884)validation : 5/10 (0.5884)training: 6/10 (0.5884)validation : 6/10 (0.5884)early stopping at 6 with loss 0.5884
AttentionModel-training is done: 6/10
2021-10-31 | reset count: 0 | final loss: 0.5884 at epoch 4
[strategy | get_logger | INFO]: ====================strategy start====================
[strategy | _get_infer_dirs | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick188_test02/infer 이 strategy 대상에 추가됨
[strategy | _backtest | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick188_test02/infer is starting
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.155 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠹ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache⠦ [Compustat API] : Loading get_historic_universe from cache⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading get_historic_universe from cache done in 1.964 secs
⠧ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.266 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.065 secs
⠋ [Compustat API] : Loading get_monthly_total_return from cache✓ [Compustat API] : Loading get_monthly_total_return from cache done in 0.063 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.079 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.072 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.071 secs
⠋ [Compustat API] : Loading get_monthly_volume_data from cache✓ [Compustat API] : Loading get_monthly_volume_data from cache done in 0.081 secs
[strategy | _allocating | INFO]: [STRATEGY] allocating start
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_weight_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] performance.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] combined.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] weight.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] universe.csv is saved.
[comparison | get_logger | INFO]: ====================comparison start====================
[comparison | compare | INFO]: [5. COMPARISON] comparison is saved.
[comparison | compare | INFO]:                        Harvest_Green_concept001_pick188_test02_price_return  ...  SPCOMP_total_win
2016-01-31_2021-04-30                                            0.24552     ...          0.765625

[1 rows x 27 columns]
2015-12-31
2016-01-31
2016-02-29
2016-03-31
2016-04-30
2016-05-31
2016-06-30
2016-07-31
2016-08-31
2016-09-30
2016-10-31
2016-11-30
2016-12-31
2017-01-31
2017-02-28
2017-03-31
2017-04-30
2017-05-31
2017-06-30
2017-07-31
2017-08-31
2017-09-30
2017-10-31
2017-11-30
2017-12-31
2018-01-31
2018-02-28
2018-03-31
2018-04-30
2018-05-31
2018-06-30
2018-07-31
2018-08-31
2018-09-30
2018-10-31
2018-11-30
2018-12-31
2019-01-31
2019-02-28
2019-03-31
2019-04-30
2019-05-31
2019-06-30
2019-07-31
2019-08-31
2019-09-30
2019-10-31
2019-11-30
2019-12-31
2020-01-31
2020-02-29
2020-03-31
2020-04-30
2020-05-31
2020-06-30
2020-07-31
2020-08-31
2020-09-30
2020-10-31
2020-11-30
2020-12-31
2021-01-31
2021-02-28
2021-03-31
2021-04-30
2021-05-31
2021-06-30
2021-07-31
2021-08-31
2021-09-30
2021-10-31
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.056 secs
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.053 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.072 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.087 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.081 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.753 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.764 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.769 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.766 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.769 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.083 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.751 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.751 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.749 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.741 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.739 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.086 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.756 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.084 secs
⠋ [Compustat API] : Cleaning past days cache✓ [Compustat API] : Cleaning past days cache done in 0.001 secs
auto caching이 활성화 되었습니다. /home/sronly/sr-storage/kirin_cache/kirin_api_cache/2.19.17_2021-11-23에 캐쉬를 저장합니다.
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.148 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading masking data✓ [Compustat API] : Loading get_historic_universe from cache done in 2.029 secs
⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.33 secs
setting tensorflow random seed failed
load_data: op_lev
load_data: roe
load_data: std_u_e
load_data: retail_mfr
load_data: m1
load_data: m2
load_data: mom_12m_1m
load_data: ram_12m_0m
load_data: vol_3m
load_data: t3y
load_data: t5y
load_data: t7y
load_data: t1y_ff
load_data: t5y_ff
load_data: t10y_t2y
load_data: export_growth
load_data: import_growth
load_data: real_gig
load_data: res_mom_12m_1m_0m
load_data: res_vol_6m_3m_0m
load_data: at
load_data: ae_m
load_data: ia_ta
load_data: rc_a
load_data: aaa_t10y
load_data: baa_t10y
load_data: aaa_ff
load_data: real_pig
load_data: federal_tg
load_data: real_gdp
load_data: pr_1m_0m
load_data: mv
load_data: btm
load_data: snp500_pr
load_data: wilshire500_pr
load_data: ted
load_data: ppi
load_data: trimmed_pce
load_data: unemploy
load_data: snp500_vol
load_data: sector_values
load_data: mv
load_data: pr_1m_0m
load_data: mv
making sample. (inference: True, date:2015-12-31 00:00:00, date_number: 348)
making sample. (inference: False, date:2012-12-31 00:00:00, date_number: 312)
making sample. (inference: False, date:2013-01-31 00:00:00, date_number: 313)
making sample. (inference: False, date:2013-02-28 00:00:00, date_number: 314)
making sample. (inference: False, date:2013-03-31 00:00:00, date_number: 315)
making sample. (inference: False, date:2013-04-30 00:00:00, date_number: 316)
making sample. (inference: False, date:2013-05-31 00:00:00, date_number: 317)
making sample. (inference: False, date:2013-06-30 00:00:00, date_number: 318)
making sample. (inference: False, date:2013-07-31 00:00:00, date_number: 319)
making sample. (inference: False, date:2013-08-31 00:00:00, date_number: 320)
making sample. (inference: False, date:2013-09-30 00:00:00, date_number: 321)
making sample. (inference: False, date:2013-10-31 00:00:00, date_number: 322)
making sample. (inference: False, date:2013-11-30 00:00:00, date_number: 323)
making sample. (inference: False, date:2013-12-31 00:00:00, date_number: 324)
making sample. (inference: False, date:2014-01-31 00:00:00, date_number: 325)
making sample. (inference: False, date:2014-02-28 00:00:00, date_number: 326)
making sample. (inference: False, date:2014-03-31 00:00:00, date_number: 327)
making sample. (inference: False, date:2014-04-30 00:00:00, date_number: 328)
making sample. (inference: False, date:2014-05-31 00:00:00, date_number: 329)
making sample. (inference: False, date:2014-06-30 00:00:00, date_number: 330)
making sample. (inference: False, date:2014-07-31 00:00:00, date_number: 331)
making sample. (inference: False, date:2014-08-31 00:00:00, date_number: 332)
making sample. (inference: False, date:2014-09-30 00:00:00, date_number: 333)
making sample. (inference: False, date:2014-10-31 00:00:00, date_number: 334)
making sample. (inference: False, date:2014-11-30 00:00:00, date_number: 335)
making sample. (inference: False, date:2014-12-31 00:00:00, date_number: 336)
making sample. (inference: False, date:2015-01-31 00:00:00, date_number: 337)
making sample. (inference: False, date:2015-02-28 00:00:00, date_number: 338)
making sample. (inference: False, date:2015-03-31 00:00:00, date_number: 339)
making sample. (inference: False, date:2015-04-30 00:00:00, date_number: 340)
making sample. (inference: False, date:2015-05-31 00:00:00, date_number: 341)
making sample. (inference: False, date:2015-06-30 00:00:00, date_number: 342)
making sample. (inference: False, date:2015-07-31 00:00:00, date_number: 343)
making sample. (inference: False, date:2015-08-31 00:00:00, date_number: 344)
making sample. (inference: False, date:2015-09-30 00:00:00, date_number: 345)
making sample. (inference: False, date:2015-10-31 00:00:00, date_number: 346)
making sample. (inference: False, date:2015-11-30 00:00:00, date_number: 347)
[AttentionModel/2015-12-31 | get_logger | INFO]: ====================AttentionModel/2015-12-31 start====================
making sample. (inference: True, date:2016-01-31 00:00:00, date_number: 349)
making sample. (inference: False, date:2015-12-31 00:00:00, date_number: 348)
[AttentionModel/2016-01-31 | get_logger | INFO]: ====================AttentionModel/2016-01-31 start====================
[strategy_integration.py] set_seed(2015-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5683)training: 2/10 (0.5683)validation : 2/10 (0.5660)training: 3/10 (0.5660)validation : 3/10 (0.5660)training: 4/10 (0.5660)validation : 4/10 (0.5637)training: 5/10 (0.5637)validation : 5/10 (0.5635)training: 6/10 (0.5635)validation : 6/10 (0.5635)training: 7/10 (0.5635)validation : 7/10 (0.5635)early stopping at 7 with loss 0.5635
AttentionModel-training is done: 7/10
2015-12-31 | reset count: 0 | final loss: 0.5635 at epoch 5
[strategy_integration.py] set_seed(2016-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5914)training: 2/10 (0.5914)validation : 2/10 (0.5894)training: 3/10 (0.5894)validation : 3/10 (0.5894)training: 4/10 (0.5894)validation : 4/10 (0.5877)training: 5/10 (0.5877)validation : 5/10 (0.5877)training: 6/10 (0.5877)validation : 6/10 (0.5877)early stopping at 6 with loss 0.5877
AttentionModel-training is done: 6/10
2016-01-31 | reset count: 0 | final loss: 0.5877 at epoch 4
making sample. (inference: True, date:2016-02-29 00:00:00, date_number: 350)
making sample. (inference: False, date:2016-01-31 00:00:00, date_number: 349)
[AttentionModel/2016-02-29 | get_logger | INFO]: ====================AttentionModel/2016-02-29 start====================
making sample. (inference: True, date:2016-03-31 00:00:00, date_number: 351)
making sample. (inference: False, date:2016-02-29 00:00:00, date_number: 350)
[AttentionModel/2016-03-31 | get_logger | INFO]: ====================AttentionModel/2016-03-31 start====================
[strategy_integration.py] set_seed(2016-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5813)training: 2/10 (0.5813)validation : 2/10 (0.5803)training: 3/10 (0.5803)validation : 3/10 (0.5786)training: 4/10 (0.5786)validation : 4/10 (0.5786)training: 5/10 (0.5786)validation : 5/10 (0.5767)training: 6/10 (0.5767)validation : 6/10 (0.5767)early stopping at 6 with loss 0.5767
AttentionModel-training is done: 6/10
2016-02-29 | reset count: 0 | final loss: 0.5767 at epoch 5
making sample. (inference: True, date:2016-04-30 00:00:00, date_number: 352)
making sample. (inference: False, date:2016-03-31 00:00:00, date_number: 351)
[AttentionModel/2016-04-30 | get_logger | INFO]: ====================AttentionModel/2016-04-30 start====================
[strategy_integration.py] set_seed(2016-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5801)training: 2/10 (0.5801)validation : 2/10 (0.5801)training: 3/10 (0.5801)validation : 3/10 (0.5754)training: 4/10 (0.5754)validation : 4/10 (0.5754)training: 5/10 (0.5754)validation : 5/10 (0.5754)training: 6/10 (0.5754)validation : 6/10 (0.5745)training: 7/10 (0.5745)validation : 7/10 (0.5745)training: 8/10 (0.5745)validation : 8/10 (0.5745)training: 9/10 (0.5745)validation : 9/10 (0.5745)early stopping at 9 with loss 0.5745
AttentionModel-training is done: 9/10
2016-03-31 | reset count: 0 | final loss: 0.5745 at epoch 6
making sample. (inference: True, date:2016-05-31 00:00:00, date_number: 353)
making sample. (inference: False, date:2016-04-30 00:00:00, date_number: 352)
[AttentionModel/2016-05-31 | get_logger | INFO]: ====================AttentionModel/2016-05-31 start====================
[strategy_integration.py] set_seed(2016-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5741)training: 2/10 (0.5741)validation : 2/10 (0.5741)training: 3/10 (0.5741)validation : 3/10 (0.5728)training: 4/10 (0.5728)validation : 4/10 (0.5692)training: 5/10 (0.5692)validation : 5/10 (0.5692)training: 6/10 (0.5692)validation : 6/10 (0.5692)training: 7/10 (0.5692)validation : 7/10 (0.5692)early stopping at 7 with loss 0.5692
AttentionModel-training is done: 7/10
2016-04-30 | reset count: 0 | final loss: 0.5692 at epoch 4
making sample. (inference: True, date:2016-06-30 00:00:00, date_number: 354)
making sample. (inference: False, date:2016-05-31 00:00:00, date_number: 353)
[strategy_integration.py] set_seed(2016-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5723)training: 2/10 (0.5723)validation : 2/10 (0.5648)training: 3/10 (0.5648)validation : 3/10 (0.5648)training: 4/10 (0.5648)validation : 4/10 (0.5648)training: 5/10 (0.5648)validation : 5/10 (0.5648)early stopping at 5 with loss 0.5648
AttentionModel-training is done: 5/10
2016-05-31 | reset count: 0 | final loss: 0.5648 at epoch 2
[AttentionModel/2016-06-30 | get_logger | INFO]: ====================AttentionModel/2016-06-30 start====================
making sample. (inference: True, date:2016-07-31 00:00:00, date_number: 355)
making sample. (inference: False, date:2016-06-30 00:00:00, date_number: 354)
[AttentionModel/2016-07-31 | get_logger | INFO]: ====================AttentionModel/2016-07-31 start====================
[strategy_integration.py] set_seed(2016-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5851)training: 2/10 (0.5851)validation : 2/10 (0.5761)training: 3/10 (0.5761)validation : 3/10 (0.5761)training: 4/10 (0.5761)validation : 4/10 (0.5761)training: 5/10 (0.5761)validation : 5/10 (0.5761)training: 6/10 (0.5761)validation : 6/10 (0.5759)training: 7/10 (0.5759)validation : 7/10 (0.5759)training: 8/10 (0.5759)validation : 8/10 (0.5759)early stopping at 8 with loss 0.5759
AttentionModel-training is done: 8/10
2016-06-30 | reset count: 0 | final loss: 0.5759 at epoch 6
[strategy_integration.py] set_seed(2016-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5707)training: 2/10 (0.5707)validation : 2/10 (0.5707)training: 3/10 (0.5707)validation : 3/10 (0.5707)training: 4/10 (0.5707)validation : 4/10 (0.5707)training: 5/10 (0.5707)validation : 5/10 (0.5707)early stopping at 5 with loss 0.5707
AttentionModel-training is done: 5/10
2016-07-31 | reset count: 0 | final loss: 0.5707 at epoch 1
making sample. (inference: True, date:2016-08-31 00:00:00, date_number: 356)
making sample. (inference: False, date:2016-07-31 00:00:00, date_number: 355)
[AttentionModel/2016-08-31 | get_logger | INFO]: ====================AttentionModel/2016-08-31 start====================
making sample. (inference: True, date:2016-09-30 00:00:00, date_number: 357)
making sample. (inference: False, date:2016-08-31 00:00:00, date_number: 356)
[AttentionModel/2016-09-30 | get_logger | INFO]: ====================AttentionModel/2016-09-30 start====================
[strategy_integration.py] set_seed(2016-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5614)training: 2/10 (0.5614)validation : 2/10 (0.5596)training: 3/10 (0.5596)validation : 3/10 (0.5596)training: 4/10 (0.5596)validation : 4/10 (0.5596)training: 5/10 (0.5596)validation : 5/10 (0.5596)training: 6/10 (0.5596)validation : 6/10 (0.5590)training: 7/10 (0.5590)validation : 7/10 (0.5590)early stopping at 7 with loss 0.5590
AttentionModel-training is done: 7/10
2016-08-31 | reset count: 0 | final loss: 0.5590 at epoch 6
[strategy_integration.py] set_seed(2016-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5651)training: 2/10 (0.5651)validation : 2/10 (0.5651)training: 3/10 (0.5651)validation : 3/10 (0.5651)training: 4/10 (0.5651)validation : 4/10 (0.5651)training: 5/10 (0.5651)validation : 5/10 (0.5651)early stopping at 5 with loss 0.5651
AttentionModel-training is done: 5/10
2016-09-30 | reset count: 0 | final loss: 0.5651 at epoch 1
making sample. (inference: True, date:2016-10-31 00:00:00, date_number: 358)
making sample. (inference: False, date:2016-09-30 00:00:00, date_number: 357)
[AttentionModel/2016-10-31 | get_logger | INFO]: ====================AttentionModel/2016-10-31 start====================
making sample. (inference: True, date:2016-11-30 00:00:00, date_number: 359)
making sample. (inference: False, date:2016-10-31 00:00:00, date_number: 358)
[AttentionModel/2016-11-30 | get_logger | INFO]: ====================AttentionModel/2016-11-30 start====================
[strategy_integration.py] set_seed(2016-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5768)training: 2/10 (0.5768)validation : 2/10 (0.5740)training: 3/10 (0.5740)validation : 3/10 (0.5729)training: 4/10 (0.5729)validation : 4/10 (0.5702)training: 5/10 (0.5702)validation : 5/10 (0.5702)training: 6/10 (0.5702)validation : 6/10 (0.5702)early stopping at 6 with loss 0.5702
AttentionModel-training is done: 6/10
2016-10-31 | reset count: 0 | final loss: 0.5702 at epoch 4
making sample. (inference: True, date:2016-12-31 00:00:00, date_number: 360)
making sample. (inference: False, date:2016-11-30 00:00:00, date_number: 359)
[AttentionModel/2016-12-31 | get_logger | INFO]: ====================AttentionModel/2016-12-31 start====================
[strategy_integration.py] set_seed(2016-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5709)training: 2/10 (0.5709)validation : 2/10 (0.5698)training: 3/10 (0.5698)validation : 3/10 (0.5694)training: 4/10 (0.5694)validation : 4/10 (0.5686)training: 5/10 (0.5686)validation : 5/10 (0.5686)training: 6/10 (0.5686)validation : 6/10 (0.5679)training: 7/10 (0.5679)validation : 7/10 (0.5665)training: 8/10 (0.5665)validation : 8/10 (0.5665)training: 9/10 (0.5665)validation : 9/10 (0.5665)training: 10/10 (0.5665)validation : 10/10 (0.5665)early stopping at 10 with loss 0.5665
AttentionModel-training is done: 10/10
2016-11-30 | reset count: 0 | final loss: 0.5665 at epoch 7
making sample. (inference: True, date:2017-01-31 00:00:00, date_number: 361)
making sample. (inference: False, date:2016-12-31 00:00:00, date_number: 360)
[strategy_integration.py] set_seed(2016-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5636)training: 2/10 (0.5636)validation : 2/10 (0.5617)training: 3/10 (0.5617)validation : 3/10 (0.5614)training: 4/10 (0.5614)validation : 4/10 (0.5614)training: 5/10 (0.5614)validation : 5/10 (0.5614)training: 6/10 (0.5614)validation : 6/10 (0.5614)early stopping at 6 with loss 0.5614
AttentionModel-training is done: 6/10
2016-12-31 | reset count: 0 | final loss: 0.5614 at epoch 3
[AttentionModel/2017-01-31 | get_logger | INFO]: ====================AttentionModel/2017-01-31 start====================
making sample. (inference: True, date:2017-02-28 00:00:00, date_number: 362)
making sample. (inference: False, date:2017-01-31 00:00:00, date_number: 361)
[AttentionModel/2017-02-28 | get_logger | INFO]: ====================AttentionModel/2017-02-28 start====================
[strategy_integration.py] set_seed(2017-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5667)training: 2/10 (0.5667)validation : 2/10 (0.5600)training: 3/10 (0.5600)validation : 3/10 (0.5600)training: 4/10 (0.5600)validation : 4/10 (0.5600)training: 5/10 (0.5600)validation : 5/10 (0.5600)training: 6/10 (0.5600)validation : 6/10 (0.5600)training: 7/10 (0.5600)validation : 7/10 (0.5600)early stopping at 7 with loss 0.5600
AttentionModel-training is done: 7/10
2017-01-31 | reset count: 0 | final loss: 0.5600 at epoch 2
[strategy_integration.py] set_seed(2017-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5668)training: 2/10 (0.5668)validation : 2/10 (0.5668)training: 3/10 (0.5668)validation : 3/10 (0.5643)training: 4/10 (0.5643)validation : 4/10 (0.5643)training: 5/10 (0.5643)validation : 5/10 (0.5643)early stopping at 5 with loss 0.5643
AttentionModel-training is done: 5/10
2017-02-28 | reset count: 0 | final loss: 0.5643 at epoch 3
making sample. (inference: True, date:2017-03-31 00:00:00, date_number: 363)
making sample. (inference: False, date:2017-02-28 00:00:00, date_number: 362)
[AttentionModel/2017-03-31 | get_logger | INFO]: ====================AttentionModel/2017-03-31 start====================
making sample. (inference: True, date:2017-04-30 00:00:00, date_number: 364)
making sample. (inference: False, date:2017-03-31 00:00:00, date_number: 363)
[AttentionModel/2017-04-30 | get_logger | INFO]: ====================AttentionModel/2017-04-30 start====================
[strategy_integration.py] set_seed(2017-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5660)training: 2/10 (0.5660)validation : 2/10 (0.5589)training: 3/10 (0.5589)validation : 3/10 (0.5588)training: 4/10 (0.5588)validation : 4/10 (0.5588)training: 5/10 (0.5588)validation : 5/10 (0.5588)early stopping at 5 with loss 0.5588
AttentionModel-training is done: 5/10
2017-03-31 | reset count: 0 | final loss: 0.5588 at epoch 3
making sample. (inference: True, date:2017-05-31 00:00:00, date_number: 365)
making sample. (inference: False, date:2017-04-30 00:00:00, date_number: 364)
[AttentionModel/2017-05-31 | get_logger | INFO]: ====================AttentionModel/2017-05-31 start====================
[strategy_integration.py] set_seed(2017-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5648)training: 2/10 (0.5648)validation : 2/10 (0.5632)training: 3/10 (0.5632)validation : 3/10 (0.5612)training: 4/10 (0.5612)validation : 4/10 (0.5612)training: 5/10 (0.5612)validation : 5/10 (0.5602)training: 6/10 (0.5602)validation : 6/10 (0.5602)early stopping at 6 with loss 0.5602
AttentionModel-training is done: 6/10
2017-04-30 | reset count: 0 | final loss: 0.5602 at epoch 5
[strategy_integration.py] set_seed(2017-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5631)training: 2/10 (0.5631)validation : 2/10 (0.5631)training: 3/10 (0.5631)validation : 3/10 (0.5601)training: 4/10 (0.5601)validation : 4/10 (0.5601)training: 5/10 (0.5601)validation : 5/10 (0.5601)early stopping at 5 with loss 0.5601
AttentionModel-training is done: 5/10
2017-05-31 | reset count: 0 | final loss: 0.5601 at epoch 3
making sample. (inference: True, date:2017-06-30 00:00:00, date_number: 366)
making sample. (inference: False, date:2017-05-31 00:00:00, date_number: 365)
[AttentionModel/2017-06-30 | get_logger | INFO]: ====================AttentionModel/2017-06-30 start====================
making sample. (inference: True, date:2017-07-31 00:00:00, date_number: 367)
making sample. (inference: False, date:2017-06-30 00:00:00, date_number: 366)
[AttentionModel/2017-07-31 | get_logger | INFO]: ====================AttentionModel/2017-07-31 start====================
[strategy_integration.py] set_seed(2017-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5690)training: 2/10 (0.5690)validation : 2/10 (0.5662)training: 3/10 (0.5662)validation : 3/10 (0.5636)training: 4/10 (0.5636)validation : 4/10 (0.5636)training: 5/10 (0.5636)validation : 5/10 (0.5636)training: 6/10 (0.5636)validation : 6/10 (0.5636)early stopping at 6 with loss 0.5636
AttentionModel-training is done: 6/10
2017-06-30 | reset count: 0 | final loss: 0.5636 at epoch 3
[strategy_integration.py] set_seed(2017-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5661)training: 2/10 (0.5661)validation : 2/10 (0.5638)training: 3/10 (0.5638)validation : 3/10 (0.5638)training: 4/10 (0.5638)validation : 4/10 (0.5601)training: 5/10 (0.5601)validation : 5/10 (0.5601)training: 6/10 (0.5601)validation : 6/10 (0.5601)early stopping at 6 with loss 0.5601
AttentionModel-training is done: 6/10
2017-07-31 | reset count: 0 | final loss: 0.5601 at epoch 4
making sample. (inference: True, date:2017-08-31 00:00:00, date_number: 368)
making sample. (inference: False, date:2017-07-31 00:00:00, date_number: 367)
[AttentionModel/2017-08-31 | get_logger | INFO]: ====================AttentionModel/2017-08-31 start====================
making sample. (inference: True, date:2017-09-30 00:00:00, date_number: 369)
making sample. (inference: False, date:2017-08-31 00:00:00, date_number: 368)
[AttentionModel/2017-09-30 | get_logger | INFO]: ====================AttentionModel/2017-09-30 start====================
[strategy_integration.py] set_seed(2017-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5641)training: 2/10 (0.5641)validation : 2/10 (0.5629)training: 3/10 (0.5629)validation : 3/10 (0.5601)training: 4/10 (0.5601)validation : 4/10 (0.5601)training: 5/10 (0.5601)validation : 5/10 (0.5601)early stopping at 5 with loss 0.5601
AttentionModel-training is done: 5/10
2017-08-31 | reset count: 0 | final loss: 0.5601 at epoch 3
making sample. (inference: True, date:2017-10-31 00:00:00, date_number: 370)
making sample. (inference: False, date:2017-09-30 00:00:00, date_number: 369)
[AttentionModel/2017-10-31 | get_logger | INFO]: ====================AttentionModel/2017-10-31 start====================
[strategy_integration.py] set_seed(2017-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5698)training: 2/10 (0.5698)validation : 2/10 (0.5698)training: 3/10 (0.5698)validation : 3/10 (0.5687)training: 4/10 (0.5687)validation : 4/10 (0.5687)training: 5/10 (0.5687)validation : 5/10 (0.5664)training: 6/10 (0.5664)validation : 6/10 (0.5656)training: 7/10 (0.5656)validation : 7/10 (0.5656)early stopping at 7 with loss 0.5656
AttentionModel-training is done: 7/10
2017-09-30 | reset count: 0 | final loss: 0.5656 at epoch 6
making sample. (inference: True, date:2017-11-30 00:00:00, date_number: 371)
making sample. (inference: False, date:2017-10-31 00:00:00, date_number: 370)
[AttentionModel/2017-11-30 | get_logger | INFO]: ====================AttentionModel/2017-11-30 start====================
[strategy_integration.py] set_seed(2017-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5695)training: 2/10 (0.5695)validation : 2/10 (0.5604)training: 3/10 (0.5604)validation : 3/10 (0.5604)training: 4/10 (0.5604)validation : 4/10 (0.5595)training: 5/10 (0.5595)validation : 5/10 (0.5595)training: 6/10 (0.5595)validation : 6/10 (0.5595)training: 7/10 (0.5595)validation : 7/10 (0.5595)early stopping at 7 with loss 0.5595
AttentionModel-training is done: 7/10
2017-10-31 | reset count: 0 | final loss: 0.5595 at epoch 4
making sample. (inference: True, date:2017-12-31 00:00:00, date_number: 372)
making sample. (inference: False, date:2017-11-30 00:00:00, date_number: 371)
[AttentionModel/2017-12-31 | get_logger | INFO]: ====================AttentionModel/2017-12-31 start====================
[strategy_integration.py] set_seed(2017-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5625)training: 2/10 (0.5625)validation : 2/10 (0.5625)training: 3/10 (0.5625)validation : 3/10 (0.5614)training: 4/10 (0.5614)validation : 4/10 (0.5611)training: 5/10 (0.5611)validation : 5/10 (0.5611)training: 6/10 (0.5611)validation : 6/10 (0.5611)early stopping at 6 with loss 0.5611
AttentionModel-training is done: 6/10
2017-11-30 | reset count: 0 | final loss: 0.5611 at epoch 4
making sample. (inference: True, date:2018-01-31 00:00:00, date_number: 373)
making sample. (inference: False, date:2017-12-31 00:00:00, date_number: 372)
[AttentionModel/2018-01-31 | get_logger | INFO]: ====================AttentionModel/2018-01-31 start====================
[strategy_integration.py] set_seed(2017-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5653)training: 2/10 (0.5653)validation : 2/10 (0.5653)training: 3/10 (0.5653)validation : 3/10 (0.5653)training: 4/10 (0.5653)validation : 4/10 (0.5619)training: 5/10 (0.5619)validation : 5/10 (0.5599)training: 6/10 (0.5599)validation : 6/10 (0.5599)training: 7/10 (0.5599)validation : 7/10 (0.5599)early stopping at 7 with loss 0.5599
AttentionModel-training is done: 7/10
2017-12-31 | reset count: 0 | final loss: 0.5599 at epoch 5
[strategy_integration.py] set_seed(2018-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5745)training: 2/10 (0.5745)validation : 2/10 (0.5735)training: 3/10 (0.5735)validation : 3/10 (0.5735)training: 4/10 (0.5735)validation : 4/10 (0.5735)training: 5/10 (0.5735)validation : 5/10 (0.5709)training: 6/10 (0.5709)validation : 6/10 (0.5709)early stopping at 6 with loss 0.5709
AttentionModel-training is done: 6/10
2018-01-31 | reset count: 0 | final loss: 0.5709 at epoch 5
making sample. (inference: True, date:2018-02-28 00:00:00, date_number: 374)
making sample. (inference: False, date:2018-01-31 00:00:00, date_number: 373)
[AttentionModel/2018-02-28 | get_logger | INFO]: ====================AttentionModel/2018-02-28 start====================
making sample. (inference: True, date:2018-03-31 00:00:00, date_number: 375)
making sample. (inference: False, date:2018-02-28 00:00:00, date_number: 374)
[AttentionModel/2018-03-31 | get_logger | INFO]: ====================AttentionModel/2018-03-31 start====================
[strategy_integration.py] set_seed(2018-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5772)training: 2/10 (0.5772)validation : 2/10 (0.5772)training: 3/10 (0.5772)validation : 3/10 (0.5772)training: 4/10 (0.5772)validation : 4/10 (0.5746)training: 5/10 (0.5746)validation : 5/10 (0.5746)training: 6/10 (0.5746)validation : 6/10 (0.5737)training: 7/10 (0.5737)validation : 7/10 (0.5712)training: 8/10 (0.5712)validation : 8/10 (0.5712)training: 9/10 (0.5712)validation : 9/10 (0.5712)early stopping at 9 with loss 0.5712
AttentionModel-training is done: 9/10
2018-02-28 | reset count: 0 | final loss: 0.5712 at epoch 7
[strategy_integration.py] set_seed(2018-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5694)training: 2/10 (0.5694)validation : 2/10 (0.5694)training: 3/10 (0.5694)validation : 3/10 (0.5694)training: 4/10 (0.5694)validation : 4/10 (0.5694)training: 5/10 (0.5694)validation : 5/10 (0.5670)training: 6/10 (0.5670)validation : 6/10 (0.5670)training: 7/10 (0.5670)validation : 7/10 (0.5670)training: 8/10 (0.5670)validation : 8/10 (0.5670)early stopping at 8 with loss 0.5670
AttentionModel-training is done: 8/10
2018-03-31 | reset count: 0 | final loss: 0.5670 at epoch 5
making sample. (inference: True, date:2018-04-30 00:00:00, date_number: 376)
making sample. (inference: False, date:2018-03-31 00:00:00, date_number: 375)
[AttentionModel/2018-04-30 | get_logger | INFO]: ====================AttentionModel/2018-04-30 start====================
making sample. (inference: True, date:2018-05-31 00:00:00, date_number: 377)
making sample. (inference: False, date:2018-04-30 00:00:00, date_number: 376)
[AttentionModel/2018-05-31 | get_logger | INFO]: ====================AttentionModel/2018-05-31 start====================
[strategy_integration.py] set_seed(2018-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5742)training: 2/10 (0.5742)validation : 2/10 (0.5742)training: 3/10 (0.5742)validation : 3/10 (0.5741)training: 4/10 (0.5741)validation : 4/10 (0.5731)training: 5/10 (0.5731)validation : 5/10 (0.5731)training: 6/10 (0.5731)validation : 6/10 (0.5721)training: 7/10 (0.5721)validation : 7/10 (0.5721)training: 8/10 (0.5721)validation : 8/10 (0.5721)early stopping at 8 with loss 0.5721
AttentionModel-training is done: 8/10
2018-04-30 | reset count: 0 | final loss: 0.5721 at epoch 6
[strategy_integration.py] set_seed(2018-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5742)training: 2/10 (0.5742)validation : 2/10 (0.5723)training: 3/10 (0.5723)validation : 3/10 (0.5717)training: 4/10 (0.5717)validation : 4/10 (0.5717)training: 5/10 (0.5717)validation : 5/10 (0.5712)early stopping at 5 with loss 0.5712
AttentionModel-training is done: 5/10
2018-05-31 | reset count: 0 | final loss: 0.5712 at epoch 5
making sample. (inference: True, date:2018-06-30 00:00:00, date_number: 378)
making sample. (inference: False, date:2018-05-31 00:00:00, date_number: 377)
[AttentionModel/2018-06-30 | get_logger | INFO]: ====================AttentionModel/2018-06-30 start====================
making sample. (inference: True, date:2018-07-31 00:00:00, date_number: 379)
making sample. (inference: False, date:2018-06-30 00:00:00, date_number: 378)
[AttentionModel/2018-07-31 | get_logger | INFO]: ====================AttentionModel/2018-07-31 start====================
[strategy_integration.py] set_seed(2018-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5625)training: 2/10 (0.5625)validation : 2/10 (0.5625)training: 3/10 (0.5625)validation : 3/10 (0.5603)training: 4/10 (0.5603)validation : 4/10 (0.5603)training: 5/10 (0.5603)validation : 5/10 (0.5566)training: 6/10 (0.5566)validation : 6/10 (0.5548)training: 7/10 (0.5548)validation : 7/10 (0.5548)training: 8/10 (0.5548)validation : 8/10 (0.5548)early stopping at 8 with loss 0.5548
AttentionModel-training is done: 8/10
2018-06-30 | reset count: 0 | final loss: 0.5548 at epoch 6
[strategy_integration.py] set_seed(2018-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5733)training: 2/10 (0.5733)validation : 2/10 (0.5733)training: 3/10 (0.5733)validation : 3/10 (0.5722)training: 4/10 (0.5722)validation : 4/10 (0.5698)training: 5/10 (0.5698)validation : 5/10 (0.5698)training: 6/10 (0.5698)validation : 6/10 (0.5698)early stopping at 6 with loss 0.5698
AttentionModel-training is done: 6/10
2018-07-31 | reset count: 0 | final loss: 0.5698 at epoch 4
making sample. (inference: True, date:2018-08-31 00:00:00, date_number: 380)
making sample. (inference: False, date:2018-07-31 00:00:00, date_number: 379)
[AttentionModel/2018-08-31 | get_logger | INFO]: ====================AttentionModel/2018-08-31 start====================
making sample. (inference: True, date:2018-09-30 00:00:00, date_number: 381)
making sample. (inference: False, date:2018-08-31 00:00:00, date_number: 380)
[AttentionModel/2018-09-30 | get_logger | INFO]: ====================AttentionModel/2018-09-30 start====================
[strategy_integration.py] set_seed(2018-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5710)training: 2/10 (0.5710)validation : 2/10 (0.5660)training: 3/10 (0.5660)validation : 3/10 (0.5660)training: 4/10 (0.5660)validation : 4/10 (0.5660)training: 5/10 (0.5660)validation : 5/10 (0.5660)early stopping at 5 with loss 0.5660
AttentionModel-training is done: 5/10
2018-08-31 | reset count: 0 | final loss: 0.5660 at epoch 2
making sample. (inference: True, date:2018-10-31 00:00:00, date_number: 382)
making sample. (inference: False, date:2018-09-30 00:00:00, date_number: 381)
[AttentionModel/2018-10-31 | get_logger | INFO]: ====================AttentionModel/2018-10-31 start====================
[strategy_integration.py] set_seed(2018-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5718)training: 2/10 (0.5718)validation : 2/10 (0.5718)training: 3/10 (0.5718)validation : 3/10 (0.5714)training: 4/10 (0.5714)validation : 4/10 (0.5704)training: 5/10 (0.5704)validation : 5/10 (0.5704)training: 6/10 (0.5704)validation : 6/10 (0.5704)early stopping at 6 with loss 0.5704
AttentionModel-training is done: 6/10
2018-09-30 | reset count: 0 | final loss: 0.5704 at epoch 4
[strategy_integration.py] set_seed(2018-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5819)training: 2/10 (0.5819)validation : 2/10 (0.5802)training: 3/10 (0.5802)validation : 3/10 (0.5802)training: 4/10 (0.5802)validation : 4/10 (0.5802)training: 5/10 (0.5802)validation : 5/10 (0.5802)early stopping at 5 with loss 0.5802
AttentionModel-training is done: 5/10
2018-10-31 | reset count: 0 | final loss: 0.5802 at epoch 2
making sample. (inference: True, date:2018-11-30 00:00:00, date_number: 383)
making sample. (inference: False, date:2018-10-31 00:00:00, date_number: 382)
[AttentionModel/2018-11-30 | get_logger | INFO]: ====================AttentionModel/2018-11-30 start====================
making sample. (inference: True, date:2018-12-31 00:00:00, date_number: 384)
making sample. (inference: False, date:2018-11-30 00:00:00, date_number: 383)
[AttentionModel/2018-12-31 | get_logger | INFO]: ====================AttentionModel/2018-12-31 start====================
[strategy_integration.py] set_seed(2018-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5709)training: 2/10 (0.5709)validation : 2/10 (0.5692)training: 3/10 (0.5692)validation : 3/10 (0.5677)training: 4/10 (0.5677)validation : 4/10 (0.5677)training: 5/10 (0.5677)validation : 5/10 (0.5677)training: 6/10 (0.5677)validation : 6/10 (0.5677)early stopping at 6 with loss 0.5677
AttentionModel-training is done: 6/10
2018-11-30 | reset count: 0 | final loss: 0.5677 at epoch 3
making sample. (inference: True, date:2019-01-31 00:00:00, date_number: 385)
making sample. (inference: False, date:2018-12-31 00:00:00, date_number: 384)
[AttentionModel/2019-01-31 | get_logger | INFO]: ====================AttentionModel/2019-01-31 start====================
[strategy_integration.py] set_seed(2018-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5685)training: 2/10 (0.5685)validation : 2/10 (0.5685)training: 3/10 (0.5685)validation : 3/10 (0.5685)training: 4/10 (0.5685)validation : 4/10 (0.5685)training: 5/10 (0.5685)validation : 5/10 (0.5670)training: 6/10 (0.5670)validation : 6/10 (0.5663)training: 7/10 (0.5663)validation : 7/10 (0.5663)early stopping at 7 with loss 0.5663
AttentionModel-training is done: 7/10
2018-12-31 | reset count: 0 | final loss: 0.5663 at epoch 6
making sample. (inference: True, date:2019-02-28 00:00:00, date_number: 386)
making sample. (inference: False, date:2019-01-31 00:00:00, date_number: 385)
[AttentionModel/2019-02-28 | get_logger | INFO]: ====================AttentionModel/2019-02-28 start====================
[strategy_integration.py] set_seed(2019-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5807)training: 2/10 (0.5807)validation : 2/10 (0.5774)training: 3/10 (0.5774)validation : 3/10 (0.5774)training: 4/10 (0.5774)validation : 4/10 (0.5766)training: 5/10 (0.5766)validation : 5/10 (0.5766)training: 6/10 (0.5766)validation : 6/10 (0.5766)training: 7/10 (0.5766)validation : 7/10 (0.5766)early stopping at 7 with loss 0.5766
AttentionModel-training is done: 7/10
2019-01-31 | reset count: 0 | final loss: 0.5766 at epoch 4
[strategy_integration.py] set_seed(2019-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5869)training: 2/10 (0.5869)validation : 2/10 (0.5862)training: 3/10 (0.5862)validation : 3/10 (0.5829)training: 4/10 (0.5829)validation : 4/10 (0.5829)training: 5/10 (0.5829)validation : 5/10 (0.5827)training: 6/10 (0.5827)validation : 6/10 (0.5827)early stopping at 6 with loss 0.5827
AttentionModel-training is done: 6/10
2019-02-28 | reset count: 0 | final loss: 0.5827 at epoch 5
making sample. (inference: True, date:2019-03-31 00:00:00, date_number: 387)
making sample. (inference: False, date:2019-02-28 00:00:00, date_number: 386)
[AttentionModel/2019-03-31 | get_logger | INFO]: ====================AttentionModel/2019-03-31 start====================
making sample. (inference: True, date:2019-04-30 00:00:00, date_number: 388)
making sample. (inference: False, date:2019-03-31 00:00:00, date_number: 387)
[AttentionModel/2019-04-30 | get_logger | INFO]: ====================AttentionModel/2019-04-30 start====================
[strategy_integration.py] set_seed(2019-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5933)training: 2/10 (0.5933)validation : 2/10 (0.5933)training: 3/10 (0.5933)validation : 3/10 (0.5872)training: 4/10 (0.5872)validation : 4/10 (0.5872)training: 5/10 (0.5872)validation : 5/10 (0.5872)training: 6/10 (0.5872)validation : 6/10 (0.5872)early stopping at 6 with loss 0.5872
AttentionModel-training is done: 6/10
2019-03-31 | reset count: 0 | final loss: 0.5872 at epoch 3
[strategy_integration.py] set_seed(2019-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5791)training: 2/10 (0.5791)validation : 2/10 (0.5791)training: 3/10 (0.5791)validation : 3/10 (0.5777)training: 4/10 (0.5777)validation : 4/10 (0.5777)training: 5/10 (0.5777)validation : 5/10 (0.5777)early stopping at 5 with loss 0.5777
AttentionModel-training is done: 5/10
2019-04-30 | reset count: 0 | final loss: 0.5777 at epoch 3
making sample. (inference: True, date:2019-05-31 00:00:00, date_number: 389)
making sample. (inference: False, date:2019-04-30 00:00:00, date_number: 388)
[AttentionModel/2019-05-31 | get_logger | INFO]: ====================AttentionModel/2019-05-31 start====================
making sample. (inference: True, date:2019-06-30 00:00:00, date_number: 390)
making sample. (inference: False, date:2019-05-31 00:00:00, date_number: 389)
[AttentionModel/2019-06-30 | get_logger | INFO]: ====================AttentionModel/2019-06-30 start====================
[strategy_integration.py] set_seed(2019-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5899)training: 2/10 (0.5899)validation : 2/10 (0.5831)training: 3/10 (0.5831)validation : 3/10 (0.5831)training: 4/10 (0.5831)validation : 4/10 (0.5831)training: 5/10 (0.5831)validation : 5/10 (0.5831)training: 6/10 (0.5831)validation : 6/10 (0.5831)training: 7/10 (0.5831)validation : 7/10 (0.5831)training: 8/10 (0.5831)validation : 8/10 (0.5831)early stopping at 8 with loss 0.5831
AttentionModel-training is done: 8/10
2019-05-31 | reset count: 0 | final loss: 0.5831 at epoch 2
[strategy_integration.py] set_seed(2019-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5758)training: 2/10 (0.5758)validation : 2/10 (0.5758)training: 3/10 (0.5758)validation : 3/10 (0.5758)training: 4/10 (0.5758)validation : 4/10 (0.5758)training: 5/10 (0.5758)validation : 5/10 (0.5758)early stopping at 5 with loss 0.5758
AttentionModel-training is done: 5/10
2019-06-30 | reset count: 0 | final loss: 0.5758 at epoch 1
making sample. (inference: True, date:2019-07-31 00:00:00, date_number: 391)
making sample. (inference: False, date:2019-06-30 00:00:00, date_number: 390)
[AttentionModel/2019-07-31 | get_logger | INFO]: ====================AttentionModel/2019-07-31 start====================
making sample. (inference: True, date:2019-08-31 00:00:00, date_number: 392)
making sample. (inference: False, date:2019-07-31 00:00:00, date_number: 391)
[AttentionModel/2019-08-31 | get_logger | INFO]: ====================AttentionModel/2019-08-31 start====================
[strategy_integration.py] set_seed(2019-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5867)training: 2/10 (0.5867)validation : 2/10 (0.5856)training: 3/10 (0.5856)validation : 3/10 (0.5856)training: 4/10 (0.5856)validation : 4/10 (0.5848)training: 5/10 (0.5848)validation : 5/10 (0.5807)training: 6/10 (0.5807)validation : 6/10 (0.5807)training: 7/10 (0.5807)validation : 7/10 (0.5807)early stopping at 7 with loss 0.5807
AttentionModel-training is done: 7/10
2019-07-31 | reset count: 0 | final loss: 0.5807 at epoch 5
[strategy_integration.py] set_seed(2019-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5845)training: 2/10 (0.5845)validation : 2/10 (0.5774)training: 3/10 (0.5774)validation : 3/10 (0.5774)training: 4/10 (0.5774)validation : 4/10 (0.5750)training: 5/10 (0.5750)validation : 5/10 (0.5750)training: 6/10 (0.5750)validation : 6/10 (0.5750)training: 7/10 (0.5750)validation : 7/10 (0.5750)early stopping at 7 with loss 0.5750
AttentionModel-training is done: 7/10
2019-08-31 | reset count: 0 | final loss: 0.5750 at epoch 4
making sample. (inference: True, date:2019-09-30 00:00:00, date_number: 393)
making sample. (inference: False, date:2019-08-31 00:00:00, date_number: 392)
[AttentionModel/2019-09-30 | get_logger | INFO]: ====================AttentionModel/2019-09-30 start====================
making sample. (inference: True, date:2019-10-31 00:00:00, date_number: 394)
making sample. (inference: False, date:2019-09-30 00:00:00, date_number: 393)
[AttentionModel/2019-10-31 | get_logger | INFO]: ====================AttentionModel/2019-10-31 start====================
[strategy_integration.py] set_seed(2019-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5917)training: 2/10 (0.5917)validation : 2/10 (0.5852)training: 3/10 (0.5852)validation : 3/10 (0.5852)training: 4/10 (0.5852)validation : 4/10 (0.5850)training: 5/10 (0.5850)validation : 5/10 (0.5850)training: 6/10 (0.5850)validation : 6/10 (0.5850)early stopping at 6 with loss 0.5850
AttentionModel-training is done: 6/10
2019-09-30 | reset count: 0 | final loss: 0.5850 at epoch 4
making sample. (inference: True, date:2019-11-30 00:00:00, date_number: 395)
making sample. (inference: False, date:2019-10-31 00:00:00, date_number: 394)
[AttentionModel/2019-11-30 | get_logger | INFO]: ====================AttentionModel/2019-11-30 start====================
[strategy_integration.py] set_seed(2019-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5931)training: 2/10 (0.5931)validation : 2/10 (0.5931)training: 3/10 (0.5931)validation : 3/10 (0.5898)training: 4/10 (0.5898)validation : 4/10 (0.5857)training: 5/10 (0.5857)validation : 5/10 (0.5857)training: 6/10 (0.5857)validation : 6/10 (0.5857)training: 7/10 (0.5857)validation : 7/10 (0.5857)early stopping at 7 with loss 0.5857
AttentionModel-training is done: 7/10
2019-10-31 | reset count: 0 | final loss: 0.5857 at epoch 4
making sample. (inference: True, date:2019-12-31 00:00:00, date_number: 396)
making sample. (inference: False, date:2019-11-30 00:00:00, date_number: 395)
[AttentionModel/2019-12-31 | get_logger | INFO]: ====================AttentionModel/2019-12-31 start====================
[strategy_integration.py] set_seed(2019-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5802)training: 2/10 (0.5802)validation : 2/10 (0.5802)training: 3/10 (0.5802)validation : 3/10 (0.5751)training: 4/10 (0.5751)validation : 4/10 (0.5751)training: 5/10 (0.5751)validation : 5/10 (0.5751)training: 6/10 (0.5751)validation : 6/10 (0.5744)training: 7/10 (0.5744)validation : 7/10 (0.5744)training: 8/10 (0.5744)validation : 8/10 (0.5744)early stopping at 8 with loss 0.5744
AttentionModel-training is done: 8/10
2019-11-30 | reset count: 0 | final loss: 0.5744 at epoch 6
making sample. (inference: True, date:2020-01-31 00:00:00, date_number: 397)
making sample. (inference: False, date:2019-12-31 00:00:00, date_number: 396)
[AttentionModel/2020-01-31 | get_logger | INFO]: ====================AttentionModel/2020-01-31 start====================
[strategy_integration.py] set_seed(2019-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5759)training: 2/10 (0.5759)validation : 2/10 (0.5755)training: 3/10 (0.5755)validation : 3/10 (0.5755)training: 4/10 (0.5755)validation : 4/10 (0.5736)training: 5/10 (0.5736)validation : 5/10 (0.5736)training: 6/10 (0.5736)validation : 6/10 (0.5736)training: 7/10 (0.5736)validation : 7/10 (0.5736)early stopping at 7 with loss 0.5736
AttentionModel-training is done: 7/10
2019-12-31 | reset count: 0 | final loss: 0.5736 at epoch 4
making sample. (inference: True, date:2020-02-29 00:00:00, date_number: 398)
making sample. (inference: False, date:2020-01-31 00:00:00, date_number: 397)
[AttentionModel/2020-02-29 | get_logger | INFO]: ====================AttentionModel/2020-02-29 start====================
[strategy_integration.py] set_seed(2020-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5883)training: 2/10 (0.5883)validation : 2/10 (0.5852)training: 3/10 (0.5852)validation : 3/10 (0.5850)training: 4/10 (0.5850)validation : 4/10 (0.5850)training: 5/10 (0.5850)validation : 5/10 (0.5818)training: 6/10 (0.5818)validation : 6/10 (0.5818)training: 7/10 (0.5818)validation : 7/10 (0.5793)training: 8/10 (0.5793)validation : 8/10 (0.5793)early stopping at 8 with loss 0.5793
AttentionModel-training is done: 8/10
2020-01-31 | reset count: 0 | final loss: 0.5793 at epoch 7
[strategy_integration.py] set_seed(2020-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5796)training: 2/10 (0.5796)validation : 2/10 (0.5749)training: 3/10 (0.5749)validation : 3/10 (0.5743)training: 4/10 (0.5743)validation : 4/10 (0.5743)training: 5/10 (0.5743)validation : 5/10 (0.5743)early stopping at 5 with loss 0.5743
AttentionModel-training is done: 5/10
2020-02-29 | reset count: 0 | final loss: 0.5743 at epoch 3
making sample. (inference: True, date:2020-03-31 00:00:00, date_number: 399)
making sample. (inference: False, date:2020-02-29 00:00:00, date_number: 398)
[AttentionModel/2020-03-31 | get_logger | INFO]: ====================AttentionModel/2020-03-31 start====================
making sample. (inference: True, date:2020-04-30 00:00:00, date_number: 400)
making sample. (inference: False, date:2020-03-31 00:00:00, date_number: 399)
[AttentionModel/2020-04-30 | get_logger | INFO]: ====================AttentionModel/2020-04-30 start====================
[strategy_integration.py] set_seed(2020-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5777)training: 2/10 (0.5777)validation : 2/10 (0.5757)training: 3/10 (0.5757)validation : 3/10 (0.5757)training: 4/10 (0.5757)validation : 4/10 (0.5752)training: 5/10 (0.5752)validation : 5/10 (0.5752)training: 6/10 (0.5752)validation : 6/10 (0.5739)training: 7/10 (0.5739)validation : 7/10 (0.5739)early stopping at 7 with loss 0.5739
AttentionModel-training is done: 7/10
2020-03-31 | reset count: 0 | final loss: 0.5739 at epoch 6
[strategy_integration.py] set_seed(2020-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5775)training: 2/10 (0.5775)validation : 2/10 (0.5713)training: 3/10 (0.5713)validation : 3/10 (0.5713)training: 4/10 (0.5713)validation : 4/10 (0.5706)training: 5/10 (0.5706)validation : 5/10 (0.5706)training: 6/10 (0.5706)validation : 6/10 (0.5706)training: 7/10 (0.5706)validation : 7/10 (0.5706)early stopping at 7 with loss 0.5706
AttentionModel-training is done: 7/10
2020-04-30 | reset count: 0 | final loss: 0.5706 at epoch 4
making sample. (inference: True, date:2020-05-31 00:00:00, date_number: 401)
making sample. (inference: False, date:2020-04-30 00:00:00, date_number: 400)
[AttentionModel/2020-05-31 | get_logger | INFO]: ====================AttentionModel/2020-05-31 start====================
making sample. (inference: True, date:2020-06-30 00:00:00, date_number: 402)
making sample. (inference: False, date:2020-05-31 00:00:00, date_number: 401)
[AttentionModel/2020-06-30 | get_logger | INFO]: ====================AttentionModel/2020-06-30 start====================
[strategy_integration.py] set_seed(2020-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5806)training: 2/10 (0.5806)validation : 2/10 (0.5780)training: 3/10 (0.5780)validation : 3/10 (0.5778)training: 4/10 (0.5778)validation : 4/10 (0.5778)training: 5/10 (0.5778)validation : 5/10 (0.5771)early stopping at 5 with loss 0.5771
AttentionModel-training is done: 5/10
2020-05-31 | reset count: 0 | final loss: 0.5771 at epoch 5
making sample. (inference: True, date:2020-07-31 00:00:00, date_number: 403)
making sample. (inference: False, date:2020-06-30 00:00:00, date_number: 402)
[AttentionModel/2020-07-31 | get_logger | INFO]: ====================AttentionModel/2020-07-31 start====================
[strategy_integration.py] set_seed(2020-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5756)training: 2/10 (0.5756)validation : 2/10 (0.5683)training: 3/10 (0.5683)validation : 3/10 (0.5678)training: 4/10 (0.5678)validation : 4/10 (0.5644)training: 5/10 (0.5644)validation : 5/10 (0.5644)training: 6/10 (0.5644)validation : 6/10 (0.5644)early stopping at 6 with loss 0.5644
AttentionModel-training is done: 6/10
2020-06-30 | reset count: 0 | final loss: 0.5644 at epoch 4
making sample. (inference: True, date:2020-08-31 00:00:00, date_number: 404)
making sample. (inference: False, date:2020-07-31 00:00:00, date_number: 403)
[AttentionModel/2020-08-31 | get_logger | INFO]: ====================AttentionModel/2020-08-31 start====================
[strategy_integration.py] set_seed(2020-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5782)training: 2/10 (0.5782)validation : 2/10 (0.5776)training: 3/10 (0.5776)validation : 3/10 (0.5765)training: 4/10 (0.5765)validation : 4/10 (0.5737)training: 5/10 (0.5737)validation : 5/10 (0.5737)training: 6/10 (0.5737)validation : 6/10 (0.5723)training: 7/10 (0.5723)validation : 7/10 (0.5723)early stopping at 7 with loss 0.5723
AttentionModel-training is done: 7/10
2020-07-31 | reset count: 0 | final loss: 0.5723 at epoch 6
[strategy_integration.py] set_seed(2020-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5717)training: 2/10 (0.5717)validation : 2/10 (0.5700)training: 3/10 (0.5700)validation : 3/10 (0.5700)training: 4/10 (0.5700)validation : 4/10 (0.5687)training: 5/10 (0.5687)validation : 5/10 (0.5687)training: 6/10 (0.5687)validation : 6/10 (0.5687)early stopping at 6 with loss 0.5687
AttentionModel-training is done: 6/10
2020-08-31 | reset count: 0 | final loss: 0.5687 at epoch 4
making sample. (inference: True, date:2020-09-30 00:00:00, date_number: 405)
making sample. (inference: False, date:2020-08-31 00:00:00, date_number: 404)
[AttentionModel/2020-09-30 | get_logger | INFO]: ====================AttentionModel/2020-09-30 start====================
making sample. (inference: True, date:2020-10-31 00:00:00, date_number: 406)
making sample. (inference: False, date:2020-09-30 00:00:00, date_number: 405)
[AttentionModel/2020-10-31 | get_logger | INFO]: ====================AttentionModel/2020-10-31 start====================
[strategy_integration.py] set_seed(2020-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5705)training: 2/10 (0.5705)validation : 2/10 (0.5705)training: 3/10 (0.5705)validation : 3/10 (0.5655)training: 4/10 (0.5655)validation : 4/10 (0.5655)training: 5/10 (0.5655)validation : 5/10 (0.5655)training: 6/10 (0.5655)validation : 6/10 (0.5643)training: 7/10 (0.5643)validation : 7/10 (0.5643)training: 8/10 (0.5643)validation : 8/10 (0.5643)early stopping at 8 with loss 0.5643
AttentionModel-training is done: 8/10
2020-09-30 | reset count: 0 | final loss: 0.5643 at epoch 6
[strategy_integration.py] set_seed(2020-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5812)training: 2/10 (0.5812)validation : 2/10 (0.5769)training: 3/10 (0.5769)validation : 3/10 (0.5769)training: 4/10 (0.5769)validation : 4/10 (0.5769)training: 5/10 (0.5769)validation : 5/10 (0.5769)training: 6/10 (0.5769)validation : 6/10 (0.5746)training: 7/10 (0.5746)validation : 7/10 (0.5746)early stopping at 7 with loss 0.5746
AttentionModel-training is done: 7/10
2020-10-31 | reset count: 0 | final loss: 0.5746 at epoch 6
making sample. (inference: True, date:2020-11-30 00:00:00, date_number: 407)
making sample. (inference: False, date:2020-10-31 00:00:00, date_number: 406)
[AttentionModel/2020-11-30 | get_logger | INFO]: ====================AttentionModel/2020-11-30 start====================
making sample. (inference: True, date:2020-12-31 00:00:00, date_number: 408)
making sample. (inference: False, date:2020-11-30 00:00:00, date_number: 407)
[AttentionModel/2020-12-31 | get_logger | INFO]: ====================AttentionModel/2020-12-31 start====================
[strategy_integration.py] set_seed(2020-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5719)training: 2/10 (0.5719)validation : 2/10 (0.5719)training: 3/10 (0.5719)validation : 3/10 (0.5655)training: 4/10 (0.5655)validation : 4/10 (0.5655)training: 5/10 (0.5655)validation : 5/10 (0.5655)training: 6/10 (0.5655)validation : 6/10 (0.5655)early stopping at 6 with loss 0.5655
AttentionModel-training is done: 6/10
2020-11-30 | reset count: 0 | final loss: 0.5655 at epoch 3
[strategy_integration.py] set_seed(2020-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5633)training: 2/10 (0.5633)validation : 2/10 (0.5608)training: 3/10 (0.5608)validation : 3/10 (0.5608)training: 4/10 (0.5608)validation : 4/10 (0.5608)training: 5/10 (0.5608)validation : 5/10 (0.5608)early stopping at 5 with loss 0.5608
AttentionModel-training is done: 5/10
2020-12-31 | reset count: 0 | final loss: 0.5608 at epoch 2
making sample. (inference: True, date:2021-01-31 00:00:00, date_number: 409)
making sample. (inference: False, date:2020-12-31 00:00:00, date_number: 408)
[AttentionModel/2021-01-31 | get_logger | INFO]: ====================AttentionModel/2021-01-31 start====================
making sample. (inference: True, date:2021-02-28 00:00:00, date_number: 410)
making sample. (inference: False, date:2021-01-31 00:00:00, date_number: 409)
[AttentionModel/2021-02-28 | get_logger | INFO]: ====================AttentionModel/2021-02-28 start====================
[strategy_integration.py] set_seed(2021-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5659)training: 2/10 (0.5659)validation : 2/10 (0.5635)training: 3/10 (0.5635)validation : 3/10 (0.5627)training: 4/10 (0.5627)validation : 4/10 (0.5627)training: 5/10 (0.5627)validation : 5/10 (0.5627)early stopping at 5 with loss 0.5627
AttentionModel-training is done: 5/10
2021-01-31 | reset count: 0 | final loss: 0.5627 at epoch 3
[strategy_integration.py] set_seed(2021-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5614)training: 2/10 (0.5614)validation : 2/10 (0.5570)training: 3/10 (0.5570)validation : 3/10 (0.5562)training: 4/10 (0.5562)validation : 4/10 (0.5562)training: 5/10 (0.5562)validation : 5/10 (0.5562)early stopping at 5 with loss 0.5562
AttentionModel-training is done: 5/10
2021-02-28 | reset count: 0 | final loss: 0.5562 at epoch 3
making sample. (inference: True, date:2021-03-31 00:00:00, date_number: 411)
making sample. (inference: False, date:2021-02-28 00:00:00, date_number: 410)
[AttentionModel/2021-03-31 | get_logger | INFO]: ====================AttentionModel/2021-03-31 start====================
making sample. (inference: True, date:2021-04-30 00:00:00, date_number: 412)
making sample. (inference: False, date:2021-03-31 00:00:00, date_number: 411)
[AttentionModel/2021-04-30 | get_logger | INFO]: ====================AttentionModel/2021-04-30 start====================
[strategy_integration.py] set_seed(2021-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5629)training: 2/10 (0.5629)validation : 2/10 (0.5629)training: 3/10 (0.5629)validation : 3/10 (0.5606)training: 4/10 (0.5606)validation : 4/10 (0.5606)training: 5/10 (0.5606)validation : 5/10 (0.5606)training: 6/10 (0.5606)validation : 6/10 (0.5606)early stopping at 6 with loss 0.5606
AttentionModel-training is done: 6/10
2021-03-31 | reset count: 0 | final loss: 0.5606 at epoch 3
[strategy_integration.py] set_seed(2021-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5658)training: 2/10 (0.5658)validation : 2/10 (0.5601)training: 3/10 (0.5601)validation : 3/10 (0.5601)training: 4/10 (0.5601)validation : 4/10 (0.5588)training: 5/10 (0.5588)validation : 5/10 (0.5588)training: 6/10 (0.5588)validation : 6/10 (0.5588)early stopping at 6 with loss 0.5588
AttentionModel-training is done: 6/10
2021-04-30 | reset count: 0 | final loss: 0.5588 at epoch 4
making sample. (inference: True, date:2021-05-31 00:00:00, date_number: 413)
making sample. (inference: False, date:2021-04-30 00:00:00, date_number: 412)
[AttentionModel/2021-05-31 | get_logger | INFO]: ====================AttentionModel/2021-05-31 start====================
making sample. (inference: True, date:2021-06-30 00:00:00, date_number: 414)
making sample. (inference: False, date:2021-05-31 00:00:00, date_number: 413)
[AttentionModel/2021-06-30 | get_logger | INFO]: ====================AttentionModel/2021-06-30 start====================
[strategy_integration.py] set_seed(2021-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5627)training: 2/10 (0.5627)validation : 2/10 (0.5616)training: 3/10 (0.5616)validation : 3/10 (0.5616)training: 4/10 (0.5616)validation : 4/10 (0.5597)training: 5/10 (0.5597)validation : 5/10 (0.5592)training: 6/10 (0.5592)validation : 6/10 (0.5592)early stopping at 6 with loss 0.5592
AttentionModel-training is done: 6/10
2021-05-31 | reset count: 0 | final loss: 0.5592 at epoch 5
[strategy_integration.py] set_seed(2021-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5696)training: 2/10 (0.5696)validation : 2/10 (0.5641)training: 3/10 (0.5641)validation : 3/10 (0.5638)training: 4/10 (0.5638)validation : 4/10 (0.5636)training: 5/10 (0.5636)validation : 5/10 (0.5636)training: 6/10 (0.5636)validation : 6/10 (0.5636)early stopping at 6 with loss 0.5636
AttentionModel-training is done: 6/10
2021-06-30 | reset count: 0 | final loss: 0.5636 at epoch 4
making sample. (inference: True, date:2021-07-31 00:00:00, date_number: 415)
making sample. (inference: False, date:2021-06-30 00:00:00, date_number: 414)
[AttentionModel/2021-07-31 | get_logger | INFO]: ====================AttentionModel/2021-07-31 start====================
making sample. (inference: True, date:2021-08-31 00:00:00, date_number: 416)
making sample. (inference: False, date:2021-07-31 00:00:00, date_number: 415)
[AttentionModel/2021-08-31 | get_logger | INFO]: ====================AttentionModel/2021-08-31 start====================
[strategy_integration.py] set_seed(2021-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5789)training: 2/10 (0.5789)validation : 2/10 (0.5743)training: 3/10 (0.5743)validation : 3/10 (0.5730)training: 4/10 (0.5730)validation : 4/10 (0.5730)training: 5/10 (0.5730)validation : 5/10 (0.5729)training: 6/10 (0.5729)validation : 6/10 (0.5729)early stopping at 6 with loss 0.5729
AttentionModel-training is done: 6/10
2021-07-31 | reset count: 0 | final loss: 0.5729 at epoch 5
[strategy_integration.py] set_seed(2021-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5690)training: 2/10 (0.5690)validation : 2/10 (0.5647)training: 3/10 (0.5647)validation : 3/10 (0.5615)training: 4/10 (0.5615)validation : 4/10 (0.5615)training: 5/10 (0.5615)validation : 5/10 (0.5615)training: 6/10 (0.5615)validation : 6/10 (0.5615)early stopping at 6 with loss 0.5615
AttentionModel-training is done: 6/10
2021-08-31 | reset count: 0 | final loss: 0.5615 at epoch 3
making sample. (inference: True, date:2021-09-30 00:00:00, date_number: 417)
making sample. (inference: False, date:2021-08-31 00:00:00, date_number: 416)
[AttentionModel/2021-09-30 | get_logger | INFO]: ====================AttentionModel/2021-09-30 start====================
making sample. (inference: True, date:2021-10-31 00:00:00, date_number: 418)
making sample. (inference: False, date:2021-09-30 00:00:00, date_number: 417)
[AttentionModel/2021-10-31 | get_logger | INFO]: ====================AttentionModel/2021-10-31 start====================
[strategy_integration.py] set_seed(2021-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5770)training: 2/10 (0.5770)validation : 2/10 (0.5745)training: 3/10 (0.5745)validation : 3/10 (0.5745)training: 4/10 (0.5745)validation : 4/10 (0.5725)training: 5/10 (0.5725)validation : 5/10 (0.5725)training: 6/10 (0.5725)validation : 6/10 (0.5725)training: 7/10 (0.5725)validation : 7/10 (0.5723)training: 8/10 (0.5723)validation : 8/10 (0.5723)early stopping at 8 with loss 0.5723
AttentionModel-training is done: 8/10
2021-09-30 | reset count: 0 | final loss: 0.5723 at epoch 7
[strategy_integration.py] set_seed(2021-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5810)training: 2/10 (0.5810)validation : 2/10 (0.5760)training: 3/10 (0.5760)validation : 3/10 (0.5760)training: 4/10 (0.5760)validation : 4/10 (0.5752)training: 5/10 (0.5752)validation : 5/10 (0.5752)training: 6/10 (0.5752)validation : 6/10 (0.5752)training: 7/10 (0.5752)validation : 7/10 (0.5752)early stopping at 7 with loss 0.5752
AttentionModel-training is done: 7/10
2021-10-31 | reset count: 0 | final loss: 0.5752 at epoch 4
[strategy | get_logger | INFO]: ====================strategy start====================
[strategy | _get_infer_dirs | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick188_test03/infer 이 strategy 대상에 추가됨
[strategy | _backtest | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick188_test03/infer is starting
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.153 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠹ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading masking data⠦ [Compustat API] : Loading masking data⠦ [Compustat API] : Loading get_historic_universe from cache✓ [Compustat API] : Loading get_historic_universe from cache done in 1.933 secs
⠧ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.234 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.066 secs
⠋ [Compustat API] : Loading get_monthly_total_return from cache✓ [Compustat API] : Loading get_monthly_total_return from cache done in 0.065 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.081 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.075 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.074 secs
⠋ [Compustat API] : Loading get_monthly_volume_data from cache✓ [Compustat API] : Loading get_monthly_volume_data from cache done in 0.084 secs
[strategy | _allocating | INFO]: [STRATEGY] allocating start
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_weight_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] performance.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] combined.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] weight.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] universe.csv is saved.
[comparison | get_logger | INFO]: ====================comparison start====================
[comparison | compare | INFO]: [5. COMPARISON] comparison is saved.
[comparison | compare | INFO]:                        Harvest_Green_concept001_pick188_test03_price_return  ...  SPCOMP_total_win
2016-01-31_2021-04-30                                           0.231781     ...          0.765625

[1 rows x 27 columns]
2015-12-31
2016-01-31
2016-02-29
2016-03-31
2016-04-30
2016-05-31
2016-06-30
2016-07-31
2016-08-31
2016-09-30
2016-10-31
2016-11-30
2016-12-31
2017-01-31
2017-02-28
2017-03-31
2017-04-30
2017-05-31
2017-06-30
2017-07-31
2017-08-31
2017-09-30
2017-10-31
2017-11-30
2017-12-31
2018-01-31
2018-02-28
2018-03-31
2018-04-30
2018-05-31
2018-06-30
2018-07-31
2018-08-31
2018-09-30
2018-10-31
2018-11-30
2018-12-31
2019-01-31
2019-02-28
2019-03-31
2019-04-30
2019-05-31
2019-06-30
2019-07-31
2019-08-31
2019-09-30
2019-10-31
2019-11-30
2019-12-31
2020-01-31
2020-02-29
2020-03-31
2020-04-30
2020-05-31
2020-06-30
2020-07-31
2020-08-31
2020-09-30
2020-10-31
2020-11-30
2020-12-31
2021-01-31
2021-02-28
2021-03-31
2021-04-30
2021-05-31
2021-06-30
2021-07-31
2021-08-31
2021-09-30
2021-10-31
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.053 secs
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.05 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.07 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.085 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.08 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.708 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.722 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.718 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.716 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.719 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.079 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.702 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.7 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.697 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.692 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.691 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.088 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.711 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.078 secs
⠋ [Compustat API] : Cleaning past days cache✓ [Compustat API] : Cleaning past days cache done in 0.0 secs
auto caching이 활성화 되었습니다. /home/sronly/sr-storage/kirin_cache/kirin_api_cache/2.19.17_2021-11-23에 캐쉬를 저장합니다.
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.148 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache✓ [Compustat API] : Loading get_historic_universe from cache done in 2.009 secs
⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.31 secs
setting tensorflow random seed failed
load_data: op_lev
load_data: roe
load_data: std_u_e
load_data: retail_mfr
load_data: m1
load_data: m2
load_data: mom_12m_1m
load_data: ram_12m_0m
load_data: vol_3m
load_data: t3y
load_data: t5y
load_data: t7y
load_data: t1y_ff
load_data: t5y_ff
load_data: t10y_t2y
load_data: export_growth
load_data: import_growth
load_data: real_gig
load_data: res_mom_12m_1m_0m
load_data: res_vol_6m_3m_0m
load_data: at
load_data: pr_1m_0m
load_data: mv
load_data: btm
load_data: snp500_pr
load_data: wilshire500_pr
load_data: ted
load_data: ppi
load_data: trimmed_pce
load_data: unemploy
load_data: wti
load_data: capa_util
load_data: snp500_pe
load_data: ae_m
load_data: ia_ta
load_data: rc_a
load_data: aaa_t10y
load_data: baa_t10y
load_data: aaa_ff
load_data: real_pig
load_data: federal_tg
load_data: real_gdp
load_data: gpa
load_data: rev_surp
load_data: cash_at
load_data: baa_ff
load_data: core_cpi
load_data: core_pce
load_data: corporate_tg
load_data: industrial_prod
load_data: home_pr
load_data: r_s
load_data: r_a
load_data: fred_ff
load_data: t3m
load_data: t6m
load_data: t2y
load_data: t10y
load_data: aaa
load_data: baa
load_data: core_ppi
load_data: cpi
load_data: pce
load_data: snp500_vol
load_data: ret_noa
load_data: etm
load_data: ia_mv
load_data: sector_values
load_data: mv
load_data: pr_1m_0m
load_data: mv
making sample. (inference: True, date:2015-12-31 00:00:00, date_number: 348)
making sample. (inference: False, date:2012-12-31 00:00:00, date_number: 312)
making sample. (inference: False, date:2013-01-31 00:00:00, date_number: 313)
making sample. (inference: False, date:2013-02-28 00:00:00, date_number: 314)
making sample. (inference: False, date:2013-03-31 00:00:00, date_number: 315)
making sample. (inference: False, date:2013-04-30 00:00:00, date_number: 316)
making sample. (inference: False, date:2013-05-31 00:00:00, date_number: 317)
making sample. (inference: False, date:2013-06-30 00:00:00, date_number: 318)
making sample. (inference: False, date:2013-07-31 00:00:00, date_number: 319)
making sample. (inference: False, date:2013-08-31 00:00:00, date_number: 320)
making sample. (inference: False, date:2013-09-30 00:00:00, date_number: 321)
making sample. (inference: False, date:2013-10-31 00:00:00, date_number: 322)
making sample. (inference: False, date:2013-11-30 00:00:00, date_number: 323)
making sample. (inference: False, date:2013-12-31 00:00:00, date_number: 324)
making sample. (inference: False, date:2014-01-31 00:00:00, date_number: 325)
making sample. (inference: False, date:2014-02-28 00:00:00, date_number: 326)
making sample. (inference: False, date:2014-03-31 00:00:00, date_number: 327)
making sample. (inference: False, date:2014-04-30 00:00:00, date_number: 328)
making sample. (inference: False, date:2014-05-31 00:00:00, date_number: 329)
making sample. (inference: False, date:2014-06-30 00:00:00, date_number: 330)
making sample. (inference: False, date:2014-07-31 00:00:00, date_number: 331)
making sample. (inference: False, date:2014-08-31 00:00:00, date_number: 332)
making sample. (inference: False, date:2014-09-30 00:00:00, date_number: 333)
making sample. (inference: False, date:2014-10-31 00:00:00, date_number: 334)
making sample. (inference: False, date:2014-11-30 00:00:00, date_number: 335)
making sample. (inference: False, date:2014-12-31 00:00:00, date_number: 336)
making sample. (inference: False, date:2015-01-31 00:00:00, date_number: 337)
making sample. (inference: False, date:2015-02-28 00:00:00, date_number: 338)
making sample. (inference: False, date:2015-03-31 00:00:00, date_number: 339)
making sample. (inference: False, date:2015-04-30 00:00:00, date_number: 340)
making sample. (inference: False, date:2015-05-31 00:00:00, date_number: 341)
making sample. (inference: False, date:2015-06-30 00:00:00, date_number: 342)
making sample. (inference: False, date:2015-07-31 00:00:00, date_number: 343)
making sample. (inference: False, date:2015-08-31 00:00:00, date_number: 344)
making sample. (inference: False, date:2015-09-30 00:00:00, date_number: 345)
making sample. (inference: False, date:2015-10-31 00:00:00, date_number: 346)
making sample. (inference: False, date:2015-11-30 00:00:00, date_number: 347)
[AttentionModel/2015-12-31 | get_logger | INFO]: ====================AttentionModel/2015-12-31 start====================
making sample. (inference: True, date:2016-01-31 00:00:00, date_number: 349)
making sample. (inference: False, date:2015-12-31 00:00:00, date_number: 348)
[AttentionModel/2016-01-31 | get_logger | INFO]: ====================AttentionModel/2016-01-31 start====================
[strategy_integration.py] set_seed(2015-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5692)training: 2/10 (0.5692)validation : 2/10 (0.5662)training: 3/10 (0.5662)validation : 3/10 (0.5658)training: 4/10 (0.5658)validation : 4/10 (0.5658)training: 5/10 (0.5658)validation : 5/10 (0.5658)early stopping at 5 with loss 0.5658
AttentionModel-training is done: 5/10
2015-12-31 | reset count: 0 | final loss: 0.5658 at epoch 3
making sample. (inference: True, date:2016-02-29 00:00:00, date_number: 350)
making sample. (inference: False, date:2016-01-31 00:00:00, date_number: 349)
[strategy_integration.py] set_seed(2016-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5819)training: 2/10 (0.5819)validation : 2/10 (0.5819)training: 3/10 (0.5819)validation : 3/10 (0.5806)training: 4/10 (0.5806)validation : 4/10 (0.5806)training: 5/10 (0.5806)validation : 5/10 (0.5806)training: 6/10 (0.5806)validation : 6/10 (0.5806)early stopping at 6 with loss 0.5806
AttentionModel-training is done: 6/10
2016-01-31 | reset count: 0 | final loss: 0.5806 at epoch 3
[AttentionModel/2016-02-29 | get_logger | INFO]: ====================AttentionModel/2016-02-29 start====================
making sample. (inference: True, date:2016-03-31 00:00:00, date_number: 351)
making sample. (inference: False, date:2016-02-29 00:00:00, date_number: 350)
[AttentionModel/2016-03-31 | get_logger | INFO]: ====================AttentionModel/2016-03-31 start====================
[strategy_integration.py] set_seed(2016-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5780)training: 2/10 (0.5780)validation : 2/10 (0.5713)training: 3/10 (0.5713)validation : 3/10 (0.5713)training: 4/10 (0.5713)validation : 4/10 (0.5713)training: 5/10 (0.5713)validation : 5/10 (0.5713)early stopping at 5 with loss 0.5713
AttentionModel-training is done: 5/10
2016-02-29 | reset count: 0 | final loss: 0.5713 at epoch 2
making sample. (inference: True, date:2016-04-30 00:00:00, date_number: 352)
making sample. (inference: False, date:2016-03-31 00:00:00, date_number: 351)
[AttentionModel/2016-04-30 | get_logger | INFO]: ====================AttentionModel/2016-04-30 start====================
[strategy_integration.py] set_seed(2016-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5600)training: 2/10 (0.5600)validation : 2/10 (0.5600)training: 3/10 (0.5600)validation : 3/10 (0.5600)training: 4/10 (0.5600)validation : 4/10 (0.5591)training: 5/10 (0.5591)validation : 5/10 (0.5582)training: 6/10 (0.5582)validation : 6/10 (0.5582)training: 7/10 (0.5582)validation : 7/10 (0.5582)early stopping at 7 with loss 0.5582
AttentionModel-training is done: 7/10
2016-03-31 | reset count: 0 | final loss: 0.5582 at epoch 5
making sample. (inference: True, date:2016-05-31 00:00:00, date_number: 353)
making sample. (inference: False, date:2016-04-30 00:00:00, date_number: 352)
[AttentionModel/2016-05-31 | get_logger | INFO]: ====================AttentionModel/2016-05-31 start====================
[strategy_integration.py] set_seed(2016-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5697)training: 2/10 (0.5697)validation : 2/10 (0.5697)training: 3/10 (0.5697)validation : 3/10 (0.5697)training: 4/10 (0.5697)validation : 4/10 (0.5697)training: 5/10 (0.5697)validation : 5/10 (0.5650)training: 6/10 (0.5650)validation : 6/10 (0.5650)early stopping at 6 with loss 0.5650
AttentionModel-training is done: 6/10
2016-04-30 | reset count: 0 | final loss: 0.5650 at epoch 5
[strategy_integration.py] set_seed(2016-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5628)training: 2/10 (0.5628)validation : 2/10 (0.5628)training: 3/10 (0.5628)validation : 3/10 (0.5628)training: 4/10 (0.5628)validation : 4/10 (0.5628)training: 5/10 (0.5628)validation : 5/10 (0.5628)early stopping at 5 with loss 0.5628
AttentionModel-training is done: 5/10
2016-05-31 | reset count: 0 | final loss: 0.5628 at epoch 1
making sample. (inference: True, date:2016-06-30 00:00:00, date_number: 354)
making sample. (inference: False, date:2016-05-31 00:00:00, date_number: 353)
[AttentionModel/2016-06-30 | get_logger | INFO]: ====================AttentionModel/2016-06-30 start====================
making sample. (inference: True, date:2016-07-31 00:00:00, date_number: 355)
making sample. (inference: False, date:2016-06-30 00:00:00, date_number: 354)
[AttentionModel/2016-07-31 | get_logger | INFO]: ====================AttentionModel/2016-07-31 start====================
[strategy_integration.py] set_seed(2016-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5750)training: 2/10 (0.5750)validation : 2/10 (0.5739)training: 3/10 (0.5739)validation : 3/10 (0.5722)training: 4/10 (0.5722)validation : 4/10 (0.5700)training: 5/10 (0.5700)validation : 5/10 (0.5700)training: 6/10 (0.5700)validation : 6/10 (0.5700)early stopping at 6 with loss 0.5700
AttentionModel-training is done: 6/10
2016-06-30 | reset count: 0 | final loss: 0.5700 at epoch 4
making sample. (inference: True, date:2016-08-31 00:00:00, date_number: 356)
making sample. (inference: False, date:2016-07-31 00:00:00, date_number: 355)
[strategy_integration.py] set_seed(2016-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5713)training: 2/10 (0.5713)validation : 2/10 (0.5705)training: 3/10 (0.5705)validation : 3/10 (0.5705)training: 4/10 (0.5705)validation : 4/10 (0.5672)training: 5/10 (0.5672)validation : 5/10 (0.5672)training: 6/10 (0.5672)validation : 6/10 (0.5672)training: 7/10 (0.5672)validation : 7/10 (0.5672)early stopping at 7 with loss 0.5672
AttentionModel-training is done: 7/10
2016-07-31 | reset count: 0 | final loss: 0.5672 at epoch 4
[AttentionModel/2016-08-31 | get_logger | INFO]: ====================AttentionModel/2016-08-31 start====================
making sample. (inference: True, date:2016-09-30 00:00:00, date_number: 357)
making sample. (inference: False, date:2016-08-31 00:00:00, date_number: 356)
[AttentionModel/2016-09-30 | get_logger | INFO]: ====================AttentionModel/2016-09-30 start====================
[strategy_integration.py] set_seed(2016-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5637)training: 2/10 (0.5637)validation : 2/10 (0.5636)training: 3/10 (0.5636)validation : 3/10 (0.5636)training: 4/10 (0.5636)validation : 4/10 (0.5602)training: 5/10 (0.5602)validation : 5/10 (0.5563)training: 6/10 (0.5563)validation : 6/10 (0.5563)training: 7/10 (0.5563)validation : 7/10 (0.5563)early stopping at 7 with loss 0.5563
AttentionModel-training is done: 7/10
2016-08-31 | reset count: 0 | final loss: 0.5563 at epoch 5
[strategy_integration.py] set_seed(2016-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5630)training: 2/10 (0.5630)validation : 2/10 (0.5630)training: 3/10 (0.5630)validation : 3/10 (0.5630)training: 4/10 (0.5630)validation : 4/10 (0.5630)training: 5/10 (0.5630)validation : 5/10 (0.5630)early stopping at 5 with loss 0.5630
AttentionModel-training is done: 5/10
2016-09-30 | reset count: 0 | final loss: 0.5630 at epoch 1
making sample. (inference: True, date:2016-10-31 00:00:00, date_number: 358)
making sample. (inference: False, date:2016-09-30 00:00:00, date_number: 357)
[AttentionModel/2016-10-31 | get_logger | INFO]: ====================AttentionModel/2016-10-31 start====================
making sample. (inference: True, date:2016-11-30 00:00:00, date_number: 359)
making sample. (inference: False, date:2016-10-31 00:00:00, date_number: 358)
[AttentionModel/2016-11-30 | get_logger | INFO]: ====================AttentionModel/2016-11-30 start====================
[strategy_integration.py] set_seed(2016-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5642)training: 2/10 (0.5642)validation : 2/10 (0.5642)training: 3/10 (0.5642)validation : 3/10 (0.5618)training: 4/10 (0.5618)validation : 4/10 (0.5618)training: 5/10 (0.5618)validation : 5/10 (0.5618)training: 6/10 (0.5618)validation : 6/10 (0.5618)early stopping at 6 with loss 0.5618
AttentionModel-training is done: 6/10
2016-10-31 | reset count: 0 | final loss: 0.5618 at epoch 3
making sample. (inference: True, date:2016-12-31 00:00:00, date_number: 360)
making sample. (inference: False, date:2016-11-30 00:00:00, date_number: 359)
[AttentionModel/2016-12-31 | get_logger | INFO]: ====================AttentionModel/2016-12-31 start====================
[strategy_integration.py] set_seed(2016-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5570)training: 2/10 (0.5570)validation : 2/10 (0.5570)training: 3/10 (0.5570)validation : 3/10 (0.5570)training: 4/10 (0.5570)validation : 4/10 (0.5551)training: 5/10 (0.5551)validation : 5/10 (0.5551)training: 6/10 (0.5551)validation : 6/10 (0.5551)training: 7/10 (0.5551)validation : 7/10 (0.5551)early stopping at 7 with loss 0.5551
AttentionModel-training is done: 7/10
2016-11-30 | reset count: 0 | final loss: 0.5551 at epoch 4
[strategy_integration.py] set_seed(2016-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5627)training: 2/10 (0.5627)validation : 2/10 (0.5586)training: 3/10 (0.5586)validation : 3/10 (0.5560)training: 4/10 (0.5560)validation : 4/10 (0.5560)training: 5/10 (0.5560)validation : 5/10 (0.5560)early stopping at 5 with loss 0.5560
AttentionModel-training is done: 5/10
2016-12-31 | reset count: 0 | final loss: 0.5560 at epoch 3
making sample. (inference: True, date:2017-01-31 00:00:00, date_number: 361)
making sample. (inference: False, date:2016-12-31 00:00:00, date_number: 360)
[AttentionModel/2017-01-31 | get_logger | INFO]: ====================AttentionModel/2017-01-31 start====================
making sample. (inference: True, date:2017-02-28 00:00:00, date_number: 362)
making sample. (inference: False, date:2017-01-31 00:00:00, date_number: 361)
[AttentionModel/2017-02-28 | get_logger | INFO]: ====================AttentionModel/2017-02-28 start====================
[strategy_integration.py] set_seed(2017-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5730)training: 2/10 (0.5730)validation : 2/10 (0.5730)training: 3/10 (0.5730)validation : 3/10 (0.5730)training: 4/10 (0.5730)validation : 4/10 (0.5730)training: 5/10 (0.5730)validation : 5/10 (0.5730)early stopping at 5 with loss 0.5730
AttentionModel-training is done: 5/10
2017-01-31 | reset count: 0 | final loss: 0.5730 at epoch 1
making sample. (inference: True, date:2017-03-31 00:00:00, date_number: 363)
making sample. (inference: False, date:2017-02-28 00:00:00, date_number: 362)
[AttentionModel/2017-03-31 | get_logger | INFO]: ====================AttentionModel/2017-03-31 start====================
[strategy_integration.py] set_seed(2017-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5744)training: 2/10 (0.5744)validation : 2/10 (0.5744)training: 3/10 (0.5744)validation : 3/10 (0.5744)training: 4/10 (0.5744)validation : 4/10 (0.5714)training: 5/10 (0.5714)validation : 5/10 (0.5714)training: 6/10 (0.5714)validation : 6/10 (0.5714)training: 7/10 (0.5714)validation : 7/10 (0.5714)early stopping at 7 with loss 0.5714
AttentionModel-training is done: 7/10
2017-02-28 | reset count: 0 | final loss: 0.5714 at epoch 4
making sample. (inference: True, date:2017-04-30 00:00:00, date_number: 364)
making sample. (inference: False, date:2017-03-31 00:00:00, date_number: 363)
[AttentionModel/2017-04-30 | get_logger | INFO]: ====================AttentionModel/2017-04-30 start====================
[strategy_integration.py] set_seed(2017-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5611)training: 2/10 (0.5611)validation : 2/10 (0.5596)training: 3/10 (0.5596)validation : 3/10 (0.5596)training: 4/10 (0.5596)validation : 4/10 (0.5596)training: 5/10 (0.5596)validation : 5/10 (0.5596)early stopping at 5 with loss 0.5596
AttentionModel-training is done: 5/10
2017-03-31 | reset count: 0 | final loss: 0.5596 at epoch 2
making sample. (inference: True, date:2017-05-31 00:00:00, date_number: 365)
making sample. (inference: False, date:2017-04-30 00:00:00, date_number: 364)
[AttentionModel/2017-05-31 | get_logger | INFO]: ====================AttentionModel/2017-05-31 start====================
[strategy_integration.py] set_seed(2017-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5769)training: 2/10 (0.5769)validation : 2/10 (0.5757)training: 3/10 (0.5757)validation : 3/10 (0.5732)training: 4/10 (0.5732)validation : 4/10 (0.5732)training: 5/10 (0.5732)validation : 5/10 (0.5732)training: 6/10 (0.5732)validation : 6/10 (0.5714)training: 7/10 (0.5714)validation : 7/10 (0.5714)training: 8/10 (0.5714)validation : 8/10 (0.5714)training: 9/10 (0.5714)validation : 9/10 (0.5714)early stopping at 9 with loss 0.5714
AttentionModel-training is done: 9/10
2017-04-30 | reset count: 0 | final loss: 0.5714 at epoch 6
[strategy_integration.py] set_seed(2017-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5704)training: 2/10 (0.5704)validation : 2/10 (0.5704)training: 3/10 (0.5704)validation : 3/10 (0.5704)training: 4/10 (0.5704)validation : 4/10 (0.5704)training: 5/10 (0.5704)validation : 5/10 (0.5686)training: 6/10 (0.5686)validation : 6/10 (0.5685)training: 7/10 (0.5685)validation : 7/10 (0.5685)early stopping at 7 with loss 0.5685
AttentionModel-training is done: 7/10
2017-05-31 | reset count: 0 | final loss: 0.5685 at epoch 6
making sample. (inference: True, date:2017-06-30 00:00:00, date_number: 366)
making sample. (inference: False, date:2017-05-31 00:00:00, date_number: 365)
[AttentionModel/2017-06-30 | get_logger | INFO]: ====================AttentionModel/2017-06-30 start====================
making sample. (inference: True, date:2017-07-31 00:00:00, date_number: 367)
making sample. (inference: False, date:2017-06-30 00:00:00, date_number: 366)
[AttentionModel/2017-07-31 | get_logger | INFO]: ====================AttentionModel/2017-07-31 start====================
[strategy_integration.py] set_seed(2017-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5723)training: 2/10 (0.5723)validation : 2/10 (0.5689)training: 3/10 (0.5689)validation : 3/10 (0.5658)training: 4/10 (0.5658)validation : 4/10 (0.5658)training: 5/10 (0.5658)validation : 5/10 (0.5658)training: 6/10 (0.5658)validation : 6/10 (0.5635)training: 7/10 (0.5635)validation : 7/10 (0.5635)early stopping at 7 with loss 0.5635
AttentionModel-training is done: 7/10
2017-06-30 | reset count: 0 | final loss: 0.5635 at epoch 6
[strategy_integration.py] set_seed(2017-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5672)training: 2/10 (0.5672)validation : 2/10 (0.5671)training: 3/10 (0.5671)validation : 3/10 (0.5671)training: 4/10 (0.5671)validation : 4/10 (0.5671)training: 5/10 (0.5671)validation : 5/10 (0.5671)early stopping at 5 with loss 0.5671
AttentionModel-training is done: 5/10
2017-07-31 | reset count: 0 | final loss: 0.5671 at epoch 2
making sample. (inference: True, date:2017-08-31 00:00:00, date_number: 368)
making sample. (inference: False, date:2017-07-31 00:00:00, date_number: 367)
[AttentionModel/2017-08-31 | get_logger | INFO]: ====================AttentionModel/2017-08-31 start====================
making sample. (inference: True, date:2017-09-30 00:00:00, date_number: 369)
making sample. (inference: False, date:2017-08-31 00:00:00, date_number: 368)
[AttentionModel/2017-09-30 | get_logger | INFO]: ====================AttentionModel/2017-09-30 start====================
[strategy_integration.py] set_seed(2017-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5510)training: 2/10 (0.5510)validation : 2/10 (0.5510)training: 3/10 (0.5510)validation : 3/10 (0.5510)training: 4/10 (0.5510)validation : 4/10 (0.5510)training: 5/10 (0.5510)validation : 5/10 (0.5510)training: 6/10 (0.5510)validation : 6/10 (0.5510)training: 7/10 (0.5510)validation : 7/10 (0.5510)early stopping at 7 with loss 0.5510
AttentionModel-training is done: 7/10
2017-08-31 | reset count: 0 | final loss: 0.5510 at epoch 1
[strategy_integration.py] set_seed(2017-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5564)training: 2/10 (0.5564)validation : 2/10 (0.5552)training: 3/10 (0.5552)validation : 3/10 (0.5552)training: 4/10 (0.5552)validation : 4/10 (0.5552)training: 5/10 (0.5552)validation : 5/10 (0.5552)early stopping at 5 with loss 0.5552
AttentionModel-training is done: 5/10
2017-09-30 | reset count: 0 | final loss: 0.5552 at epoch 2
making sample. (inference: True, date:2017-10-31 00:00:00, date_number: 370)
making sample. (inference: False, date:2017-09-30 00:00:00, date_number: 369)
[AttentionModel/2017-10-31 | get_logger | INFO]: ====================AttentionModel/2017-10-31 start====================
making sample. (inference: True, date:2017-11-30 00:00:00, date_number: 371)
making sample. (inference: False, date:2017-10-31 00:00:00, date_number: 370)
[AttentionModel/2017-11-30 | get_logger | INFO]: ====================AttentionModel/2017-11-30 start====================
[strategy_integration.py] set_seed(2017-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5643)training: 2/10 (0.5643)validation : 2/10 (0.5641)training: 3/10 (0.5641)validation : 3/10 (0.5602)training: 4/10 (0.5602)validation : 4/10 (0.5602)training: 5/10 (0.5602)validation : 5/10 (0.5602)training: 6/10 (0.5602)validation : 6/10 (0.5602)early stopping at 6 with loss 0.5602
AttentionModel-training is done: 6/10
2017-10-31 | reset count: 0 | final loss: 0.5602 at epoch 3
making sample. (inference: True, date:2017-12-31 00:00:00, date_number: 372)
making sample. (inference: False, date:2017-11-30 00:00:00, date_number: 371)
[AttentionModel/2017-12-31 | get_logger | INFO]: ====================AttentionModel/2017-12-31 start====================
[strategy_integration.py] set_seed(2017-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5558)training: 2/10 (0.5558)validation : 2/10 (0.5548)training: 3/10 (0.5548)validation : 3/10 (0.5548)training: 4/10 (0.5548)validation : 4/10 (0.5548)training: 5/10 (0.5548)validation : 5/10 (0.5521)training: 6/10 (0.5521)validation : 6/10 (0.5521)training: 7/10 (0.5521)validation : 7/10 (0.5521)training: 8/10 (0.5521)validation : 8/10 (0.5521)early stopping at 8 with loss 0.5521
AttentionModel-training is done: 8/10
2017-11-30 | reset count: 0 | final loss: 0.5521 at epoch 5
making sample. (inference: True, date:2018-01-31 00:00:00, date_number: 373)
making sample. (inference: False, date:2017-12-31 00:00:00, date_number: 372)
[AttentionModel/2018-01-31 | get_logger | INFO]: ====================AttentionModel/2018-01-31 start====================
[strategy_integration.py] set_seed(2017-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5631)training: 2/10 (0.5631)validation : 2/10 (0.5588)training: 3/10 (0.5588)validation : 3/10 (0.5588)training: 4/10 (0.5588)validation : 4/10 (0.5579)training: 5/10 (0.5579)validation : 5/10 (0.5579)training: 6/10 (0.5579)validation : 6/10 (0.5579)training: 7/10 (0.5579)validation : 7/10 (0.5579)early stopping at 7 with loss 0.5579
AttentionModel-training is done: 7/10
2017-12-31 | reset count: 0 | final loss: 0.5579 at epoch 4
making sample. (inference: True, date:2018-02-28 00:00:00, date_number: 374)
making sample. (inference: False, date:2018-01-31 00:00:00, date_number: 373)
[AttentionModel/2018-02-28 | get_logger | INFO]: ====================AttentionModel/2018-02-28 start====================
[strategy_integration.py] set_seed(2018-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5603)training: 2/10 (0.5603)validation : 2/10 (0.5566)training: 3/10 (0.5566)validation : 3/10 (0.5555)training: 4/10 (0.5555)validation : 4/10 (0.5555)training: 5/10 (0.5555)validation : 5/10 (0.5539)training: 6/10 (0.5539)validation : 6/10 (0.5539)early stopping at 6 with loss 0.5539
AttentionModel-training is done: 6/10
2018-01-31 | reset count: 0 | final loss: 0.5539 at epoch 5
[strategy_integration.py] set_seed(2018-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5659)training: 2/10 (0.5659)validation : 2/10 (0.5639)training: 3/10 (0.5639)validation : 3/10 (0.5639)training: 4/10 (0.5639)validation : 4/10 (0.5639)training: 5/10 (0.5639)validation : 5/10 (0.5639)early stopping at 5 with loss 0.5639
AttentionModel-training is done: 5/10
2018-02-28 | reset count: 0 | final loss: 0.5639 at epoch 2
making sample. (inference: True, date:2018-03-31 00:00:00, date_number: 375)
making sample. (inference: False, date:2018-02-28 00:00:00, date_number: 374)
[AttentionModel/2018-03-31 | get_logger | INFO]: ====================AttentionModel/2018-03-31 start====================
making sample. (inference: True, date:2018-04-30 00:00:00, date_number: 376)
making sample. (inference: False, date:2018-03-31 00:00:00, date_number: 375)
[AttentionModel/2018-04-30 | get_logger | INFO]: ====================AttentionModel/2018-04-30 start====================
[strategy_integration.py] set_seed(2018-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5662)training: 2/10 (0.5662)validation : 2/10 (0.5662)training: 3/10 (0.5662)validation : 3/10 (0.5628)training: 4/10 (0.5628)validation : 4/10 (0.5628)training: 5/10 (0.5628)validation : 5/10 (0.5624)training: 6/10 (0.5624)validation : 6/10 (0.5622)training: 7/10 (0.5622)validation : 7/10 (0.5622)early stopping at 7 with loss 0.5622
AttentionModel-training is done: 7/10
2018-03-31 | reset count: 0 | final loss: 0.5622 at epoch 6
[strategy_integration.py] set_seed(2018-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5756)training: 2/10 (0.5756)validation : 2/10 (0.5718)training: 3/10 (0.5718)validation : 3/10 (0.5718)training: 4/10 (0.5718)validation : 4/10 (0.5718)training: 5/10 (0.5718)validation : 5/10 (0.5718)training: 6/10 (0.5718)validation : 6/10 (0.5718)training: 7/10 (0.5718)validation : 7/10 (0.5718)early stopping at 7 with loss 0.5718
AttentionModel-training is done: 7/10
2018-04-30 | reset count: 0 | final loss: 0.5718 at epoch 2
making sample. (inference: True, date:2018-05-31 00:00:00, date_number: 377)
making sample. (inference: False, date:2018-04-30 00:00:00, date_number: 376)
[AttentionModel/2018-05-31 | get_logger | INFO]: ====================AttentionModel/2018-05-31 start====================
making sample. (inference: True, date:2018-06-30 00:00:00, date_number: 378)
making sample. (inference: False, date:2018-05-31 00:00:00, date_number: 377)
[AttentionModel/2018-06-30 | get_logger | INFO]: ====================AttentionModel/2018-06-30 start====================
[strategy_integration.py] set_seed(2018-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5575)training: 2/10 (0.5575)validation : 2/10 (0.5541)training: 3/10 (0.5541)validation : 3/10 (0.5541)training: 4/10 (0.5541)validation : 4/10 (0.5541)training: 5/10 (0.5541)validation : 5/10 (0.5521)training: 6/10 (0.5521)validation : 6/10 (0.5511)training: 7/10 (0.5511)validation : 7/10 (0.5511)training: 8/10 (0.5511)validation : 8/10 (0.5511)early stopping at 8 with loss 0.5511
AttentionModel-training is done: 8/10
2018-05-31 | reset count: 0 | final loss: 0.5511 at epoch 6
[strategy_integration.py] set_seed(2018-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5660)training: 2/10 (0.5660)validation : 2/10 (0.5660)training: 3/10 (0.5660)validation : 3/10 (0.5660)training: 4/10 (0.5660)validation : 4/10 (0.5629)training: 5/10 (0.5629)validation : 5/10 (0.5629)training: 6/10 (0.5629)validation : 6/10 (0.5629)training: 7/10 (0.5629)validation : 7/10 (0.5629)early stopping at 7 with loss 0.5629
AttentionModel-training is done: 7/10
2018-06-30 | reset count: 0 | final loss: 0.5629 at epoch 4
making sample. (inference: True, date:2018-07-31 00:00:00, date_number: 379)
making sample. (inference: False, date:2018-06-30 00:00:00, date_number: 378)
[AttentionModel/2018-07-31 | get_logger | INFO]: ====================AttentionModel/2018-07-31 start====================
making sample. (inference: True, date:2018-08-31 00:00:00, date_number: 380)
making sample. (inference: False, date:2018-07-31 00:00:00, date_number: 379)
[AttentionModel/2018-08-31 | get_logger | INFO]: ====================AttentionModel/2018-08-31 start====================
[strategy_integration.py] set_seed(2018-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5735)training: 2/10 (0.5735)validation : 2/10 (0.5735)training: 3/10 (0.5735)validation : 3/10 (0.5735)training: 4/10 (0.5735)validation : 4/10 (0.5735)training: 5/10 (0.5735)validation : 5/10 (0.5693)training: 6/10 (0.5693)validation : 6/10 (0.5693)training: 7/10 (0.5693)validation : 7/10 (0.5693)training: 8/10 (0.5693)validation : 8/10 (0.5693)early stopping at 8 with loss 0.5693
AttentionModel-training is done: 8/10
2018-07-31 | reset count: 0 | final loss: 0.5693 at epoch 5
[strategy_integration.py] set_seed(2018-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5726)training: 2/10 (0.5726)validation : 2/10 (0.5671)training: 3/10 (0.5671)validation : 3/10 (0.5671)training: 4/10 (0.5671)validation : 4/10 (0.5671)training: 5/10 (0.5671)validation : 5/10 (0.5654)training: 6/10 (0.5654)validation : 6/10 (0.5641)training: 7/10 (0.5641)validation : 7/10 (0.5641)training: 8/10 (0.5641)validation : 8/10 (0.5641)early stopping at 8 with loss 0.5641
AttentionModel-training is done: 8/10
2018-08-31 | reset count: 0 | final loss: 0.5641 at epoch 6
making sample. (inference: True, date:2018-09-30 00:00:00, date_number: 381)
making sample. (inference: False, date:2018-08-31 00:00:00, date_number: 380)
[AttentionModel/2018-09-30 | get_logger | INFO]: ====================AttentionModel/2018-09-30 start====================
making sample. (inference: True, date:2018-10-31 00:00:00, date_number: 382)
making sample. (inference: False, date:2018-09-30 00:00:00, date_number: 381)
[AttentionModel/2018-10-31 | get_logger | INFO]: ====================AttentionModel/2018-10-31 start====================
[strategy_integration.py] set_seed(2018-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5759)training: 2/10 (0.5759)validation : 2/10 (0.5735)training: 3/10 (0.5735)validation : 3/10 (0.5735)training: 4/10 (0.5735)validation : 4/10 (0.5708)training: 5/10 (0.5708)validation : 5/10 (0.5708)training: 6/10 (0.5708)validation : 6/10 (0.5708)training: 7/10 (0.5708)validation : 7/10 (0.5708)early stopping at 7 with loss 0.5708
AttentionModel-training is done: 7/10
2018-09-30 | reset count: 0 | final loss: 0.5708 at epoch 4
[strategy_integration.py] set_seed(2018-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5791)training: 2/10 (0.5791)validation : 2/10 (0.5729)training: 3/10 (0.5729)validation : 3/10 (0.5726)training: 4/10 (0.5726)validation : 4/10 (0.5716)training: 5/10 (0.5716)validation : 5/10 (0.5716)training: 6/10 (0.5716)validation : 6/10 (0.5716)early stopping at 6 with loss 0.5716
AttentionModel-training is done: 6/10
2018-10-31 | reset count: 0 | final loss: 0.5716 at epoch 4
making sample. (inference: True, date:2018-11-30 00:00:00, date_number: 383)
making sample. (inference: False, date:2018-10-31 00:00:00, date_number: 382)
[AttentionModel/2018-11-30 | get_logger | INFO]: ====================AttentionModel/2018-11-30 start====================
making sample. (inference: True, date:2018-12-31 00:00:00, date_number: 384)
making sample. (inference: False, date:2018-11-30 00:00:00, date_number: 383)
[AttentionModel/2018-12-31 | get_logger | INFO]: ====================AttentionModel/2018-12-31 start====================
[strategy_integration.py] set_seed(2018-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5854)training: 2/10 (0.5854)validation : 2/10 (0.5847)training: 3/10 (0.5847)validation : 3/10 (0.5840)training: 4/10 (0.5840)validation : 4/10 (0.5805)training: 5/10 (0.5805)validation : 5/10 (0.5805)training: 6/10 (0.5805)validation : 6/10 (0.5792)training: 7/10 (0.5792)validation : 7/10 (0.5792)training: 8/10 (0.5792)validation : 8/10 (0.5792)training: 9/10 (0.5792)validation : 9/10 (0.5792)early stopping at 9 with loss 0.5792
AttentionModel-training is done: 9/10
2018-11-30 | reset count: 0 | final loss: 0.5792 at epoch 6
[strategy_integration.py] set_seed(2018-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5761)training: 2/10 (0.5761)validation : 2/10 (0.5733)training: 3/10 (0.5733)validation : 3/10 (0.5717)training: 4/10 (0.5717)validation : 4/10 (0.5717)training: 5/10 (0.5717)validation : 5/10 (0.5717)training: 6/10 (0.5717)validation : 6/10 (0.5717)early stopping at 6 with loss 0.5717
AttentionModel-training is done: 6/10
2018-12-31 | reset count: 0 | final loss: 0.5717 at epoch 3
making sample. (inference: True, date:2019-01-31 00:00:00, date_number: 385)
making sample. (inference: False, date:2018-12-31 00:00:00, date_number: 384)
[AttentionModel/2019-01-31 | get_logger | INFO]: ====================AttentionModel/2019-01-31 start====================
making sample. (inference: True, date:2019-02-28 00:00:00, date_number: 386)
making sample. (inference: False, date:2019-01-31 00:00:00, date_number: 385)
[AttentionModel/2019-02-28 | get_logger | INFO]: ====================AttentionModel/2019-02-28 start====================
[strategy_integration.py] set_seed(2019-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5960)training: 2/10 (0.5960)validation : 2/10 (0.5954)training: 3/10 (0.5954)validation : 3/10 (0.5951)training: 4/10 (0.5951)validation : 4/10 (0.5951)training: 5/10 (0.5951)validation : 5/10 (0.5951)early stopping at 5 with loss 0.5951
AttentionModel-training is done: 5/10
2019-01-31 | reset count: 0 | final loss: 0.5951 at epoch 3
[strategy_integration.py] set_seed(2019-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5928)training: 2/10 (0.5928)validation : 2/10 (0.5910)training: 3/10 (0.5910)validation : 3/10 (0.5882)training: 4/10 (0.5882)validation : 4/10 (0.5882)training: 5/10 (0.5882)validation : 5/10 (0.5882)early stopping at 5 with loss 0.5882
AttentionModel-training is done: 5/10
2019-02-28 | reset count: 0 | final loss: 0.5882 at epoch 3
making sample. (inference: True, date:2019-03-31 00:00:00, date_number: 387)
making sample. (inference: False, date:2019-02-28 00:00:00, date_number: 386)
[AttentionModel/2019-03-31 | get_logger | INFO]: ====================AttentionModel/2019-03-31 start====================
making sample. (inference: True, date:2019-04-30 00:00:00, date_number: 388)
making sample. (inference: False, date:2019-03-31 00:00:00, date_number: 387)
[AttentionModel/2019-04-30 | get_logger | INFO]: ====================AttentionModel/2019-04-30 start====================
[strategy_integration.py] set_seed(2019-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5877)training: 2/10 (0.5877)validation : 2/10 (0.5819)training: 3/10 (0.5819)validation : 3/10 (0.5784)training: 4/10 (0.5784)validation : 4/10 (0.5784)training: 5/10 (0.5784)validation : 5/10 (0.5784)training: 6/10 (0.5784)validation : 6/10 (0.5784)early stopping at 6 with loss 0.5784
AttentionModel-training is done: 6/10
2019-03-31 | reset count: 0 | final loss: 0.5784 at epoch 3
making sample. (inference: True, date:2019-05-31 00:00:00, date_number: 389)
making sample. (inference: False, date:2019-04-30 00:00:00, date_number: 388)
[AttentionModel/2019-05-31 | get_logger | INFO]: ====================AttentionModel/2019-05-31 start====================
[strategy_integration.py] set_seed(2019-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5797)training: 2/10 (0.5797)validation : 2/10 (0.5723)training: 3/10 (0.5723)validation : 3/10 (0.5723)training: 4/10 (0.5723)validation : 4/10 (0.5723)training: 5/10 (0.5723)validation : 5/10 (0.5723)training: 6/10 (0.5723)validation : 6/10 (0.5723)training: 7/10 (0.5723)validation : 7/10 (0.5723)early stopping at 7 with loss 0.5723
AttentionModel-training is done: 7/10
2019-04-30 | reset count: 0 | final loss: 0.5723 at epoch 2
making sample. (inference: True, date:2019-06-30 00:00:00, date_number: 390)
making sample. (inference: False, date:2019-05-31 00:00:00, date_number: 389)
[AttentionModel/2019-06-30 | get_logger | INFO]: ====================AttentionModel/2019-06-30 start====================
[strategy_integration.py] set_seed(2019-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5837)training: 2/10 (0.5837)validation : 2/10 (0.5785)training: 3/10 (0.5785)validation : 3/10 (0.5785)training: 4/10 (0.5785)validation : 4/10 (0.5785)training: 5/10 (0.5785)validation : 5/10 (0.5785)early stopping at 5 with loss 0.5785
AttentionModel-training is done: 5/10
2019-05-31 | reset count: 0 | final loss: 0.5785 at epoch 2
making sample. (inference: True, date:2019-07-31 00:00:00, date_number: 391)
making sample. (inference: False, date:2019-06-30 00:00:00, date_number: 390)
[AttentionModel/2019-07-31 | get_logger | INFO]: ====================AttentionModel/2019-07-31 start====================
[strategy_integration.py] set_seed(2019-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5811)training: 2/10 (0.5811)validation : 2/10 (0.5789)training: 3/10 (0.5789)validation : 3/10 (0.5789)training: 4/10 (0.5789)validation : 4/10 (0.5789)training: 5/10 (0.5789)validation : 5/10 (0.5789)training: 6/10 (0.5789)validation : 6/10 (0.5789)early stopping at 6 with loss 0.5789
AttentionModel-training is done: 6/10
2019-06-30 | reset count: 0 | final loss: 0.5789 at epoch 2
making sample. (inference: True, date:2019-08-31 00:00:00, date_number: 392)
making sample. (inference: False, date:2019-07-31 00:00:00, date_number: 391)
[AttentionModel/2019-08-31 | get_logger | INFO]: ====================AttentionModel/2019-08-31 start====================
[strategy_integration.py] set_seed(2019-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5859)training: 2/10 (0.5859)validation : 2/10 (0.5815)training: 3/10 (0.5815)validation : 3/10 (0.5815)training: 4/10 (0.5815)validation : 4/10 (0.5815)training: 5/10 (0.5815)validation : 5/10 (0.5815)early stopping at 5 with loss 0.5815
AttentionModel-training is done: 5/10
2019-07-31 | reset count: 0 | final loss: 0.5815 at epoch 2
making sample. (inference: True, date:2019-09-30 00:00:00, date_number: 393)
making sample. (inference: False, date:2019-08-31 00:00:00, date_number: 392)
[AttentionModel/2019-09-30 | get_logger | INFO]: ====================AttentionModel/2019-09-30 start====================
[strategy_integration.py] set_seed(2019-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5795)training: 2/10 (0.5795)validation : 2/10 (0.5759)training: 3/10 (0.5759)validation : 3/10 (0.5759)training: 4/10 (0.5759)validation : 4/10 (0.5759)training: 5/10 (0.5759)validation : 5/10 (0.5722)training: 6/10 (0.5722)validation : 6/10 (0.5722)early stopping at 6 with loss 0.5722
AttentionModel-training is done: 6/10
2019-08-31 | reset count: 0 | final loss: 0.5722 at epoch 5
making sample. (inference: True, date:2019-10-31 00:00:00, date_number: 394)
making sample. (inference: False, date:2019-09-30 00:00:00, date_number: 393)
[AttentionModel/2019-10-31 | get_logger | INFO]: ====================AttentionModel/2019-10-31 start====================
[strategy_integration.py] set_seed(2019-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5807)training: 2/10 (0.5807)validation : 2/10 (0.5807)training: 3/10 (0.5807)validation : 3/10 (0.5807)training: 4/10 (0.5807)validation : 4/10 (0.5807)training: 5/10 (0.5807)validation : 5/10 (0.5793)training: 6/10 (0.5793)validation : 6/10 (0.5793)early stopping at 6 with loss 0.5793
AttentionModel-training is done: 6/10
2019-09-30 | reset count: 0 | final loss: 0.5793 at epoch 5
making sample. (inference: True, date:2019-11-30 00:00:00, date_number: 395)
making sample. (inference: False, date:2019-10-31 00:00:00, date_number: 394)
[AttentionModel/2019-11-30 | get_logger | INFO]: ====================AttentionModel/2019-11-30 start====================
[strategy_integration.py] set_seed(2019-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5906)training: 2/10 (0.5906)validation : 2/10 (0.5878)training: 3/10 (0.5878)validation : 3/10 (0.5878)training: 4/10 (0.5878)validation : 4/10 (0.5867)training: 5/10 (0.5867)validation : 5/10 (0.5867)training: 6/10 (0.5867)validation : 6/10 (0.5867)training: 7/10 (0.5867)validation : 7/10 (0.5866)training: 8/10 (0.5866)validation : 8/10 (0.5866)training: 9/10 (0.5866)validation : 9/10 (0.5866)training: 10/10 (0.5866)validation : 10/10 (0.5852)AttentionModel-training is done: 10/10
2019-10-31 | reset count: 0 | final loss: 0.5852 at epoch 10
[strategy_integration.py] set_seed(2019-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5948)training: 2/10 (0.5948)validation : 2/10 (0.5838)training: 3/10 (0.5838)validation : 3/10 (0.5838)training: 4/10 (0.5838)validation : 4/10 (0.5838)training: 5/10 (0.5838)validation : 5/10 (0.5838)training: 6/10 (0.5838)validation : 6/10 (0.5838)early stopping at 6 with loss 0.5838
AttentionModel-training is done: 6/10
2019-11-30 | reset count: 0 | final loss: 0.5838 at epoch 2
making sample. (inference: True, date:2019-12-31 00:00:00, date_number: 396)
making sample. (inference: False, date:2019-11-30 00:00:00, date_number: 395)
[AttentionModel/2019-12-31 | get_logger | INFO]: ====================AttentionModel/2019-12-31 start====================
making sample. (inference: True, date:2020-01-31 00:00:00, date_number: 397)
making sample. (inference: False, date:2019-12-31 00:00:00, date_number: 396)
[AttentionModel/2020-01-31 | get_logger | INFO]: ====================AttentionModel/2020-01-31 start====================
[strategy_integration.py] set_seed(2019-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5834)training: 2/10 (0.5834)validation : 2/10 (0.5801)training: 3/10 (0.5801)validation : 3/10 (0.5782)training: 4/10 (0.5782)validation : 4/10 (0.5782)training: 5/10 (0.5782)validation : 5/10 (0.5782)training: 6/10 (0.5782)validation : 6/10 (0.5782)early stopping at 6 with loss 0.5782
AttentionModel-training is done: 6/10
2019-12-31 | reset count: 0 | final loss: 0.5782 at epoch 3
[strategy_integration.py] set_seed(2020-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5886)training: 2/10 (0.5886)validation : 2/10 (0.5848)training: 3/10 (0.5848)validation : 3/10 (0.5848)training: 4/10 (0.5848)validation : 4/10 (0.5848)training: 5/10 (0.5848)validation : 5/10 (0.5848)early stopping at 5 with loss 0.5848
AttentionModel-training is done: 5/10
2020-01-31 | reset count: 0 | final loss: 0.5848 at epoch 2
making sample. (inference: True, date:2020-02-29 00:00:00, date_number: 398)
making sample. (inference: False, date:2020-01-31 00:00:00, date_number: 397)
[AttentionModel/2020-02-29 | get_logger | INFO]: ====================AttentionModel/2020-02-29 start====================
making sample. (inference: True, date:2020-03-31 00:00:00, date_number: 399)
making sample. (inference: False, date:2020-02-29 00:00:00, date_number: 398)
[AttentionModel/2020-03-31 | get_logger | INFO]: ====================AttentionModel/2020-03-31 start====================
[strategy_integration.py] set_seed(2020-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5834)training: 2/10 (0.5834)validation : 2/10 (0.5764)training: 3/10 (0.5764)validation : 3/10 (0.5764)training: 4/10 (0.5764)validation : 4/10 (0.5764)training: 5/10 (0.5764)validation : 5/10 (0.5764)training: 6/10 (0.5764)validation : 6/10 (0.5764)early stopping at 6 with loss 0.5764
AttentionModel-training is done: 6/10
2020-02-29 | reset count: 0 | final loss: 0.5764 at epoch 2
making sample. (inference: True, date:2020-04-30 00:00:00, date_number: 400)
making sample. (inference: False, date:2020-03-31 00:00:00, date_number: 399)
[AttentionModel/2020-04-30 | get_logger | INFO]: ====================AttentionModel/2020-04-30 start====================
[strategy_integration.py] set_seed(2020-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5754)training: 2/10 (0.5754)validation : 2/10 (0.5754)training: 3/10 (0.5754)validation : 3/10 (0.5754)training: 4/10 (0.5754)validation : 4/10 (0.5754)training: 5/10 (0.5754)validation : 5/10 (0.5754)training: 6/10 (0.5754)validation : 6/10 (0.5754)early stopping at 6 with loss 0.5754
AttentionModel-training is done: 6/10
2020-03-31 | reset count: 0 | final loss: 0.5754 at epoch 1
making sample. (inference: True, date:2020-05-31 00:00:00, date_number: 401)
making sample. (inference: False, date:2020-04-30 00:00:00, date_number: 400)
[AttentionModel/2020-05-31 | get_logger | INFO]: ====================AttentionModel/2020-05-31 start====================
[strategy_integration.py] set_seed(2020-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5867)training: 2/10 (0.5867)validation : 2/10 (0.5821)training: 3/10 (0.5821)validation : 3/10 (0.5804)training: 4/10 (0.5804)validation : 4/10 (0.5804)training: 5/10 (0.5804)validation : 5/10 (0.5804)training: 6/10 (0.5804)validation : 6/10 (0.5804)early stopping at 6 with loss 0.5804
AttentionModel-training is done: 6/10
2020-04-30 | reset count: 0 | final loss: 0.5804 at epoch 3
making sample. (inference: True, date:2020-06-30 00:00:00, date_number: 402)
making sample. (inference: False, date:2020-05-31 00:00:00, date_number: 401)
[strategy_integration.py] set_seed(2020-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5790)training: 2/10 (0.5790)validation : 2/10 (0.5788)training: 3/10 (0.5788)validation : 3/10 (0.5737)training: 4/10 (0.5737)validation : 4/10 (0.5737)training: 5/10 (0.5737)validation : 5/10 (0.5737)training: 6/10 (0.5737)validation : 6/10 (0.5737)early stopping at 6 with loss 0.5737
AttentionModel-training is done: 6/10
2020-05-31 | reset count: 0 | final loss: 0.5737 at epoch 3
[AttentionModel/2020-06-30 | get_logger | INFO]: ====================AttentionModel/2020-06-30 start====================
making sample. (inference: True, date:2020-07-31 00:00:00, date_number: 403)
making sample. (inference: False, date:2020-06-30 00:00:00, date_number: 402)
[AttentionModel/2020-07-31 | get_logger | INFO]: ====================AttentionModel/2020-07-31 start====================
[strategy_integration.py] set_seed(2020-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5796)training: 2/10 (0.5796)validation : 2/10 (0.5773)training: 3/10 (0.5773)validation : 3/10 (0.5766)training: 4/10 (0.5766)validation : 4/10 (0.5739)training: 5/10 (0.5739)validation : 5/10 (0.5739)training: 6/10 (0.5739)validation : 6/10 (0.5739)early stopping at 6 with loss 0.5739
AttentionModel-training is done: 6/10
2020-06-30 | reset count: 0 | final loss: 0.5739 at epoch 5
making sample. (inference: True, date:2020-08-31 00:00:00, date_number: 404)
making sample. (inference: False, date:2020-07-31 00:00:00, date_number: 403)
[AttentionModel/2020-08-31 | get_logger | INFO]: ====================AttentionModel/2020-08-31 start====================
[strategy_integration.py] set_seed(2020-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5834)training: 2/10 (0.5834)validation : 2/10 (0.5767)training: 3/10 (0.5767)validation : 3/10 (0.5767)training: 4/10 (0.5767)validation : 4/10 (0.5767)training: 5/10 (0.5767)validation : 5/10 (0.5767)training: 6/10 (0.5767)validation : 6/10 (0.5767)training: 7/10 (0.5767)validation : 7/10 (0.5767)training: 8/10 (0.5767)validation : 8/10 (0.5767)training: 9/10 (0.5767)validation : 9/10 (0.5767)early stopping at 9 with loss 0.5767
AttentionModel-training is done: 9/10
2020-07-31 | reset count: 0 | final loss: 0.5767 at epoch 2
[strategy_integration.py] set_seed(2020-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5692)training: 2/10 (0.5692)validation : 2/10 (0.5689)training: 3/10 (0.5689)validation : 3/10 (0.5689)training: 4/10 (0.5689)validation : 4/10 (0.5689)training: 5/10 (0.5689)validation : 5/10 (0.5689)early stopping at 5 with loss 0.5689
AttentionModel-training is done: 5/10
2020-08-31 | reset count: 0 | final loss: 0.5689 at epoch 2
making sample. (inference: True, date:2020-09-30 00:00:00, date_number: 405)
making sample. (inference: False, date:2020-08-31 00:00:00, date_number: 404)
[AttentionModel/2020-09-30 | get_logger | INFO]: ====================AttentionModel/2020-09-30 start====================
making sample. (inference: True, date:2020-10-31 00:00:00, date_number: 406)
making sample. (inference: False, date:2020-09-30 00:00:00, date_number: 405)
[AttentionModel/2020-10-31 | get_logger | INFO]: ====================AttentionModel/2020-10-31 start====================
[strategy_integration.py] set_seed(2020-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5760)training: 2/10 (0.5760)validation : 2/10 (0.5708)training: 3/10 (0.5708)validation : 3/10 (0.5703)training: 4/10 (0.5703)validation : 4/10 (0.5703)training: 5/10 (0.5703)validation : 5/10 (0.5703)training: 6/10 (0.5703)validation : 6/10 (0.5703)early stopping at 6 with loss 0.5703
AttentionModel-training is done: 6/10
2020-09-30 | reset count: 0 | final loss: 0.5703 at epoch 3
[strategy_integration.py] set_seed(2020-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5770)training: 2/10 (0.5770)validation : 2/10 (0.5724)training: 3/10 (0.5724)validation : 3/10 (0.5724)training: 4/10 (0.5724)validation : 4/10 (0.5724)training: 5/10 (0.5724)validation : 5/10 (0.5671)training: 6/10 (0.5671)validation : 6/10 (0.5671)early stopping at 6 with loss 0.5671
AttentionModel-training is done: 6/10
2020-10-31 | reset count: 0 | final loss: 0.5671 at epoch 5
making sample. (inference: True, date:2020-11-30 00:00:00, date_number: 407)
making sample. (inference: False, date:2020-10-31 00:00:00, date_number: 406)
[AttentionModel/2020-11-30 | get_logger | INFO]: ====================AttentionModel/2020-11-30 start====================
making sample. (inference: True, date:2020-12-31 00:00:00, date_number: 408)
making sample. (inference: False, date:2020-11-30 00:00:00, date_number: 407)
[AttentionModel/2020-12-31 | get_logger | INFO]: ====================AttentionModel/2020-12-31 start====================
[strategy_integration.py] set_seed(2020-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5634)training: 2/10 (0.5634)validation : 2/10 (0.5634)training: 3/10 (0.5634)validation : 3/10 (0.5634)training: 4/10 (0.5634)validation : 4/10 (0.5634)training: 5/10 (0.5634)validation : 5/10 (0.5616)training: 6/10 (0.5616)validation : 6/10 (0.5616)training: 7/10 (0.5616)validation : 7/10 (0.5606)training: 8/10 (0.5606)validation : 8/10 (0.5606)early stopping at 8 with loss 0.5606
AttentionModel-training is done: 8/10
2020-11-30 | reset count: 0 | final loss: 0.5606 at epoch 7
[strategy_integration.py] set_seed(2020-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5626)training: 2/10 (0.5626)validation : 2/10 (0.5592)training: 3/10 (0.5592)validation : 3/10 (0.5592)training: 4/10 (0.5592)validation : 4/10 (0.5592)training: 5/10 (0.5592)validation : 5/10 (0.5592)early stopping at 5 with loss 0.5592
AttentionModel-training is done: 5/10
2020-12-31 | reset count: 0 | final loss: 0.5592 at epoch 2
making sample. (inference: True, date:2021-01-31 00:00:00, date_number: 409)
making sample. (inference: False, date:2020-12-31 00:00:00, date_number: 408)
[AttentionModel/2021-01-31 | get_logger | INFO]: ====================AttentionModel/2021-01-31 start====================
making sample. (inference: True, date:2021-02-28 00:00:00, date_number: 410)
making sample. (inference: False, date:2021-01-31 00:00:00, date_number: 409)
[AttentionModel/2021-02-28 | get_logger | INFO]: ====================AttentionModel/2021-02-28 start====================
[strategy_integration.py] set_seed(2021-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5641)training: 2/10 (0.5641)validation : 2/10 (0.5591)training: 3/10 (0.5591)validation : 3/10 (0.5580)training: 4/10 (0.5580)validation : 4/10 (0.5574)training: 5/10 (0.5574)validation : 5/10 (0.5574)training: 6/10 (0.5574)validation : 6/10 (0.5554)training: 7/10 (0.5554)validation : 7/10 (0.5554)early stopping at 7 with loss 0.5554
AttentionModel-training is done: 7/10
2021-01-31 | reset count: 0 | final loss: 0.5554 at epoch 6
[strategy_integration.py] set_seed(2021-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5579)training: 2/10 (0.5579)validation : 2/10 (0.5579)training: 3/10 (0.5579)validation : 3/10 (0.5579)training: 4/10 (0.5579)validation : 4/10 (0.5579)training: 5/10 (0.5579)validation : 5/10 (0.5571)early stopping at 5 with loss 0.5571
AttentionModel-training is done: 5/10
2021-02-28 | reset count: 0 | final loss: 0.5571 at epoch 5
making sample. (inference: True, date:2021-03-31 00:00:00, date_number: 411)
making sample. (inference: False, date:2021-02-28 00:00:00, date_number: 410)
[AttentionModel/2021-03-31 | get_logger | INFO]: ====================AttentionModel/2021-03-31 start====================
making sample. (inference: True, date:2021-04-30 00:00:00, date_number: 412)
making sample. (inference: False, date:2021-03-31 00:00:00, date_number: 411)
[AttentionModel/2021-04-30 | get_logger | INFO]: ====================AttentionModel/2021-04-30 start====================
[strategy_integration.py] set_seed(2021-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5703)training: 2/10 (0.5703)validation : 2/10 (0.5685)training: 3/10 (0.5685)validation : 3/10 (0.5657)training: 4/10 (0.5657)validation : 4/10 (0.5657)training: 5/10 (0.5657)validation : 5/10 (0.5657)early stopping at 5 with loss 0.5657
AttentionModel-training is done: 5/10
2021-03-31 | reset count: 0 | final loss: 0.5657 at epoch 3
making sample. (inference: True, date:2021-05-31 00:00:00, date_number: 413)
making sample. (inference: False, date:2021-04-30 00:00:00, date_number: 412)
[AttentionModel/2021-05-31 | get_logger | INFO]: ====================AttentionModel/2021-05-31 start====================
[strategy_integration.py] set_seed(2021-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5738)training: 2/10 (0.5738)validation : 2/10 (0.5686)training: 3/10 (0.5686)validation : 3/10 (0.5686)training: 4/10 (0.5686)validation : 4/10 (0.5686)training: 5/10 (0.5686)validation : 5/10 (0.5686)training: 6/10 (0.5686)validation : 6/10 (0.5686)early stopping at 6 with loss 0.5686
AttentionModel-training is done: 6/10
2021-04-30 | reset count: 0 | final loss: 0.5686 at epoch 2
making sample. (inference: True, date:2021-06-30 00:00:00, date_number: 414)
making sample. (inference: False, date:2021-05-31 00:00:00, date_number: 413)
[AttentionModel/2021-06-30 | get_logger | INFO]: ====================AttentionModel/2021-06-30 start====================
[strategy_integration.py] set_seed(2021-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5671)training: 2/10 (0.5671)validation : 2/10 (0.5671)training: 3/10 (0.5671)validation : 3/10 (0.5659)training: 4/10 (0.5659)validation : 4/10 (0.5621)training: 5/10 (0.5621)validation : 5/10 (0.5621)training: 6/10 (0.5621)validation : 6/10 (0.5621)training: 7/10 (0.5621)validation : 7/10 (0.5621)early stopping at 7 with loss 0.5621
AttentionModel-training is done: 7/10
2021-05-31 | reset count: 0 | final loss: 0.5621 at epoch 4
making sample. (inference: True, date:2021-07-31 00:00:00, date_number: 415)
making sample. (inference: False, date:2021-06-30 00:00:00, date_number: 414)
[AttentionModel/2021-07-31 | get_logger | INFO]: ====================AttentionModel/2021-07-31 start====================
[strategy_integration.py] set_seed(2021-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5921)training: 2/10 (0.5921)validation : 2/10 (0.5861)training: 3/10 (0.5861)validation : 3/10 (0.5861)training: 4/10 (0.5861)validation : 4/10 (0.5860)training: 5/10 (0.5860)validation : 5/10 (0.5855)training: 6/10 (0.5855)validation : 6/10 (0.5837)training: 7/10 (0.5837)validation : 7/10 (0.5837)training: 8/10 (0.5837)validation : 8/10 (0.5812)training: 9/10 (0.5812)validation : 9/10 (0.5812)training: 10/10 (0.5812)validation : 10/10 (0.5812)AttentionModel-training is done: 10/10
2021-06-30 | reset count: 0 | final loss: 0.5812 at epoch 8
[strategy_integration.py] set_seed(2021-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5710)training: 2/10 (0.5710)validation : 2/10 (0.5705)training: 3/10 (0.5705)validation : 3/10 (0.5700)training: 4/10 (0.5700)validation : 4/10 (0.5670)training: 5/10 (0.5670)validation : 5/10 (0.5667)training: 6/10 (0.5667)validation : 6/10 (0.5667)training: 7/10 (0.5667)validation : 7/10 (0.5667)early stopping at 7 with loss 0.5667
AttentionModel-training is done: 7/10
2021-07-31 | reset count: 0 | final loss: 0.5667 at epoch 5
making sample. (inference: True, date:2021-08-31 00:00:00, date_number: 416)
making sample. (inference: False, date:2021-07-31 00:00:00, date_number: 415)
[AttentionModel/2021-08-31 | get_logger | INFO]: ====================AttentionModel/2021-08-31 start====================
making sample. (inference: True, date:2021-09-30 00:00:00, date_number: 417)
making sample. (inference: False, date:2021-08-31 00:00:00, date_number: 416)
[AttentionModel/2021-09-30 | get_logger | INFO]: ====================AttentionModel/2021-09-30 start====================
[strategy_integration.py] set_seed(2021-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5809)training: 2/10 (0.5809)validation : 2/10 (0.5807)training: 3/10 (0.5807)validation : 3/10 (0.5807)training: 4/10 (0.5807)validation : 4/10 (0.5807)training: 5/10 (0.5807)validation : 5/10 (0.5795)early stopping at 5 with loss 0.5795
AttentionModel-training is done: 5/10
2021-08-31 | reset count: 0 | final loss: 0.5795 at epoch 5
making sample. (inference: True, date:2021-10-31 00:00:00, date_number: 418)
making sample. (inference: False, date:2021-09-30 00:00:00, date_number: 417)
[AttentionModel/2021-10-31 | get_logger | INFO]: ====================AttentionModel/2021-10-31 start====================
[strategy_integration.py] set_seed(2021-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5715)training: 2/10 (0.5715)validation : 2/10 (0.5707)training: 3/10 (0.5707)validation : 3/10 (0.5707)training: 4/10 (0.5707)validation : 4/10 (0.5696)training: 5/10 (0.5696)validation : 5/10 (0.5696)training: 6/10 (0.5696)validation : 6/10 (0.5696)training: 7/10 (0.5696)validation : 7/10 (0.5696)early stopping at 7 with loss 0.5696
AttentionModel-training is done: 7/10
2021-09-30 | reset count: 0 | final loss: 0.5696 at epoch 4
[strategy_integration.py] set_seed(2021-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5771)training: 2/10 (0.5771)validation : 2/10 (0.5760)training: 3/10 (0.5760)validation : 3/10 (0.5760)training: 4/10 (0.5760)validation : 4/10 (0.5760)training: 5/10 (0.5760)validation : 5/10 (0.5760)early stopping at 5 with loss 0.5760
AttentionModel-training is done: 5/10
2021-10-31 | reset count: 0 | final loss: 0.5760 at epoch 2
[strategy | get_logger | INFO]: ====================strategy start====================
[strategy | _get_infer_dirs | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick250_test01/infer 이 strategy 대상에 추가됨
[strategy | _backtest | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick250_test01/infer is starting
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.154 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠹ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache⠦ [Compustat API] : Loading get_historic_universe from cache⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading get_historic_universe from cache done in 1.936 secs
⠧ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.237 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.063 secs
⠋ [Compustat API] : Loading get_monthly_total_return from cache✓ [Compustat API] : Loading get_monthly_total_return from cache done in 0.063 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.077 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.072 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.071 secs
⠋ [Compustat API] : Loading get_monthly_volume_data from cache✓ [Compustat API] : Loading get_monthly_volume_data from cache done in 0.081 secs
[strategy | _allocating | INFO]: [STRATEGY] allocating start
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_weight_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] performance.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] combined.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] weight.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] universe.csv is saved.
[comparison | get_logger | INFO]: ====================comparison start====================
[comparison | compare | INFO]: [5. COMPARISON] comparison is saved.
[comparison | compare | INFO]:                        Harvest_Green_concept001_pick250_test01_price_return  ...  SPCOMP_total_win
2016-01-31_2021-04-30                                           0.226744     ...          0.765625

[1 rows x 27 columns]
2015-12-31
2016-01-31
2016-02-29
2016-03-31
2016-04-30
2016-05-31
2016-06-30
2016-07-31
2016-08-31
2016-09-30
2016-10-31
2016-11-30
2016-12-31
2017-01-31
2017-02-28
2017-03-31
2017-04-30
2017-05-31
2017-06-30
2017-07-31
2017-08-31
2017-09-30
2017-10-31
2017-11-30
2017-12-31
2018-01-31
2018-02-28
2018-03-31
2018-04-30
2018-05-31
2018-06-30
2018-07-31
2018-08-31
2018-09-30
2018-10-31
2018-11-30
2018-12-31
2019-01-31
2019-02-28
2019-03-31
2019-04-30
2019-05-31
2019-06-30
2019-07-31
2019-08-31
2019-09-30
2019-10-31
2019-11-30
2019-12-31
2020-01-31
2020-02-29
2020-03-31
2020-04-30
2020-05-31
2020-06-30
2020-07-31
2020-08-31
2020-09-30
2020-10-31
2020-11-30
2020-12-31
2021-01-31
2021-02-28
2021-03-31
2021-04-30
2021-05-31
2021-06-30
2021-07-31
2021-08-31
2021-09-30
2021-10-31
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.048 secs
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.045 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.063 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.078 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.072 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache⠹ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.701 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.693 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.695 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.694 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.696 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.08 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.682 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.681 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.681 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.678 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.673 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.083 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.696 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.08 secs
⠋ [Compustat API] : Cleaning past days cache✓ [Compustat API] : Cleaning past days cache done in 0.0 secs
auto caching이 활성화 되었습니다. /home/sronly/sr-storage/kirin_cache/kirin_api_cache/2.19.17_2021-11-23에 캐쉬를 저장합니다.
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.149 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠹ [Compustat API] : Loading masking data⠹ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache✓ [Compustat API] : Loading get_historic_universe from cache done in 2.209 secs
⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.51 secs
setting tensorflow random seed failed
load_data: op_lev
load_data: roe
load_data: std_u_e
load_data: retail_mfr
load_data: m1
load_data: m2
load_data: ae_m
load_data: ia_ta
load_data: rc_a
load_data: t1y_ff
load_data: t5y_ff
load_data: t10y_t2y
load_data: export_growth
load_data: import_growth
load_data: real_gig
load_data: res_mom_12m_1m_0m
load_data: res_vol_6m_3m_0m
load_data: at
load_data: gpa
load_data: rev_surp
load_data: cash_at
load_data: baa_ff
load_data: core_cpi
load_data: core_pce
load_data: corporate_tg
load_data: industrial_prod
load_data: home_pr
load_data: r_s
load_data: r_a
load_data: fred_ff
load_data: t3y
load_data: t5y
load_data: t7y
load_data: aaa_t10y
load_data: baa_t10y
load_data: aaa_ff
load_data: real_pig
load_data: federal_tg
load_data: real_gdp
load_data: mom_12m_1m
load_data: ram_12m_0m
load_data: vol_3m
load_data: snp500_pr
load_data: wilshire500_pr
load_data: ted
load_data: ppi
load_data: trimmed_pce
load_data: unemploy
load_data: snp500_vol
load_data: pr_1m_0m
load_data: mv
load_data: btm
load_data: t3m
load_data: t6m
load_data: t2y
load_data: t10y
load_data: aaa
load_data: baa
load_data: core_ppi
load_data: cpi
load_data: pce
load_data: wti
load_data: capa_util
load_data: snp500_pe
load_data: sector_values
load_data: mv
load_data: pr_1m_0m
load_data: mv
making sample. (inference: True, date:2015-12-31 00:00:00, date_number: 348)
making sample. (inference: False, date:2012-12-31 00:00:00, date_number: 312)
making sample. (inference: False, date:2013-01-31 00:00:00, date_number: 313)
making sample. (inference: False, date:2013-02-28 00:00:00, date_number: 314)
making sample. (inference: False, date:2013-03-31 00:00:00, date_number: 315)
making sample. (inference: False, date:2013-04-30 00:00:00, date_number: 316)
making sample. (inference: False, date:2013-05-31 00:00:00, date_number: 317)
making sample. (inference: False, date:2013-06-30 00:00:00, date_number: 318)
making sample. (inference: False, date:2013-07-31 00:00:00, date_number: 319)
making sample. (inference: False, date:2013-08-31 00:00:00, date_number: 320)
making sample. (inference: False, date:2013-09-30 00:00:00, date_number: 321)
making sample. (inference: False, date:2013-10-31 00:00:00, date_number: 322)
making sample. (inference: False, date:2013-11-30 00:00:00, date_number: 323)
making sample. (inference: False, date:2013-12-31 00:00:00, date_number: 324)
making sample. (inference: False, date:2014-01-31 00:00:00, date_number: 325)
making sample. (inference: False, date:2014-02-28 00:00:00, date_number: 326)
making sample. (inference: False, date:2014-03-31 00:00:00, date_number: 327)
making sample. (inference: False, date:2014-04-30 00:00:00, date_number: 328)
making sample. (inference: False, date:2014-05-31 00:00:00, date_number: 329)
making sample. (inference: False, date:2014-06-30 00:00:00, date_number: 330)
making sample. (inference: False, date:2014-07-31 00:00:00, date_number: 331)
making sample. (inference: False, date:2014-08-31 00:00:00, date_number: 332)
making sample. (inference: False, date:2014-09-30 00:00:00, date_number: 333)
making sample. (inference: False, date:2014-10-31 00:00:00, date_number: 334)
making sample. (inference: False, date:2014-11-30 00:00:00, date_number: 335)
making sample. (inference: False, date:2014-12-31 00:00:00, date_number: 336)
making sample. (inference: False, date:2015-01-31 00:00:00, date_number: 337)
making sample. (inference: False, date:2015-02-28 00:00:00, date_number: 338)
making sample. (inference: False, date:2015-03-31 00:00:00, date_number: 339)
making sample. (inference: False, date:2015-04-30 00:00:00, date_number: 340)
making sample. (inference: False, date:2015-05-31 00:00:00, date_number: 341)
making sample. (inference: False, date:2015-06-30 00:00:00, date_number: 342)
making sample. (inference: False, date:2015-07-31 00:00:00, date_number: 343)
making sample. (inference: False, date:2015-08-31 00:00:00, date_number: 344)
making sample. (inference: False, date:2015-09-30 00:00:00, date_number: 345)
making sample. (inference: False, date:2015-10-31 00:00:00, date_number: 346)
making sample. (inference: False, date:2015-11-30 00:00:00, date_number: 347)
[AttentionModel/2015-12-31 | get_logger | INFO]: ====================AttentionModel/2015-12-31 start====================
making sample. (inference: True, date:2016-01-31 00:00:00, date_number: 349)
making sample. (inference: False, date:2015-12-31 00:00:00, date_number: 348)
[AttentionModel/2016-01-31 | get_logger | INFO]: ====================AttentionModel/2016-01-31 start====================
[strategy_integration.py] set_seed(2015-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5805)training: 2/10 (0.5805)validation : 2/10 (0.5753)training: 3/10 (0.5753)validation : 3/10 (0.5753)training: 4/10 (0.5753)validation : 4/10 (0.5753)training: 5/10 (0.5753)validation : 5/10 (0.5714)training: 6/10 (0.5714)validation : 6/10 (0.5714)training: 7/10 (0.5714)validation : 7/10 (0.5714)training: 8/10 (0.5714)validation : 8/10 (0.5714)early stopping at 8 with loss 0.5714
AttentionModel-training is done: 8/10
2015-12-31 | reset count: 0 | final loss: 0.5714 at epoch 5
[strategy_integration.py] set_seed(2016-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5710)training: 2/10 (0.5710)validation : 2/10 (0.5699)training: 3/10 (0.5699)validation : 3/10 (0.5699)training: 4/10 (0.5699)validation : 4/10 (0.5699)training: 5/10 (0.5699)validation : 5/10 (0.5699)early stopping at 5 with loss 0.5699
AttentionModel-training is done: 5/10
2016-01-31 | reset count: 0 | final loss: 0.5699 at epoch 2
making sample. (inference: True, date:2016-02-29 00:00:00, date_number: 350)
making sample. (inference: False, date:2016-01-31 00:00:00, date_number: 349)
[AttentionModel/2016-02-29 | get_logger | INFO]: ====================AttentionModel/2016-02-29 start====================
making sample. (inference: True, date:2016-03-31 00:00:00, date_number: 351)
making sample. (inference: False, date:2016-02-29 00:00:00, date_number: 350)
[AttentionModel/2016-03-31 | get_logger | INFO]: ====================AttentionModel/2016-03-31 start====================
[strategy_integration.py] set_seed(2016-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5796)training: 2/10 (0.5796)validation : 2/10 (0.5748)training: 3/10 (0.5748)validation : 3/10 (0.5743)training: 4/10 (0.5743)validation : 4/10 (0.5743)training: 5/10 (0.5743)validation : 5/10 (0.5718)training: 6/10 (0.5718)validation : 6/10 (0.5718)early stopping at 6 with loss 0.5718
AttentionModel-training is done: 6/10
2016-02-29 | reset count: 0 | final loss: 0.5718 at epoch 5
[strategy_integration.py] set_seed(2016-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5696)training: 2/10 (0.5696)validation : 2/10 (0.5680)training: 3/10 (0.5680)validation : 3/10 (0.5656)training: 4/10 (0.5656)validation : 4/10 (0.5656)training: 5/10 (0.5656)validation : 5/10 (0.5656)early stopping at 5 with loss 0.5656
AttentionModel-training is done: 5/10
2016-03-31 | reset count: 0 | final loss: 0.5656 at epoch 3
making sample. (inference: True, date:2016-04-30 00:00:00, date_number: 352)
making sample. (inference: False, date:2016-03-31 00:00:00, date_number: 351)
[AttentionModel/2016-04-30 | get_logger | INFO]: ====================AttentionModel/2016-04-30 start====================
making sample. (inference: True, date:2016-05-31 00:00:00, date_number: 353)
making sample. (inference: False, date:2016-04-30 00:00:00, date_number: 352)
[AttentionModel/2016-05-31 | get_logger | INFO]: ====================AttentionModel/2016-05-31 start====================
[strategy_integration.py] set_seed(2016-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5805)training: 2/10 (0.5805)validation : 2/10 (0.5761)training: 3/10 (0.5761)validation : 3/10 (0.5761)training: 4/10 (0.5761)validation : 4/10 (0.5745)training: 5/10 (0.5745)validation : 5/10 (0.5718)training: 6/10 (0.5718)validation : 6/10 (0.5718)training: 7/10 (0.5718)validation : 7/10 (0.5718)early stopping at 7 with loss 0.5718
AttentionModel-training is done: 7/10
2016-04-30 | reset count: 0 | final loss: 0.5718 at epoch 5
[strategy_integration.py] set_seed(2016-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5745)training: 2/10 (0.5745)validation : 2/10 (0.5737)training: 3/10 (0.5737)validation : 3/10 (0.5737)training: 4/10 (0.5737)validation : 4/10 (0.5728)training: 5/10 (0.5728)validation : 5/10 (0.5716)training: 6/10 (0.5716)validation : 6/10 (0.5716)early stopping at 6 with loss 0.5716
AttentionModel-training is done: 6/10
2016-05-31 | reset count: 0 | final loss: 0.5716 at epoch 5
making sample. (inference: True, date:2016-06-30 00:00:00, date_number: 354)
making sample. (inference: False, date:2016-05-31 00:00:00, date_number: 353)
[AttentionModel/2016-06-30 | get_logger | INFO]: ====================AttentionModel/2016-06-30 start====================
making sample. (inference: True, date:2016-07-31 00:00:00, date_number: 355)
making sample. (inference: False, date:2016-06-30 00:00:00, date_number: 354)
[AttentionModel/2016-07-31 | get_logger | INFO]: ====================AttentionModel/2016-07-31 start====================
[strategy_integration.py] set_seed(2016-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5664)training: 2/10 (0.5664)validation : 2/10 (0.5640)training: 3/10 (0.5640)validation : 3/10 (0.5640)training: 4/10 (0.5640)validation : 4/10 (0.5640)training: 5/10 (0.5640)validation : 5/10 (0.5640)training: 6/10 (0.5640)validation : 6/10 (0.5640)early stopping at 6 with loss 0.5640
AttentionModel-training is done: 6/10
2016-06-30 | reset count: 0 | final loss: 0.5640 at epoch 4
making sample. (inference: True, date:2016-08-31 00:00:00, date_number: 356)
making sample. (inference: False, date:2016-07-31 00:00:00, date_number: 355)
[AttentionModel/2016-08-31 | get_logger | INFO]: ====================AttentionModel/2016-08-31 start====================
[strategy_integration.py] set_seed(2016-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5740)training: 2/10 (0.5740)validation : 2/10 (0.5740)training: 3/10 (0.5740)validation : 3/10 (0.5739)training: 4/10 (0.5739)validation : 4/10 (0.5701)training: 5/10 (0.5701)validation : 5/10 (0.5701)training: 6/10 (0.5701)validation : 6/10 (0.5682)training: 7/10 (0.5682)validation : 7/10 (0.5682)early stopping at 7 with loss 0.5682
AttentionModel-training is done: 7/10
2016-07-31 | reset count: 0 | final loss: 0.5682 at epoch 6
making sample. (inference: True, date:2016-09-30 00:00:00, date_number: 357)
making sample. (inference: False, date:2016-08-31 00:00:00, date_number: 356)
[AttentionModel/2016-09-30 | get_logger | INFO]: ====================AttentionModel/2016-09-30 start====================
[strategy_integration.py] set_seed(2016-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5742)training: 2/10 (0.5742)validation : 2/10 (0.5696)training: 3/10 (0.5696)validation : 3/10 (0.5696)training: 4/10 (0.5696)validation : 4/10 (0.5696)training: 5/10 (0.5696)validation : 5/10 (0.5696)training: 6/10 (0.5696)validation : 6/10 (0.5696)training: 7/10 (0.5696)validation : 7/10 (0.5696)early stopping at 7 with loss 0.5696
AttentionModel-training is done: 7/10
2016-08-31 | reset count: 0 | final loss: 0.5696 at epoch 2
making sample. (inference: True, date:2016-10-31 00:00:00, date_number: 358)
making sample. (inference: False, date:2016-09-30 00:00:00, date_number: 357)
[AttentionModel/2016-10-31 | get_logger | INFO]: ====================AttentionModel/2016-10-31 start====================
[strategy_integration.py] set_seed(2016-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5754)training: 2/10 (0.5754)validation : 2/10 (0.5712)training: 3/10 (0.5712)validation : 3/10 (0.5712)training: 4/10 (0.5712)validation : 4/10 (0.5712)training: 5/10 (0.5712)validation : 5/10 (0.5712)training: 6/10 (0.5712)validation : 6/10 (0.5695)training: 7/10 (0.5695)validation : 7/10 (0.5695)early stopping at 7 with loss 0.5695
AttentionModel-training is done: 7/10
2016-09-30 | reset count: 0 | final loss: 0.5695 at epoch 6
making sample. (inference: True, date:2016-11-30 00:00:00, date_number: 359)
making sample. (inference: False, date:2016-10-31 00:00:00, date_number: 358)
[AttentionModel/2016-11-30 | get_logger | INFO]: ====================AttentionModel/2016-11-30 start====================
[strategy_integration.py] set_seed(2016-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5770)training: 2/10 (0.5770)validation : 2/10 (0.5748)training: 3/10 (0.5748)validation : 3/10 (0.5748)training: 4/10 (0.5748)validation : 4/10 (0.5717)training: 5/10 (0.5717)validation : 5/10 (0.5717)training: 6/10 (0.5717)validation : 6/10 (0.5717)training: 7/10 (0.5717)validation : 7/10 (0.5717)early stopping at 7 with loss 0.5717
AttentionModel-training is done: 7/10
2016-10-31 | reset count: 0 | final loss: 0.5717 at epoch 4
[strategy_integration.py] set_seed(2016-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5625)training: 2/10 (0.5625)validation : 2/10 (0.5625)training: 3/10 (0.5625)validation : 3/10 (0.5608)training: 4/10 (0.5608)validation : 4/10 (0.5594)training: 5/10 (0.5594)validation : 5/10 (0.5594)training: 6/10 (0.5594)validation : 6/10 (0.5594)training: 7/10 (0.5594)validation : 7/10 (0.5594)early stopping at 7 with loss 0.5594
AttentionModel-training is done: 7/10
2016-11-30 | reset count: 0 | final loss: 0.5594 at epoch 4
making sample. (inference: True, date:2016-12-31 00:00:00, date_number: 360)
making sample. (inference: False, date:2016-11-30 00:00:00, date_number: 359)
[AttentionModel/2016-12-31 | get_logger | INFO]: ====================AttentionModel/2016-12-31 start====================
making sample. (inference: True, date:2017-01-31 00:00:00, date_number: 361)
making sample. (inference: False, date:2016-12-31 00:00:00, date_number: 360)
[AttentionModel/2017-01-31 | get_logger | INFO]: ====================AttentionModel/2017-01-31 start====================
[strategy_integration.py] set_seed(2016-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5688)training: 2/10 (0.5688)validation : 2/10 (0.5658)training: 3/10 (0.5658)validation : 3/10 (0.5658)training: 4/10 (0.5658)validation : 4/10 (0.5658)training: 5/10 (0.5658)validation : 5/10 (0.5642)training: 6/10 (0.5642)validation : 6/10 (0.5642)training: 7/10 (0.5642)validation : 7/10 (0.5642)training: 8/10 (0.5642)validation : 8/10 (0.5642)early stopping at 8 with loss 0.5642
AttentionModel-training is done: 8/10
2016-12-31 | reset count: 0 | final loss: 0.5642 at epoch 5
[strategy_integration.py] set_seed(2017-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5695)training: 2/10 (0.5695)validation : 2/10 (0.5665)training: 3/10 (0.5665)validation : 3/10 (0.5665)training: 4/10 (0.5665)validation : 4/10 (0.5665)training: 5/10 (0.5665)validation : 5/10 (0.5652)training: 6/10 (0.5652)validation : 6/10 (0.5652)training: 7/10 (0.5652)validation : 7/10 (0.5652)training: 8/10 (0.5652)validation : 8/10 (0.5652)early stopping at 8 with loss 0.5652
AttentionModel-training is done: 8/10
2017-01-31 | reset count: 0 | final loss: 0.5652 at epoch 5
making sample. (inference: True, date:2017-02-28 00:00:00, date_number: 362)
making sample. (inference: False, date:2017-01-31 00:00:00, date_number: 361)
[AttentionModel/2017-02-28 | get_logger | INFO]: ====================AttentionModel/2017-02-28 start====================
making sample. (inference: True, date:2017-03-31 00:00:00, date_number: 363)
making sample. (inference: False, date:2017-02-28 00:00:00, date_number: 362)
[AttentionModel/2017-03-31 | get_logger | INFO]: ====================AttentionModel/2017-03-31 start====================
[strategy_integration.py] set_seed(2017-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5703)training: 2/10 (0.5703)validation : 2/10 (0.5679)training: 3/10 (0.5679)validation : 3/10 (0.5679)training: 4/10 (0.5679)validation : 4/10 (0.5674)training: 5/10 (0.5674)validation : 5/10 (0.5674)training: 6/10 (0.5674)validation : 6/10 (0.5674)early stopping at 6 with loss 0.5674
AttentionModel-training is done: 6/10
2017-02-28 | reset count: 0 | final loss: 0.5674 at epoch 4
[strategy_integration.py] set_seed(2017-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5623)training: 2/10 (0.5623)validation : 2/10 (0.5623)training: 3/10 (0.5623)validation : 3/10 (0.5601)training: 4/10 (0.5601)validation : 4/10 (0.5601)training: 5/10 (0.5601)validation : 5/10 (0.5601)early stopping at 5 with loss 0.5601
AttentionModel-training is done: 5/10
2017-03-31 | reset count: 0 | final loss: 0.5601 at epoch 3
making sample. (inference: True, date:2017-04-30 00:00:00, date_number: 364)
making sample. (inference: False, date:2017-03-31 00:00:00, date_number: 363)
[AttentionModel/2017-04-30 | get_logger | INFO]: ====================AttentionModel/2017-04-30 start====================
making sample. (inference: True, date:2017-05-31 00:00:00, date_number: 365)
making sample. (inference: False, date:2017-04-30 00:00:00, date_number: 364)
[AttentionModel/2017-05-31 | get_logger | INFO]: ====================AttentionModel/2017-05-31 start====================
[strategy_integration.py] set_seed(2017-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5610)training: 2/10 (0.5610)validation : 2/10 (0.5610)training: 3/10 (0.5610)validation : 3/10 (0.5610)training: 4/10 (0.5610)validation : 4/10 (0.5532)training: 5/10 (0.5532)validation : 5/10 (0.5532)training: 6/10 (0.5532)validation : 6/10 (0.5532)training: 7/10 (0.5532)validation : 7/10 (0.5532)early stopping at 7 with loss 0.5532
AttentionModel-training is done: 7/10
2017-04-30 | reset count: 0 | final loss: 0.5532 at epoch 4
[strategy_integration.py] set_seed(2017-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5625)training: 2/10 (0.5625)validation : 2/10 (0.5572)training: 3/10 (0.5572)validation : 3/10 (0.5572)training: 4/10 (0.5572)validation : 4/10 (0.5554)training: 5/10 (0.5554)validation : 5/10 (0.5554)training: 6/10 (0.5554)validation : 6/10 (0.5554)training: 7/10 (0.5554)validation : 7/10 (0.5554)early stopping at 7 with loss 0.5554
AttentionModel-training is done: 7/10
2017-05-31 | reset count: 0 | final loss: 0.5554 at epoch 4
making sample. (inference: True, date:2017-06-30 00:00:00, date_number: 366)
making sample. (inference: False, date:2017-05-31 00:00:00, date_number: 365)
[AttentionModel/2017-06-30 | get_logger | INFO]: ====================AttentionModel/2017-06-30 start====================
making sample. (inference: True, date:2017-07-31 00:00:00, date_number: 367)
making sample. (inference: False, date:2017-06-30 00:00:00, date_number: 366)
[AttentionModel/2017-07-31 | get_logger | INFO]: ====================AttentionModel/2017-07-31 start====================
[strategy_integration.py] set_seed(2017-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5668)training: 2/10 (0.5668)validation : 2/10 (0.5668)training: 3/10 (0.5668)validation : 3/10 (0.5668)training: 4/10 (0.5668)validation : 4/10 (0.5664)training: 5/10 (0.5664)validation : 5/10 (0.5658)training: 6/10 (0.5658)validation : 6/10 (0.5652)training: 7/10 (0.5652)validation : 7/10 (0.5652)early stopping at 7 with loss 0.5652
AttentionModel-training is done: 7/10
2017-06-30 | reset count: 0 | final loss: 0.5652 at epoch 6
making sample. (inference: True, date:2017-08-31 00:00:00, date_number: 368)
making sample. (inference: False, date:2017-07-31 00:00:00, date_number: 367)
[AttentionModel/2017-08-31 | get_logger | INFO]: ====================AttentionModel/2017-08-31 start====================
[strategy_integration.py] set_seed(2017-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5669)training: 2/10 (0.5669)validation : 2/10 (0.5669)training: 3/10 (0.5669)validation : 3/10 (0.5669)training: 4/10 (0.5669)validation : 4/10 (0.5658)training: 5/10 (0.5658)validation : 5/10 (0.5633)training: 6/10 (0.5633)validation : 6/10 (0.5633)training: 7/10 (0.5633)validation : 7/10 (0.5633)training: 8/10 (0.5633)validation : 8/10 (0.5617)training: 9/10 (0.5617)validation : 9/10 (0.5617)training: 10/10 (0.5617)validation : 10/10 (0.5617)early stopping at 10 with loss 0.5617
AttentionModel-training is done: 10/10
2017-07-31 | reset count: 0 | final loss: 0.5617 at epoch 8
making sample. (inference: True, date:2017-09-30 00:00:00, date_number: 369)
making sample. (inference: False, date:2017-08-31 00:00:00, date_number: 368)
[AttentionModel/2017-09-30 | get_logger | INFO]: ====================AttentionModel/2017-09-30 start====================
[strategy_integration.py] set_seed(2017-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5717)training: 2/10 (0.5717)validation : 2/10 (0.5717)training: 3/10 (0.5717)validation : 3/10 (0.5717)training: 4/10 (0.5717)validation : 4/10 (0.5717)training: 5/10 (0.5717)validation : 5/10 (0.5717)early stopping at 5 with loss 0.5717
AttentionModel-training is done: 5/10
2017-08-31 | reset count: 0 | final loss: 0.5717 at epoch 1
making sample. (inference: True, date:2017-10-31 00:00:00, date_number: 370)
making sample. (inference: False, date:2017-09-30 00:00:00, date_number: 369)
[AttentionModel/2017-10-31 | get_logger | INFO]: ====================AttentionModel/2017-10-31 start====================
[strategy_integration.py] set_seed(2017-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5690)training: 2/10 (0.5690)validation : 2/10 (0.5662)training: 3/10 (0.5662)validation : 3/10 (0.5654)training: 4/10 (0.5654)validation : 4/10 (0.5648)training: 5/10 (0.5648)validation : 5/10 (0.5647)training: 6/10 (0.5647)validation : 6/10 (0.5597)training: 7/10 (0.5597)validation : 7/10 (0.5597)training: 8/10 (0.5597)validation : 8/10 (0.5597)training: 9/10 (0.5597)validation : 9/10 (0.5597)early stopping at 9 with loss 0.5597
AttentionModel-training is done: 9/10
2017-09-30 | reset count: 0 | final loss: 0.5597 at epoch 6
[strategy_integration.py] set_seed(2017-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5687)training: 2/10 (0.5687)validation : 2/10 (0.5687)training: 3/10 (0.5687)validation : 3/10 (0.5686)training: 4/10 (0.5686)validation : 4/10 (0.5686)training: 5/10 (0.5686)validation : 5/10 (0.5676)early stopping at 5 with loss 0.5676
AttentionModel-training is done: 5/10
2017-10-31 | reset count: 0 | final loss: 0.5676 at epoch 5
making sample. (inference: True, date:2017-11-30 00:00:00, date_number: 371)
making sample. (inference: False, date:2017-10-31 00:00:00, date_number: 370)
[AttentionModel/2017-11-30 | get_logger | INFO]: ====================AttentionModel/2017-11-30 start====================
making sample. (inference: True, date:2017-12-31 00:00:00, date_number: 372)
making sample. (inference: False, date:2017-11-30 00:00:00, date_number: 371)
[AttentionModel/2017-12-31 | get_logger | INFO]: ====================AttentionModel/2017-12-31 start====================
[strategy_integration.py] set_seed(2017-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5575)training: 2/10 (0.5575)validation : 2/10 (0.5575)training: 3/10 (0.5575)validation : 3/10 (0.5575)training: 4/10 (0.5575)validation : 4/10 (0.5575)training: 5/10 (0.5575)validation : 5/10 (0.5568)training: 6/10 (0.5568)validation : 6/10 (0.5568)training: 7/10 (0.5568)validation : 7/10 (0.5568)early stopping at 7 with loss 0.5568
AttentionModel-training is done: 7/10
2017-11-30 | reset count: 0 | final loss: 0.5568 at epoch 5
[strategy_integration.py] set_seed(2017-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5638)training: 2/10 (0.5638)validation : 2/10 (0.5620)training: 3/10 (0.5620)validation : 3/10 (0.5596)training: 4/10 (0.5596)validation : 4/10 (0.5596)training: 5/10 (0.5596)validation : 5/10 (0.5596)training: 6/10 (0.5596)validation : 6/10 (0.5596)early stopping at 6 with loss 0.5596
AttentionModel-training is done: 6/10
2017-12-31 | reset count: 0 | final loss: 0.5596 at epoch 3
making sample. (inference: True, date:2018-01-31 00:00:00, date_number: 373)
making sample. (inference: False, date:2017-12-31 00:00:00, date_number: 372)
[AttentionModel/2018-01-31 | get_logger | INFO]: ====================AttentionModel/2018-01-31 start====================
making sample. (inference: True, date:2018-02-28 00:00:00, date_number: 374)
making sample. (inference: False, date:2018-01-31 00:00:00, date_number: 373)
[AttentionModel/2018-02-28 | get_logger | INFO]: ====================AttentionModel/2018-02-28 start====================
[strategy_integration.py] set_seed(2018-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5658)training: 2/10 (0.5658)validation : 2/10 (0.5647)training: 3/10 (0.5647)validation : 3/10 (0.5614)training: 4/10 (0.5614)validation : 4/10 (0.5578)training: 5/10 (0.5578)validation : 5/10 (0.5578)training: 6/10 (0.5578)validation : 6/10 (0.5578)training: 7/10 (0.5578)validation : 7/10 (0.5578)early stopping at 7 with loss 0.5578
AttentionModel-training is done: 7/10
2018-01-31 | reset count: 0 | final loss: 0.5578 at epoch 4
[strategy_integration.py] set_seed(2018-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5671)training: 2/10 (0.5671)validation : 2/10 (0.5654)training: 3/10 (0.5654)validation : 3/10 (0.5654)training: 4/10 (0.5654)validation : 4/10 (0.5649)training: 5/10 (0.5649)validation : 5/10 (0.5649)training: 6/10 (0.5649)validation : 6/10 (0.5649)early stopping at 6 with loss 0.5649
AttentionModel-training is done: 6/10
2018-02-28 | reset count: 0 | final loss: 0.5649 at epoch 4
making sample. (inference: True, date:2018-03-31 00:00:00, date_number: 375)
making sample. (inference: False, date:2018-02-28 00:00:00, date_number: 374)
[AttentionModel/2018-03-31 | get_logger | INFO]: ====================AttentionModel/2018-03-31 start====================
making sample. (inference: True, date:2018-04-30 00:00:00, date_number: 376)
making sample. (inference: False, date:2018-03-31 00:00:00, date_number: 375)
[AttentionModel/2018-04-30 | get_logger | INFO]: ====================AttentionModel/2018-04-30 start====================
[strategy_integration.py] set_seed(2018-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5617)training: 2/10 (0.5617)validation : 2/10 (0.5617)training: 3/10 (0.5617)validation : 3/10 (0.5616)training: 4/10 (0.5616)validation : 4/10 (0.5616)training: 5/10 (0.5616)validation : 5/10 (0.5616)early stopping at 5 with loss 0.5616
AttentionModel-training is done: 5/10
2018-03-31 | reset count: 0 | final loss: 0.5616 at epoch 3
[strategy_integration.py] set_seed(2018-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5676)training: 2/10 (0.5676)validation : 2/10 (0.5676)training: 3/10 (0.5676)validation : 3/10 (0.5636)training: 4/10 (0.5636)validation : 4/10 (0.5636)training: 5/10 (0.5636)validation : 5/10 (0.5636)early stopping at 5 with loss 0.5636
AttentionModel-training is done: 5/10
2018-04-30 | reset count: 0 | final loss: 0.5636 at epoch 3
making sample. (inference: True, date:2018-05-31 00:00:00, date_number: 377)
making sample. (inference: False, date:2018-04-30 00:00:00, date_number: 376)
[AttentionModel/2018-05-31 | get_logger | INFO]: ====================AttentionModel/2018-05-31 start====================
making sample. (inference: True, date:2018-06-30 00:00:00, date_number: 378)
making sample. (inference: False, date:2018-05-31 00:00:00, date_number: 377)
[AttentionModel/2018-06-30 | get_logger | INFO]: ====================AttentionModel/2018-06-30 start====================
[strategy_integration.py] set_seed(2018-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5642)training: 2/10 (0.5642)validation : 2/10 (0.5610)training: 3/10 (0.5610)validation : 3/10 (0.5610)training: 4/10 (0.5610)validation : 4/10 (0.5584)training: 5/10 (0.5584)validation : 5/10 (0.5584)training: 6/10 (0.5584)validation : 6/10 (0.5584)training: 7/10 (0.5584)validation : 7/10 (0.5584)early stopping at 7 with loss 0.5584
AttentionModel-training is done: 7/10
2018-05-31 | reset count: 0 | final loss: 0.5584 at epoch 4
[strategy_integration.py] set_seed(2018-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5726)training: 2/10 (0.5726)validation : 2/10 (0.5726)training: 3/10 (0.5726)validation : 3/10 (0.5726)training: 4/10 (0.5726)validation : 4/10 (0.5726)training: 5/10 (0.5726)validation : 5/10 (0.5726)training: 6/10 (0.5726)validation : 6/10 (0.5726)early stopping at 6 with loss 0.5726
AttentionModel-training is done: 6/10
2018-06-30 | reset count: 0 | final loss: 0.5726 at epoch 1
making sample. (inference: True, date:2018-07-31 00:00:00, date_number: 379)
making sample. (inference: False, date:2018-06-30 00:00:00, date_number: 378)
[AttentionModel/2018-07-31 | get_logger | INFO]: ====================AttentionModel/2018-07-31 start====================
making sample. (inference: True, date:2018-08-31 00:00:00, date_number: 380)
making sample. (inference: False, date:2018-07-31 00:00:00, date_number: 379)
[AttentionModel/2018-08-31 | get_logger | INFO]: ====================AttentionModel/2018-08-31 start====================
[strategy_integration.py] set_seed(2018-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5678)training: 2/10 (0.5678)validation : 2/10 (0.5650)training: 3/10 (0.5650)validation : 3/10 (0.5650)training: 4/10 (0.5650)validation : 4/10 (0.5650)training: 5/10 (0.5650)validation : 5/10 (0.5650)early stopping at 5 with loss 0.5650
AttentionModel-training is done: 5/10
2018-07-31 | reset count: 0 | final loss: 0.5650 at epoch 2
making sample. (inference: True, date:2018-09-30 00:00:00, date_number: 381)
making sample. (inference: False, date:2018-08-31 00:00:00, date_number: 380)
[AttentionModel/2018-09-30 | get_logger | INFO]: ====================AttentionModel/2018-09-30 start====================
[strategy_integration.py] set_seed(2018-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5714)training: 2/10 (0.5714)validation : 2/10 (0.5690)training: 3/10 (0.5690)validation : 3/10 (0.5685)training: 4/10 (0.5685)validation : 4/10 (0.5638)training: 5/10 (0.5638)validation : 5/10 (0.5634)training: 6/10 (0.5634)validation : 6/10 (0.5634)training: 7/10 (0.5634)validation : 7/10 (0.5634)early stopping at 7 with loss 0.5634
AttentionModel-training is done: 7/10
2018-08-31 | reset count: 0 | final loss: 0.5634 at epoch 5
making sample. (inference: True, date:2018-10-31 00:00:00, date_number: 382)
making sample. (inference: False, date:2018-09-30 00:00:00, date_number: 381)
[AttentionModel/2018-10-31 | get_logger | INFO]: ====================AttentionModel/2018-10-31 start====================
[strategy_integration.py] set_seed(2018-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5790)training: 2/10 (0.5790)validation : 2/10 (0.5745)training: 3/10 (0.5745)validation : 3/10 (0.5745)training: 4/10 (0.5745)validation : 4/10 (0.5745)training: 5/10 (0.5745)validation : 5/10 (0.5745)early stopping at 5 with loss 0.5745
AttentionModel-training is done: 5/10
2018-09-30 | reset count: 0 | final loss: 0.5745 at epoch 2
making sample. (inference: True, date:2018-11-30 00:00:00, date_number: 383)
making sample. (inference: False, date:2018-10-31 00:00:00, date_number: 382)
[AttentionModel/2018-11-30 | get_logger | INFO]: ====================AttentionModel/2018-11-30 start====================
[strategy_integration.py] set_seed(2018-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5792)training: 2/10 (0.5792)validation : 2/10 (0.5788)training: 3/10 (0.5788)validation : 3/10 (0.5697)training: 4/10 (0.5697)validation : 4/10 (0.5697)training: 5/10 (0.5697)validation : 5/10 (0.5697)training: 6/10 (0.5697)validation : 6/10 (0.5697)early stopping at 6 with loss 0.5697
AttentionModel-training is done: 6/10
2018-10-31 | reset count: 0 | final loss: 0.5697 at epoch 3
[strategy_integration.py] set_seed(2018-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5717)training: 2/10 (0.5717)validation : 2/10 (0.5717)training: 3/10 (0.5717)validation : 3/10 (0.5698)training: 4/10 (0.5698)validation : 4/10 (0.5698)training: 5/10 (0.5698)validation : 5/10 (0.5698)early stopping at 5 with loss 0.5698
AttentionModel-training is done: 5/10
2018-11-30 | reset count: 0 | final loss: 0.5698 at epoch 3
making sample. (inference: True, date:2018-12-31 00:00:00, date_number: 384)
making sample. (inference: False, date:2018-11-30 00:00:00, date_number: 383)
[AttentionModel/2018-12-31 | get_logger | INFO]: ====================AttentionModel/2018-12-31 start====================
making sample. (inference: True, date:2019-01-31 00:00:00, date_number: 385)
making sample. (inference: False, date:2018-12-31 00:00:00, date_number: 384)
[AttentionModel/2019-01-31 | get_logger | INFO]: ====================AttentionModel/2019-01-31 start====================
[strategy_integration.py] set_seed(2018-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5656)training: 2/10 (0.5656)validation : 2/10 (0.5648)training: 3/10 (0.5648)validation : 3/10 (0.5648)training: 4/10 (0.5648)validation : 4/10 (0.5648)training: 5/10 (0.5648)validation : 5/10 (0.5641)early stopping at 5 with loss 0.5641
AttentionModel-training is done: 5/10
2018-12-31 | reset count: 0 | final loss: 0.5641 at epoch 5
making sample. (inference: True, date:2019-02-28 00:00:00, date_number: 386)
making sample. (inference: False, date:2019-01-31 00:00:00, date_number: 385)
[AttentionModel/2019-02-28 | get_logger | INFO]: ====================AttentionModel/2019-02-28 start====================
[strategy_integration.py] set_seed(2019-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5835)training: 2/10 (0.5835)validation : 2/10 (0.5827)training: 3/10 (0.5827)validation : 3/10 (0.5811)training: 4/10 (0.5811)validation : 4/10 (0.5768)training: 5/10 (0.5768)validation : 5/10 (0.5768)training: 6/10 (0.5768)validation : 6/10 (0.5768)training: 7/10 (0.5768)validation : 7/10 (0.5768)early stopping at 7 with loss 0.5768
AttentionModel-training is done: 7/10
2019-01-31 | reset count: 0 | final loss: 0.5768 at epoch 4
making sample. (inference: True, date:2019-03-31 00:00:00, date_number: 387)
making sample. (inference: False, date:2019-02-28 00:00:00, date_number: 386)
[AttentionModel/2019-03-31 | get_logger | INFO]: ====================AttentionModel/2019-03-31 start====================
[strategy_integration.py] set_seed(2019-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5870)training: 2/10 (0.5870)validation : 2/10 (0.5857)training: 3/10 (0.5857)validation : 3/10 (0.5835)training: 4/10 (0.5835)validation : 4/10 (0.5835)training: 5/10 (0.5835)validation : 5/10 (0.5825)training: 6/10 (0.5825)validation : 6/10 (0.5825)early stopping at 6 with loss 0.5825
AttentionModel-training is done: 6/10
2019-02-28 | reset count: 0 | final loss: 0.5825 at epoch 5
making sample. (inference: True, date:2019-04-30 00:00:00, date_number: 388)
making sample. (inference: False, date:2019-03-31 00:00:00, date_number: 387)
[AttentionModel/2019-04-30 | get_logger | INFO]: ====================AttentionModel/2019-04-30 start====================
[strategy_integration.py] set_seed(2019-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5862)training: 2/10 (0.5862)validation : 2/10 (0.5813)training: 3/10 (0.5813)validation : 3/10 (0.5790)training: 4/10 (0.5790)validation : 4/10 (0.5790)training: 5/10 (0.5790)validation : 5/10 (0.5790)training: 6/10 (0.5790)validation : 6/10 (0.5790)early stopping at 6 with loss 0.5790
AttentionModel-training is done: 6/10
2019-03-31 | reset count: 0 | final loss: 0.5790 at epoch 3
making sample. (inference: True, date:2019-05-31 00:00:00, date_number: 389)
making sample. (inference: False, date:2019-04-30 00:00:00, date_number: 388)
[AttentionModel/2019-05-31 | get_logger | INFO]: ====================AttentionModel/2019-05-31 start====================
[strategy_integration.py] set_seed(2019-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5797)training: 2/10 (0.5797)validation : 2/10 (0.5791)training: 3/10 (0.5791)validation : 3/10 (0.5780)training: 4/10 (0.5780)validation : 4/10 (0.5772)training: 5/10 (0.5772)validation : 5/10 (0.5772)training: 6/10 (0.5772)validation : 6/10 (0.5772)early stopping at 6 with loss 0.5772
AttentionModel-training is done: 6/10
2019-04-30 | reset count: 0 | final loss: 0.5772 at epoch 4
making sample. (inference: True, date:2019-06-30 00:00:00, date_number: 390)
making sample. (inference: False, date:2019-05-31 00:00:00, date_number: 389)
[AttentionModel/2019-06-30 | get_logger | INFO]: ====================AttentionModel/2019-06-30 start====================
[strategy_integration.py] set_seed(2019-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5766)training: 2/10 (0.5766)validation : 2/10 (0.5766)training: 3/10 (0.5766)validation : 3/10 (0.5766)training: 4/10 (0.5766)validation : 4/10 (0.5766)training: 5/10 (0.5766)validation : 5/10 (0.5766)early stopping at 5 with loss 0.5766
AttentionModel-training is done: 5/10
2019-05-31 | reset count: 0 | final loss: 0.5766 at epoch 1
making sample. (inference: True, date:2019-07-31 00:00:00, date_number: 391)
making sample. (inference: False, date:2019-06-30 00:00:00, date_number: 390)
[AttentionModel/2019-07-31 | get_logger | INFO]: ====================AttentionModel/2019-07-31 start====================
[strategy_integration.py] set_seed(2019-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5824)training: 2/10 (0.5824)validation : 2/10 (0.5796)training: 3/10 (0.5796)validation : 3/10 (0.5780)training: 4/10 (0.5780)validation : 4/10 (0.5767)training: 5/10 (0.5767)validation : 5/10 (0.5766)training: 6/10 (0.5766)validation : 6/10 (0.5766)early stopping at 6 with loss 0.5766
AttentionModel-training is done: 6/10
2019-06-30 | reset count: 0 | final loss: 0.5766 at epoch 5
making sample. (inference: True, date:2019-08-31 00:00:00, date_number: 392)
making sample. (inference: False, date:2019-07-31 00:00:00, date_number: 391)
[AttentionModel/2019-08-31 | get_logger | INFO]: ====================AttentionModel/2019-08-31 start====================
[strategy_integration.py] set_seed(2019-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5824)training: 2/10 (0.5824)validation : 2/10 (0.5806)training: 3/10 (0.5806)validation : 3/10 (0.5802)training: 4/10 (0.5802)validation : 4/10 (0.5801)training: 5/10 (0.5801)validation : 5/10 (0.5791)training: 6/10 (0.5791)validation : 6/10 (0.5791)early stopping at 6 with loss 0.5791
AttentionModel-training is done: 6/10
2019-07-31 | reset count: 0 | final loss: 0.5791 at epoch 5
making sample. (inference: True, date:2019-09-30 00:00:00, date_number: 393)
making sample. (inference: False, date:2019-08-31 00:00:00, date_number: 392)
[AttentionModel/2019-09-30 | get_logger | INFO]: ====================AttentionModel/2019-09-30 start====================
[strategy_integration.py] set_seed(2019-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5895)training: 2/10 (0.5895)validation : 2/10 (0.5890)training: 3/10 (0.5890)validation : 3/10 (0.5851)training: 4/10 (0.5851)validation : 4/10 (0.5851)training: 5/10 (0.5851)validation : 5/10 (0.5851)training: 6/10 (0.5851)validation : 6/10 (0.5851)early stopping at 6 with loss 0.5851
AttentionModel-training is done: 6/10
2019-08-31 | reset count: 0 | final loss: 0.5851 at epoch 3
[strategy_integration.py] set_seed(2019-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5805)training: 2/10 (0.5805)validation : 2/10 (0.5805)training: 3/10 (0.5805)validation : 3/10 (0.5778)training: 4/10 (0.5778)validation : 4/10 (0.5778)training: 5/10 (0.5778)validation : 5/10 (0.5778)early stopping at 5 with loss 0.5778
AttentionModel-training is done: 5/10
2019-09-30 | reset count: 0 | final loss: 0.5778 at epoch 3
making sample. (inference: True, date:2019-10-31 00:00:00, date_number: 394)
making sample. (inference: False, date:2019-09-30 00:00:00, date_number: 393)
[AttentionModel/2019-10-31 | get_logger | INFO]: ====================AttentionModel/2019-10-31 start====================
making sample. (inference: True, date:2019-11-30 00:00:00, date_number: 395)
making sample. (inference: False, date:2019-10-31 00:00:00, date_number: 394)
[AttentionModel/2019-11-30 | get_logger | INFO]: ====================AttentionModel/2019-11-30 start====================
[strategy_integration.py] set_seed(2019-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5832)training: 2/10 (0.5832)validation : 2/10 (0.5824)training: 3/10 (0.5824)validation : 3/10 (0.5824)training: 4/10 (0.5824)validation : 4/10 (0.5824)training: 5/10 (0.5824)validation : 5/10 (0.5824)early stopping at 5 with loss 0.5824
AttentionModel-training is done: 5/10
2019-10-31 | reset count: 0 | final loss: 0.5824 at epoch 2
[strategy_integration.py] set_seed(2019-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5908)training: 2/10 (0.5908)validation : 2/10 (0.5902)training: 3/10 (0.5902)validation : 3/10 (0.5877)training: 4/10 (0.5877)validation : 4/10 (0.5875)training: 5/10 (0.5875)validation : 5/10 (0.5875)early stopping at 5 with loss 0.5875
AttentionModel-training is done: 5/10
2019-11-30 | reset count: 0 | final loss: 0.5875 at epoch 4
making sample. (inference: True, date:2019-12-31 00:00:00, date_number: 396)
making sample. (inference: False, date:2019-11-30 00:00:00, date_number: 395)
[AttentionModel/2019-12-31 | get_logger | INFO]: ====================AttentionModel/2019-12-31 start====================
making sample. (inference: True, date:2020-01-31 00:00:00, date_number: 397)
making sample. (inference: False, date:2019-12-31 00:00:00, date_number: 396)
[AttentionModel/2020-01-31 | get_logger | INFO]: ====================AttentionModel/2020-01-31 start====================
[strategy_integration.py] set_seed(2019-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5762)training: 2/10 (0.5762)validation : 2/10 (0.5762)training: 3/10 (0.5762)validation : 3/10 (0.5762)training: 4/10 (0.5762)validation : 4/10 (0.5762)training: 5/10 (0.5762)validation : 5/10 (0.5762)early stopping at 5 with loss 0.5762
AttentionModel-training is done: 5/10
2019-12-31 | reset count: 0 | final loss: 0.5762 at epoch 1
making sample. (inference: True, date:2020-02-29 00:00:00, date_number: 398)
making sample. (inference: False, date:2020-01-31 00:00:00, date_number: 397)
[AttentionModel/2020-02-29 | get_logger | INFO]: ====================AttentionModel/2020-02-29 start====================
[strategy_integration.py] set_seed(2020-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5889)training: 2/10 (0.5889)validation : 2/10 (0.5889)training: 3/10 (0.5889)validation : 3/10 (0.5888)training: 4/10 (0.5888)validation : 4/10 (0.5882)training: 5/10 (0.5882)validation : 5/10 (0.5874)training: 6/10 (0.5874)validation : 6/10 (0.5862)training: 7/10 (0.5862)validation : 7/10 (0.5862)early stopping at 7 with loss 0.5862
AttentionModel-training is done: 7/10
2020-01-31 | reset count: 0 | final loss: 0.5862 at epoch 6
making sample. (inference: True, date:2020-03-31 00:00:00, date_number: 399)
making sample. (inference: False, date:2020-02-29 00:00:00, date_number: 398)
[AttentionModel/2020-03-31 | get_logger | INFO]: ====================AttentionModel/2020-03-31 start====================
[strategy_integration.py] set_seed(2020-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5845)training: 2/10 (0.5845)validation : 2/10 (0.5830)training: 3/10 (0.5830)validation : 3/10 (0.5830)training: 4/10 (0.5830)validation : 4/10 (0.5830)training: 5/10 (0.5830)validation : 5/10 (0.5830)early stopping at 5 with loss 0.5830
AttentionModel-training is done: 5/10
2020-02-29 | reset count: 0 | final loss: 0.5830 at epoch 2
making sample. (inference: True, date:2020-04-30 00:00:00, date_number: 400)
making sample. (inference: False, date:2020-03-31 00:00:00, date_number: 399)
[AttentionModel/2020-04-30 | get_logger | INFO]: ====================AttentionModel/2020-04-30 start====================
[strategy_integration.py] set_seed(2020-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5761)training: 2/10 (0.5761)validation : 2/10 (0.5751)training: 3/10 (0.5751)validation : 3/10 (0.5751)training: 4/10 (0.5751)validation : 4/10 (0.5751)training: 5/10 (0.5751)validation : 5/10 (0.5751)early stopping at 5 with loss 0.5751
AttentionModel-training is done: 5/10
2020-03-31 | reset count: 0 | final loss: 0.5751 at epoch 2
making sample. (inference: True, date:2020-05-31 00:00:00, date_number: 401)
making sample. (inference: False, date:2020-04-30 00:00:00, date_number: 400)
[AttentionModel/2020-05-31 | get_logger | INFO]: ====================AttentionModel/2020-05-31 start====================
[strategy_integration.py] set_seed(2020-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5724)training: 2/10 (0.5724)validation : 2/10 (0.5683)training: 3/10 (0.5683)validation : 3/10 (0.5683)training: 4/10 (0.5683)validation : 4/10 (0.5683)training: 5/10 (0.5683)validation : 5/10 (0.5683)training: 6/10 (0.5683)validation : 6/10 (0.5683)early stopping at 6 with loss 0.5683
AttentionModel-training is done: 6/10
2020-04-30 | reset count: 0 | final loss: 0.5683 at epoch 2
making sample. (inference: True, date:2020-06-30 00:00:00, date_number: 402)
making sample. (inference: False, date:2020-05-31 00:00:00, date_number: 401)
[AttentionModel/2020-06-30 | get_logger | INFO]: ====================AttentionModel/2020-06-30 start====================
[strategy_integration.py] set_seed(2020-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5711)training: 2/10 (0.5711)validation : 2/10 (0.5707)training: 3/10 (0.5707)validation : 3/10 (0.5673)training: 4/10 (0.5673)validation : 4/10 (0.5670)training: 5/10 (0.5670)validation : 5/10 (0.5667)training: 6/10 (0.5667)validation : 6/10 (0.5667)early stopping at 6 with loss 0.5667
AttentionModel-training is done: 6/10
2020-05-31 | reset count: 0 | final loss: 0.5667 at epoch 5
making sample. (inference: True, date:2020-07-31 00:00:00, date_number: 403)
making sample. (inference: False, date:2020-06-30 00:00:00, date_number: 402)
[AttentionModel/2020-07-31 | get_logger | INFO]: ====================AttentionModel/2020-07-31 start====================
[strategy_integration.py] set_seed(2020-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5793)training: 2/10 (0.5793)validation : 2/10 (0.5748)training: 3/10 (0.5748)validation : 3/10 (0.5748)training: 4/10 (0.5748)validation : 4/10 (0.5736)training: 5/10 (0.5736)validation : 5/10 (0.5734)training: 6/10 (0.5734)validation : 6/10 (0.5734)training: 7/10 (0.5734)validation : 7/10 (0.5734)early stopping at 7 with loss 0.5734
AttentionModel-training is done: 7/10
2020-06-30 | reset count: 0 | final loss: 0.5734 at epoch 5
[strategy_integration.py] set_seed(2020-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5852)training: 2/10 (0.5852)validation : 2/10 (0.5840)training: 3/10 (0.5840)validation : 3/10 (0.5840)training: 4/10 (0.5840)validation : 4/10 (0.5830)training: 5/10 (0.5830)validation : 5/10 (0.5814)training: 6/10 (0.5814)validation : 6/10 (0.5814)training: 7/10 (0.5814)validation : 7/10 (0.5814)early stopping at 7 with loss 0.5814
AttentionModel-training is done: 7/10
2020-07-31 | reset count: 0 | final loss: 0.5814 at epoch 5
making sample. (inference: True, date:2020-08-31 00:00:00, date_number: 404)
making sample. (inference: False, date:2020-07-31 00:00:00, date_number: 403)
[AttentionModel/2020-08-31 | get_logger | INFO]: ====================AttentionModel/2020-08-31 start====================
making sample. (inference: True, date:2020-09-30 00:00:00, date_number: 405)
making sample. (inference: False, date:2020-08-31 00:00:00, date_number: 404)
[AttentionModel/2020-09-30 | get_logger | INFO]: ====================AttentionModel/2020-09-30 start====================
[strategy_integration.py] set_seed(2020-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5681)training: 2/10 (0.5681)validation : 2/10 (0.5681)training: 3/10 (0.5681)validation : 3/10 (0.5681)training: 4/10 (0.5681)validation : 4/10 (0.5681)training: 5/10 (0.5681)validation : 5/10 (0.5681)early stopping at 5 with loss 0.5681
AttentionModel-training is done: 5/10
2020-08-31 | reset count: 0 | final loss: 0.5681 at epoch 1
[strategy_integration.py] set_seed(2020-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5739)training: 2/10 (0.5739)validation : 2/10 (0.5712)training: 3/10 (0.5712)validation : 3/10 (0.5712)training: 4/10 (0.5712)validation : 4/10 (0.5712)training: 5/10 (0.5712)validation : 5/10 (0.5712)early stopping at 5 with loss 0.5712
AttentionModel-training is done: 5/10
2020-09-30 | reset count: 0 | final loss: 0.5712 at epoch 2
making sample. (inference: True, date:2020-10-31 00:00:00, date_number: 406)
making sample. (inference: False, date:2020-09-30 00:00:00, date_number: 405)
[AttentionModel/2020-10-31 | get_logger | INFO]: ====================AttentionModel/2020-10-31 start====================
making sample. (inference: True, date:2020-11-30 00:00:00, date_number: 407)
making sample. (inference: False, date:2020-10-31 00:00:00, date_number: 406)
[AttentionModel/2020-11-30 | get_logger | INFO]: ====================AttentionModel/2020-11-30 start====================
[strategy_integration.py] set_seed(2020-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5691)training: 2/10 (0.5691)validation : 2/10 (0.5691)training: 3/10 (0.5691)validation : 3/10 (0.5673)training: 4/10 (0.5673)validation : 4/10 (0.5673)training: 5/10 (0.5673)validation : 5/10 (0.5673)early stopping at 5 with loss 0.5673
AttentionModel-training is done: 5/10
2020-10-31 | reset count: 0 | final loss: 0.5673 at epoch 3
making sample. (inference: True, date:2020-12-31 00:00:00, date_number: 408)
making sample. (inference: False, date:2020-11-30 00:00:00, date_number: 407)
[AttentionModel/2020-12-31 | get_logger | INFO]: ====================AttentionModel/2020-12-31 start====================
[strategy_integration.py] set_seed(2020-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5666)training: 2/10 (0.5666)validation : 2/10 (0.5666)training: 3/10 (0.5666)validation : 3/10 (0.5666)training: 4/10 (0.5666)validation : 4/10 (0.5666)training: 5/10 (0.5666)validation : 5/10 (0.5666)early stopping at 5 with loss 0.5666
AttentionModel-training is done: 5/10
2020-11-30 | reset count: 0 | final loss: 0.5666 at epoch 1
making sample. (inference: True, date:2021-01-31 00:00:00, date_number: 409)
making sample. (inference: False, date:2020-12-31 00:00:00, date_number: 408)
[AttentionModel/2021-01-31 | get_logger | INFO]: ====================AttentionModel/2021-01-31 start====================
[strategy_integration.py] set_seed(2020-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5674)training: 2/10 (0.5674)validation : 2/10 (0.5670)training: 3/10 (0.5670)validation : 3/10 (0.5662)training: 4/10 (0.5662)validation : 4/10 (0.5662)training: 5/10 (0.5662)validation : 5/10 (0.5635)training: 6/10 (0.5635)validation : 6/10 (0.5635)early stopping at 6 with loss 0.5635
AttentionModel-training is done: 6/10
2020-12-31 | reset count: 0 | final loss: 0.5635 at epoch 5
making sample. (inference: True, date:2021-02-28 00:00:00, date_number: 410)
making sample. (inference: False, date:2021-01-31 00:00:00, date_number: 409)
[AttentionModel/2021-02-28 | get_logger | INFO]: ====================AttentionModel/2021-02-28 start====================
[strategy_integration.py] set_seed(2021-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5533)training: 2/10 (0.5533)validation : 2/10 (0.5533)training: 3/10 (0.5533)validation : 3/10 (0.5533)training: 4/10 (0.5533)validation : 4/10 (0.5533)training: 5/10 (0.5533)validation : 5/10 (0.5533)training: 6/10 (0.5533)validation : 6/10 (0.5525)training: 7/10 (0.5525)validation : 7/10 (0.5525)training: 8/10 (0.5525)validation : 8/10 (0.5525)early stopping at 8 with loss 0.5525
AttentionModel-training is done: 8/10
2021-01-31 | reset count: 0 | final loss: 0.5525 at epoch 6
[strategy_integration.py] set_seed(2021-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5614)training: 2/10 (0.5614)validation : 2/10 (0.5614)training: 3/10 (0.5614)validation : 3/10 (0.5614)training: 4/10 (0.5614)validation : 4/10 (0.5614)training: 5/10 (0.5614)validation : 5/10 (0.5614)early stopping at 5 with loss 0.5614
AttentionModel-training is done: 5/10
2021-02-28 | reset count: 0 | final loss: 0.5614 at epoch 1
making sample. (inference: True, date:2021-03-31 00:00:00, date_number: 411)
making sample. (inference: False, date:2021-02-28 00:00:00, date_number: 410)
[AttentionModel/2021-03-31 | get_logger | INFO]: ====================AttentionModel/2021-03-31 start====================
making sample. (inference: True, date:2021-04-30 00:00:00, date_number: 412)
making sample. (inference: False, date:2021-03-31 00:00:00, date_number: 411)
[AttentionModel/2021-04-30 | get_logger | INFO]: ====================AttentionModel/2021-04-30 start====================
[strategy_integration.py] set_seed(2021-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5698)training: 2/10 (0.5698)validation : 2/10 (0.5694)training: 3/10 (0.5694)validation : 3/10 (0.5651)training: 4/10 (0.5651)validation : 4/10 (0.5651)training: 5/10 (0.5651)validation : 5/10 (0.5651)training: 6/10 (0.5651)validation : 6/10 (0.5651)early stopping at 6 with loss 0.5651
AttentionModel-training is done: 6/10
2021-03-31 | reset count: 0 | final loss: 0.5651 at epoch 3
[strategy_integration.py] set_seed(2021-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5590)training: 2/10 (0.5590)validation : 2/10 (0.5590)training: 3/10 (0.5590)validation : 3/10 (0.5565)training: 4/10 (0.5565)validation : 4/10 (0.5565)training: 5/10 (0.5565)validation : 5/10 (0.5565)early stopping at 5 with loss 0.5565
AttentionModel-training is done: 5/10
2021-04-30 | reset count: 0 | final loss: 0.5565 at epoch 3
making sample. (inference: True, date:2021-05-31 00:00:00, date_number: 413)
making sample. (inference: False, date:2021-04-30 00:00:00, date_number: 412)
[AttentionModel/2021-05-31 | get_logger | INFO]: ====================AttentionModel/2021-05-31 start====================
making sample. (inference: True, date:2021-06-30 00:00:00, date_number: 414)
making sample. (inference: False, date:2021-05-31 00:00:00, date_number: 413)
[AttentionModel/2021-06-30 | get_logger | INFO]: ====================AttentionModel/2021-06-30 start====================
[strategy_integration.py] set_seed(2021-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5673)training: 2/10 (0.5673)validation : 2/10 (0.5601)training: 3/10 (0.5601)validation : 3/10 (0.5601)training: 4/10 (0.5601)validation : 4/10 (0.5601)training: 5/10 (0.5601)validation : 5/10 (0.5601)training: 6/10 (0.5601)validation : 6/10 (0.5601)training: 7/10 (0.5601)validation : 7/10 (0.5601)early stopping at 7 with loss 0.5601
AttentionModel-training is done: 7/10
2021-05-31 | reset count: 0 | final loss: 0.5601 at epoch 2
making sample. (inference: True, date:2021-07-31 00:00:00, date_number: 415)
making sample. (inference: False, date:2021-06-30 00:00:00, date_number: 414)
[strategy_integration.py] set_seed(2021-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5746)training: 2/10 (0.5746)validation : 2/10 (0.5746)training: 3/10 (0.5746)validation : 3/10 (0.5746)training: 4/10 (0.5746)validation : 4/10 (0.5738)training: 5/10 (0.5738)validation : 5/10 (0.5738)training: 6/10 (0.5738)validation : 6/10 (0.5683)training: 7/10 (0.5683)validation : 7/10 (0.5683)early stopping at 7 with loss 0.5683
AttentionModel-training is done: 7/10
2021-06-30 | reset count: 0 | final loss: 0.5683 at epoch 6
[AttentionModel/2021-07-31 | get_logger | INFO]: ====================AttentionModel/2021-07-31 start====================
making sample. (inference: True, date:2021-08-31 00:00:00, date_number: 416)
making sample. (inference: False, date:2021-07-31 00:00:00, date_number: 415)
[AttentionModel/2021-08-31 | get_logger | INFO]: ====================AttentionModel/2021-08-31 start====================
[strategy_integration.py] set_seed(2021-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5710)training: 2/10 (0.5710)validation : 2/10 (0.5655)training: 3/10 (0.5655)validation : 3/10 (0.5633)training: 4/10 (0.5633)validation : 4/10 (0.5633)training: 5/10 (0.5633)validation : 5/10 (0.5633)training: 6/10 (0.5633)validation : 6/10 (0.5633)early stopping at 6 with loss 0.5633
AttentionModel-training is done: 6/10
2021-07-31 | reset count: 0 | final loss: 0.5633 at epoch 3
making sample. (inference: True, date:2021-09-30 00:00:00, date_number: 417)
making sample. (inference: False, date:2021-08-31 00:00:00, date_number: 416)
[AttentionModel/2021-09-30 | get_logger | INFO]: ====================AttentionModel/2021-09-30 start====================
[strategy_integration.py] set_seed(2021-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5837)training: 2/10 (0.5837)validation : 2/10 (0.5770)training: 3/10 (0.5770)validation : 3/10 (0.5770)training: 4/10 (0.5770)validation : 4/10 (0.5770)training: 5/10 (0.5770)validation : 5/10 (0.5770)training: 6/10 (0.5770)validation : 6/10 (0.5770)training: 7/10 (0.5770)validation : 7/10 (0.5770)training: 8/10 (0.5770)validation : 8/10 (0.5770)early stopping at 8 with loss 0.5770
AttentionModel-training is done: 8/10
2021-08-31 | reset count: 0 | final loss: 0.5770 at epoch 2
making sample. (inference: True, date:2021-10-31 00:00:00, date_number: 418)
making sample. (inference: False, date:2021-09-30 00:00:00, date_number: 417)
[AttentionModel/2021-10-31 | get_logger | INFO]: ====================AttentionModel/2021-10-31 start====================
[strategy_integration.py] set_seed(2021-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5742)training: 2/10 (0.5742)validation : 2/10 (0.5697)training: 3/10 (0.5697)validation : 3/10 (0.5697)training: 4/10 (0.5697)validation : 4/10 (0.5697)training: 5/10 (0.5697)validation : 5/10 (0.5696)training: 6/10 (0.5696)validation : 6/10 (0.5658)training: 7/10 (0.5658)validation : 7/10 (0.5658)early stopping at 7 with loss 0.5658
AttentionModel-training is done: 7/10
2021-09-30 | reset count: 0 | final loss: 0.5658 at epoch 6
[strategy_integration.py] set_seed(2021-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5798)training: 2/10 (0.5798)validation : 2/10 (0.5736)training: 3/10 (0.5736)validation : 3/10 (0.5736)training: 4/10 (0.5736)validation : 4/10 (0.5736)training: 5/10 (0.5736)validation : 5/10 (0.5736)early stopping at 5 with loss 0.5736
AttentionModel-training is done: 5/10
2021-10-31 | reset count: 0 | final loss: 0.5736 at epoch 2
[strategy | get_logger | INFO]: ====================strategy start====================
[strategy | _get_infer_dirs | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick250_test02/infer 이 strategy 대상에 추가됨
[strategy | _backtest | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick250_test02/infer is starting
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.15 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading masking data✓ [Compustat API] : Loading get_historic_universe from cache done in 1.964 secs
⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.265 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.062 secs
⠋ [Compustat API] : Loading get_monthly_total_return from cache✓ [Compustat API] : Loading get_monthly_total_return from cache done in 0.063 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.076 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.072 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.071 secs
⠋ [Compustat API] : Loading get_monthly_volume_data from cache✓ [Compustat API] : Loading get_monthly_volume_data from cache done in 0.081 secs
[strategy | _allocating | INFO]: [STRATEGY] allocating start
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_weight_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] performance.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] combined.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] weight.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] universe.csv is saved.
[comparison | get_logger | INFO]: ====================comparison start====================
[comparison | compare | INFO]: [5. COMPARISON] comparison is saved.
[comparison | compare | INFO]:                        Harvest_Green_concept001_pick250_test02_price_return  ...  SPCOMP_total_win
2016-01-31_2021-04-30                                           0.236444     ...          0.765625

[1 rows x 27 columns]
2015-12-31
2016-01-31
2016-02-29
2016-03-31
2016-04-30
2016-05-31
2016-06-30
2016-07-31
2016-08-31
2016-09-30
2016-10-31
2016-11-30
2016-12-31
2017-01-31
2017-02-28
2017-03-31
2017-04-30
2017-05-31
2017-06-30
2017-07-31
2017-08-31
2017-09-30
2017-10-31
2017-11-30
2017-12-31
2018-01-31
2018-02-28
2018-03-31
2018-04-30
2018-05-31
2018-06-30
2018-07-31
2018-08-31
2018-09-30
2018-10-31
2018-11-30
2018-12-31
2019-01-31
2019-02-28
2019-03-31
2019-04-30
2019-05-31
2019-06-30
2019-07-31
2019-08-31
2019-09-30
2019-10-31
2019-11-30
2019-12-31
2020-01-31
2020-02-29
2020-03-31
2020-04-30
2020-05-31
2020-06-30
2020-07-31
2020-08-31
2020-09-30
2020-10-31
2020-11-30
2020-12-31
2021-01-31
2021-02-28
2021-03-31
2021-04-30
2021-05-31
2021-06-30
2021-07-31
2021-08-31
2021-09-30
2021-10-31
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.049 secs
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.046 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.065 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.08 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.075 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.678 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.677 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.682 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.685 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.685 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.078 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.672 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.672 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.671 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.666 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.665 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.08 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.682 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.077 secs
⠋ [Compustat API] : Cleaning past days cache✓ [Compustat API] : Cleaning past days cache done in 0.0 secs
auto caching이 활성화 되었습니다. /home/sronly/sr-storage/kirin_cache/kirin_api_cache/2.19.17_2021-11-23에 캐쉬를 저장합니다.
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.149 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache✓ [Compustat API] : Loading get_historic_universe from cache done in 1.998 secs
⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.299 secs
setting tensorflow random seed failed
load_data: gpa
load_data: rev_surp
load_data: cash_at
load_data: retail_mfr
load_data: m1
load_data: m2
load_data: pr_1m_0m
load_data: mv
load_data: btm
load_data: t1y_ff
load_data: t5y_ff
load_data: t10y_t2y
load_data: export_growth
load_data: import_growth
load_data: real_gig
load_data: res_mom_12m_1m_0m
load_data: res_vol_6m_3m_0m
load_data: at
load_data: op_lev
load_data: roe
load_data: std_u_e
load_data: baa_ff
load_data: core_cpi
load_data: core_pce
load_data: corporate_tg
load_data: industrial_prod
load_data: home_pr
load_data: r_s
load_data: r_a
load_data: fred_ff
load_data: t3m
load_data: t6m
load_data: t2y
load_data: t10y
load_data: aaa
load_data: baa
load_data: core_ppi
load_data: cpi
load_data: pce
load_data: snp500_vol
load_data: ae_m
load_data: ia_ta
load_data: rc_a
load_data: aaa_t10y
load_data: baa_t10y
load_data: aaa_ff
load_data: real_pig
load_data: federal_tg
load_data: real_gdp
load_data: ret_noa
load_data: etm
load_data: ia_mv
load_data: sector_values
load_data: mv
load_data: pr_1m_0m
load_data: mv
making sample. (inference: True, date:2015-12-31 00:00:00, date_number: 348)
making sample. (inference: False, date:2012-12-31 00:00:00, date_number: 312)
making sample. (inference: False, date:2013-01-31 00:00:00, date_number: 313)
making sample. (inference: False, date:2013-02-28 00:00:00, date_number: 314)
making sample. (inference: False, date:2013-03-31 00:00:00, date_number: 315)
making sample. (inference: False, date:2013-04-30 00:00:00, date_number: 316)
making sample. (inference: False, date:2013-05-31 00:00:00, date_number: 317)
making sample. (inference: False, date:2013-06-30 00:00:00, date_number: 318)
making sample. (inference: False, date:2013-07-31 00:00:00, date_number: 319)
making sample. (inference: False, date:2013-08-31 00:00:00, date_number: 320)
making sample. (inference: False, date:2013-09-30 00:00:00, date_number: 321)
making sample. (inference: False, date:2013-10-31 00:00:00, date_number: 322)
making sample. (inference: False, date:2013-11-30 00:00:00, date_number: 323)
making sample. (inference: False, date:2013-12-31 00:00:00, date_number: 324)
making sample. (inference: False, date:2014-01-31 00:00:00, date_number: 325)
making sample. (inference: False, date:2014-02-28 00:00:00, date_number: 326)
making sample. (inference: False, date:2014-03-31 00:00:00, date_number: 327)
making sample. (inference: False, date:2014-04-30 00:00:00, date_number: 328)
making sample. (inference: False, date:2014-05-31 00:00:00, date_number: 329)
making sample. (inference: False, date:2014-06-30 00:00:00, date_number: 330)
making sample. (inference: False, date:2014-07-31 00:00:00, date_number: 331)
making sample. (inference: False, date:2014-08-31 00:00:00, date_number: 332)
making sample. (inference: False, date:2014-09-30 00:00:00, date_number: 333)
making sample. (inference: False, date:2014-10-31 00:00:00, date_number: 334)
making sample. (inference: False, date:2014-11-30 00:00:00, date_number: 335)
making sample. (inference: False, date:2014-12-31 00:00:00, date_number: 336)
making sample. (inference: False, date:2015-01-31 00:00:00, date_number: 337)
making sample. (inference: False, date:2015-02-28 00:00:00, date_number: 338)
making sample. (inference: False, date:2015-03-31 00:00:00, date_number: 339)
making sample. (inference: False, date:2015-04-30 00:00:00, date_number: 340)
making sample. (inference: False, date:2015-05-31 00:00:00, date_number: 341)
making sample. (inference: False, date:2015-06-30 00:00:00, date_number: 342)
making sample. (inference: False, date:2015-07-31 00:00:00, date_number: 343)
making sample. (inference: False, date:2015-08-31 00:00:00, date_number: 344)
making sample. (inference: False, date:2015-09-30 00:00:00, date_number: 345)
making sample. (inference: False, date:2015-10-31 00:00:00, date_number: 346)
making sample. (inference: False, date:2015-11-30 00:00:00, date_number: 347)
[AttentionModel/2015-12-31 | get_logger | INFO]: ====================AttentionModel/2015-12-31 start====================
[strategy_integration.py] set_seed(2015-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5739)training: 2/10 (0.5739)validation : 2/10 (0.5671)training: 3/10 (0.5671)validation : 3/10 (0.5671)training: 4/10 (0.5671)validation : 4/10 (0.5671)training: 5/10 (0.5671)validation : 5/10 (0.5671)early stopping at 5 with loss 0.5671
AttentionModel-training is done: 5/10
2015-12-31 | reset count: 0 | final loss: 0.5671 at epoch 2
making sample. (inference: True, date:2016-01-31 00:00:00, date_number: 349)
making sample. (inference: False, date:2015-12-31 00:00:00, date_number: 348)
[AttentionModel/2016-01-31 | get_logger | INFO]: ====================AttentionModel/2016-01-31 start====================
making sample. (inference: True, date:2016-02-29 00:00:00, date_number: 350)
making sample. (inference: False, date:2016-01-31 00:00:00, date_number: 349)
[AttentionModel/2016-02-29 | get_logger | INFO]: ====================AttentionModel/2016-02-29 start====================
[strategy_integration.py] set_seed(2016-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5792)training: 2/10 (0.5792)validation : 2/10 (0.5737)training: 3/10 (0.5737)validation : 3/10 (0.5737)training: 4/10 (0.5737)validation : 4/10 (0.5737)training: 5/10 (0.5737)validation : 5/10 (0.5737)training: 6/10 (0.5737)validation : 6/10 (0.5737)training: 7/10 (0.5737)validation : 7/10 (0.5737)training: 8/10 (0.5737)validation : 8/10 (0.5737)training: 9/10 (0.5737)validation : 9/10 (0.5737)early stopping at 9 with loss 0.5737
AttentionModel-training is done: 9/10
2016-01-31 | reset count: 0 | final loss: 0.5737 at epoch 2
[strategy_integration.py] set_seed(2016-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5755)training: 2/10 (0.5755)validation : 2/10 (0.5755)training: 3/10 (0.5755)validation : 3/10 (0.5709)training: 4/10 (0.5709)validation : 4/10 (0.5709)training: 5/10 (0.5709)validation : 5/10 (0.5709)early stopping at 5 with loss 0.5709
AttentionModel-training is done: 5/10
2016-02-29 | reset count: 0 | final loss: 0.5709 at epoch 3
making sample. (inference: True, date:2016-03-31 00:00:00, date_number: 351)
making sample. (inference: False, date:2016-02-29 00:00:00, date_number: 350)
[AttentionModel/2016-03-31 | get_logger | INFO]: ====================AttentionModel/2016-03-31 start====================
making sample. (inference: True, date:2016-04-30 00:00:00, date_number: 352)
making sample. (inference: False, date:2016-03-31 00:00:00, date_number: 351)
[AttentionModel/2016-04-30 | get_logger | INFO]: ====================AttentionModel/2016-04-30 start====================
[strategy_integration.py] set_seed(2016-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5729)training: 2/10 (0.5729)validation : 2/10 (0.5652)training: 3/10 (0.5652)validation : 3/10 (0.5652)training: 4/10 (0.5652)validation : 4/10 (0.5652)training: 5/10 (0.5652)validation : 5/10 (0.5618)training: 6/10 (0.5618)validation : 6/10 (0.5618)early stopping at 6 with loss 0.5618
AttentionModel-training is done: 6/10
2016-03-31 | reset count: 0 | final loss: 0.5618 at epoch 5
making sample. (inference: True, date:2016-05-31 00:00:00, date_number: 353)
making sample. (inference: False, date:2016-04-30 00:00:00, date_number: 352)
[AttentionModel/2016-05-31 | get_logger | INFO]: ====================AttentionModel/2016-05-31 start====================
[strategy_integration.py] set_seed(2016-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5767)training: 2/10 (0.5767)validation : 2/10 (0.5724)training: 3/10 (0.5724)validation : 3/10 (0.5724)training: 4/10 (0.5724)validation : 4/10 (0.5724)training: 5/10 (0.5724)validation : 5/10 (0.5716)training: 6/10 (0.5716)validation : 6/10 (0.5716)training: 7/10 (0.5716)validation : 7/10 (0.5716)early stopping at 7 with loss 0.5716
AttentionModel-training is done: 7/10
2016-04-30 | reset count: 0 | final loss: 0.5716 at epoch 5
making sample. (inference: True, date:2016-06-30 00:00:00, date_number: 354)
making sample. (inference: False, date:2016-05-31 00:00:00, date_number: 353)
[AttentionModel/2016-06-30 | get_logger | INFO]: ====================AttentionModel/2016-06-30 start====================
[strategy_integration.py] set_seed(2016-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5734)training: 2/10 (0.5734)validation : 2/10 (0.5708)training: 3/10 (0.5708)validation : 3/10 (0.5708)training: 4/10 (0.5708)validation : 4/10 (0.5708)training: 5/10 (0.5708)validation : 5/10 (0.5708)early stopping at 5 with loss 0.5708
AttentionModel-training is done: 5/10
2016-05-31 | reset count: 0 | final loss: 0.5708 at epoch 2
making sample. (inference: True, date:2016-07-31 00:00:00, date_number: 355)
making sample. (inference: False, date:2016-06-30 00:00:00, date_number: 354)
[AttentionModel/2016-07-31 | get_logger | INFO]: ====================AttentionModel/2016-07-31 start====================
[strategy_integration.py] set_seed(2016-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5794)training: 2/10 (0.5794)validation : 2/10 (0.5701)training: 3/10 (0.5701)validation : 3/10 (0.5701)training: 4/10 (0.5701)validation : 4/10 (0.5701)training: 5/10 (0.5701)validation : 5/10 (0.5701)early stopping at 5 with loss 0.5701
AttentionModel-training is done: 5/10
2016-06-30 | reset count: 0 | final loss: 0.5701 at epoch 2
[strategy_integration.py] set_seed(2016-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5722)training: 2/10 (0.5722)validation : 2/10 (0.5722)training: 3/10 (0.5722)validation : 3/10 (0.5722)training: 4/10 (0.5722)validation : 4/10 (0.5722)training: 5/10 (0.5722)validation : 5/10 (0.5722)early stopping at 5 with loss 0.5722
AttentionModel-training is done: 5/10
2016-07-31 | reset count: 0 | final loss: 0.5722 at epoch 1
making sample. (inference: True, date:2016-08-31 00:00:00, date_number: 356)
making sample. (inference: False, date:2016-07-31 00:00:00, date_number: 355)
[AttentionModel/2016-08-31 | get_logger | INFO]: ====================AttentionModel/2016-08-31 start====================
making sample. (inference: True, date:2016-09-30 00:00:00, date_number: 357)
making sample. (inference: False, date:2016-08-31 00:00:00, date_number: 356)
[AttentionModel/2016-09-30 | get_logger | INFO]: ====================AttentionModel/2016-09-30 start====================
[strategy_integration.py] set_seed(2016-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5776)training: 2/10 (0.5776)validation : 2/10 (0.5776)training: 3/10 (0.5776)validation : 3/10 (0.5753)training: 4/10 (0.5753)validation : 4/10 (0.5753)training: 5/10 (0.5753)validation : 5/10 (0.5736)training: 6/10 (0.5736)validation : 6/10 (0.5736)early stopping at 6 with loss 0.5736
AttentionModel-training is done: 6/10
2016-08-31 | reset count: 0 | final loss: 0.5736 at epoch 5
making sample. (inference: True, date:2016-10-31 00:00:00, date_number: 358)
making sample. (inference: False, date:2016-09-30 00:00:00, date_number: 357)
[strategy_integration.py] set_seed(2016-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5760)training: 2/10 (0.5760)validation : 2/10 (0.5757)training: 3/10 (0.5757)validation : 3/10 (0.5719)training: 4/10 (0.5719)validation : 4/10 (0.5713)training: 5/10 (0.5713)validation : 5/10 (0.5713)training: 6/10 (0.5713)validation : 6/10 (0.5688)training: 7/10 (0.5688)validation : 7/10 (0.5688)early stopping at 7 with loss 0.5688
AttentionModel-training is done: 7/10
2016-09-30 | reset count: 0 | final loss: 0.5688 at epoch 6
[AttentionModel/2016-10-31 | get_logger | INFO]: ====================AttentionModel/2016-10-31 start====================
making sample. (inference: True, date:2016-11-30 00:00:00, date_number: 359)
making sample. (inference: False, date:2016-10-31 00:00:00, date_number: 358)
[AttentionModel/2016-11-30 | get_logger | INFO]: ====================AttentionModel/2016-11-30 start====================
[strategy_integration.py] set_seed(2016-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5797)training: 2/10 (0.5797)validation : 2/10 (0.5713)training: 3/10 (0.5713)validation : 3/10 (0.5713)training: 4/10 (0.5713)validation : 4/10 (0.5713)training: 5/10 (0.5713)validation : 5/10 (0.5712)training: 6/10 (0.5712)validation : 6/10 (0.5707)training: 7/10 (0.5707)validation : 7/10 (0.5707)training: 8/10 (0.5707)validation : 8/10 (0.5707)early stopping at 8 with loss 0.5707
AttentionModel-training is done: 8/10
2016-10-31 | reset count: 0 | final loss: 0.5707 at epoch 6
making sample. (inference: True, date:2016-12-31 00:00:00, date_number: 360)
making sample. (inference: False, date:2016-11-30 00:00:00, date_number: 359)
[AttentionModel/2016-12-31 | get_logger | INFO]: ====================AttentionModel/2016-12-31 start====================
[strategy_integration.py] set_seed(2016-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5628)training: 2/10 (0.5628)validation : 2/10 (0.5601)training: 3/10 (0.5601)validation : 3/10 (0.5591)training: 4/10 (0.5591)validation : 4/10 (0.5591)training: 5/10 (0.5591)validation : 5/10 (0.5591)training: 6/10 (0.5591)validation : 6/10 (0.5565)training: 7/10 (0.5565)validation : 7/10 (0.5565)training: 8/10 (0.5565)validation : 8/10 (0.5565)training: 9/10 (0.5565)validation : 9/10 (0.5565)early stopping at 9 with loss 0.5565
AttentionModel-training is done: 9/10
2016-11-30 | reset count: 0 | final loss: 0.5565 at epoch 6
making sample. (inference: True, date:2017-01-31 00:00:00, date_number: 361)
making sample. (inference: False, date:2016-12-31 00:00:00, date_number: 360)
[AttentionModel/2017-01-31 | get_logger | INFO]: ====================AttentionModel/2017-01-31 start====================
[strategy_integration.py] set_seed(2016-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5679)training: 2/10 (0.5679)validation : 2/10 (0.5653)training: 3/10 (0.5653)validation : 3/10 (0.5652)training: 4/10 (0.5652)validation : 4/10 (0.5652)training: 5/10 (0.5652)validation : 5/10 (0.5652)early stopping at 5 with loss 0.5652
AttentionModel-training is done: 5/10
2016-12-31 | reset count: 0 | final loss: 0.5652 at epoch 3
making sample. (inference: True, date:2017-02-28 00:00:00, date_number: 362)
making sample. (inference: False, date:2017-01-31 00:00:00, date_number: 361)
[AttentionModel/2017-02-28 | get_logger | INFO]: ====================AttentionModel/2017-02-28 start====================
[strategy_integration.py] set_seed(2017-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5623)training: 2/10 (0.5623)validation : 2/10 (0.5609)training: 3/10 (0.5609)validation : 3/10 (0.5589)training: 4/10 (0.5589)validation : 4/10 (0.5586)training: 5/10 (0.5586)validation : 5/10 (0.5586)training: 6/10 (0.5586)validation : 6/10 (0.5586)early stopping at 6 with loss 0.5586
AttentionModel-training is done: 6/10
2017-01-31 | reset count: 0 | final loss: 0.5586 at epoch 4
making sample. (inference: True, date:2017-03-31 00:00:00, date_number: 363)
making sample. (inference: False, date:2017-02-28 00:00:00, date_number: 362)
[AttentionModel/2017-03-31 | get_logger | INFO]: ====================AttentionModel/2017-03-31 start====================
[strategy_integration.py] set_seed(2017-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5630)training: 2/10 (0.5630)validation : 2/10 (0.5627)training: 3/10 (0.5627)validation : 3/10 (0.5612)training: 4/10 (0.5612)validation : 4/10 (0.5612)training: 5/10 (0.5612)validation : 5/10 (0.5581)training: 6/10 (0.5581)validation : 6/10 (0.5581)training: 7/10 (0.5581)validation : 7/10 (0.5581)training: 8/10 (0.5581)validation : 8/10 (0.5581)early stopping at 8 with loss 0.5581
AttentionModel-training is done: 8/10
2017-02-28 | reset count: 0 | final loss: 0.5581 at epoch 5
making sample. (inference: True, date:2017-04-30 00:00:00, date_number: 364)
making sample. (inference: False, date:2017-03-31 00:00:00, date_number: 363)
[AttentionModel/2017-04-30 | get_logger | INFO]: ====================AttentionModel/2017-04-30 start====================
[strategy_integration.py] set_seed(2017-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5582)training: 2/10 (0.5582)validation : 2/10 (0.5582)training: 3/10 (0.5582)validation : 3/10 (0.5561)training: 4/10 (0.5561)validation : 4/10 (0.5561)training: 5/10 (0.5561)validation : 5/10 (0.5561)early stopping at 5 with loss 0.5561
AttentionModel-training is done: 5/10
2017-03-31 | reset count: 0 | final loss: 0.5561 at epoch 3
making sample. (inference: True, date:2017-05-31 00:00:00, date_number: 365)
making sample. (inference: False, date:2017-04-30 00:00:00, date_number: 364)
[AttentionModel/2017-05-31 | get_logger | INFO]: ====================AttentionModel/2017-05-31 start====================
[strategy_integration.py] set_seed(2017-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5604)training: 2/10 (0.5604)validation : 2/10 (0.5591)training: 3/10 (0.5591)validation : 3/10 (0.5591)training: 4/10 (0.5591)validation : 4/10 (0.5591)training: 5/10 (0.5591)validation : 5/10 (0.5591)early stopping at 5 with loss 0.5591
AttentionModel-training is done: 5/10
2017-04-30 | reset count: 0 | final loss: 0.5591 at epoch 2
making sample. (inference: True, date:2017-06-30 00:00:00, date_number: 366)
making sample. (inference: False, date:2017-05-31 00:00:00, date_number: 365)
[AttentionModel/2017-06-30 | get_logger | INFO]: ====================AttentionModel/2017-06-30 start====================
[strategy_integration.py] set_seed(2017-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5539)training: 2/10 (0.5539)validation : 2/10 (0.5539)training: 3/10 (0.5539)validation : 3/10 (0.5538)training: 4/10 (0.5538)validation : 4/10 (0.5538)training: 5/10 (0.5538)validation : 5/10 (0.5538)training: 6/10 (0.5538)validation : 6/10 (0.5538)early stopping at 6 with loss 0.5538
AttentionModel-training is done: 6/10
2017-05-31 | reset count: 0 | final loss: 0.5538 at epoch 3
making sample. (inference: True, date:2017-07-31 00:00:00, date_number: 367)
making sample. (inference: False, date:2017-06-30 00:00:00, date_number: 366)
[AttentionModel/2017-07-31 | get_logger | INFO]: ====================AttentionModel/2017-07-31 start====================
[strategy_integration.py] set_seed(2017-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5662)training: 2/10 (0.5662)validation : 2/10 (0.5647)training: 3/10 (0.5647)validation : 3/10 (0.5606)training: 4/10 (0.5606)validation : 4/10 (0.5606)training: 5/10 (0.5606)validation : 5/10 (0.5601)training: 6/10 (0.5601)validation : 6/10 (0.5601)early stopping at 6 with loss 0.5601
AttentionModel-training is done: 6/10
2017-06-30 | reset count: 0 | final loss: 0.5601 at epoch 5
making sample. (inference: True, date:2017-08-31 00:00:00, date_number: 368)
making sample. (inference: False, date:2017-07-31 00:00:00, date_number: 367)
[AttentionModel/2017-08-31 | get_logger | INFO]: ====================AttentionModel/2017-08-31 start====================
[strategy_integration.py] set_seed(2017-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5677)training: 2/10 (0.5677)validation : 2/10 (0.5658)training: 3/10 (0.5658)validation : 3/10 (0.5624)training: 4/10 (0.5624)validation : 4/10 (0.5624)training: 5/10 (0.5624)validation : 5/10 (0.5624)training: 6/10 (0.5624)validation : 6/10 (0.5624)early stopping at 6 with loss 0.5624
AttentionModel-training is done: 6/10
2017-07-31 | reset count: 0 | final loss: 0.5624 at epoch 3
making sample. (inference: True, date:2017-09-30 00:00:00, date_number: 369)
making sample. (inference: False, date:2017-08-31 00:00:00, date_number: 368)
[AttentionModel/2017-09-30 | get_logger | INFO]: ====================AttentionModel/2017-09-30 start====================
[strategy_integration.py] set_seed(2017-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5627)training: 2/10 (0.5627)validation : 2/10 (0.5601)training: 3/10 (0.5601)validation : 3/10 (0.5587)training: 4/10 (0.5587)validation : 4/10 (0.5587)training: 5/10 (0.5587)validation : 5/10 (0.5587)early stopping at 5 with loss 0.5587
AttentionModel-training is done: 5/10
2017-08-31 | reset count: 0 | final loss: 0.5587 at epoch 3
making sample. (inference: True, date:2017-10-31 00:00:00, date_number: 370)
making sample. (inference: False, date:2017-09-30 00:00:00, date_number: 369)
[strategy_integration.py] set_seed(2017-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5602)training: 2/10 (0.5602)validation : 2/10 (0.5589)training: 3/10 (0.5589)validation : 3/10 (0.5589)training: 4/10 (0.5589)validation : 4/10 (0.5589)training: 5/10 (0.5589)validation : 5/10 (0.5577)early stopping at 5 with loss 0.5577
AttentionModel-training is done: 5/10
2017-09-30 | reset count: 0 | final loss: 0.5577 at epoch 5
[AttentionModel/2017-10-31 | get_logger | INFO]: ====================AttentionModel/2017-10-31 start====================
making sample. (inference: True, date:2017-11-30 00:00:00, date_number: 371)
making sample. (inference: False, date:2017-10-31 00:00:00, date_number: 370)
[AttentionModel/2017-11-30 | get_logger | INFO]: ====================AttentionModel/2017-11-30 start====================
[strategy_integration.py] set_seed(2017-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5709)training: 2/10 (0.5709)validation : 2/10 (0.5652)training: 3/10 (0.5652)validation : 3/10 (0.5652)training: 4/10 (0.5652)validation : 4/10 (0.5652)training: 5/10 (0.5652)validation : 5/10 (0.5652)early stopping at 5 with loss 0.5652
AttentionModel-training is done: 5/10
2017-10-31 | reset count: 0 | final loss: 0.5652 at epoch 2
making sample. (inference: True, date:2017-12-31 00:00:00, date_number: 372)
making sample. (inference: False, date:2017-11-30 00:00:00, date_number: 371)
[AttentionModel/2017-12-31 | get_logger | INFO]: ====================AttentionModel/2017-12-31 start====================
[strategy_integration.py] set_seed(2017-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5568)training: 2/10 (0.5568)validation : 2/10 (0.5568)training: 3/10 (0.5568)validation : 3/10 (0.5568)training: 4/10 (0.5568)validation : 4/10 (0.5568)training: 5/10 (0.5568)validation : 5/10 (0.5568)training: 6/10 (0.5568)validation : 6/10 (0.5568)training: 7/10 (0.5568)validation : 7/10 (0.5568)early stopping at 7 with loss 0.5568
AttentionModel-training is done: 7/10
2017-11-30 | reset count: 0 | final loss: 0.5568 at epoch 5
making sample. (inference: True, date:2018-01-31 00:00:00, date_number: 373)
making sample. (inference: False, date:2017-12-31 00:00:00, date_number: 372)
[AttentionModel/2018-01-31 | get_logger | INFO]: ====================AttentionModel/2018-01-31 start====================
[strategy_integration.py] set_seed(2017-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5763)training: 2/10 (0.5763)validation : 2/10 (0.5688)training: 3/10 (0.5688)validation : 3/10 (0.5688)training: 4/10 (0.5688)validation : 4/10 (0.5688)training: 5/10 (0.5688)validation : 5/10 (0.5688)training: 6/10 (0.5688)validation : 6/10 (0.5688)early stopping at 6 with loss 0.5688
AttentionModel-training is done: 6/10
2017-12-31 | reset count: 0 | final loss: 0.5688 at epoch 2
[strategy_integration.py] set_seed(2018-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5705)training: 2/10 (0.5705)validation : 2/10 (0.5700)training: 3/10 (0.5700)validation : 3/10 (0.5700)training: 4/10 (0.5700)validation : 4/10 (0.5700)training: 5/10 (0.5700)validation : 5/10 (0.5700)early stopping at 5 with loss 0.5700
AttentionModel-training is done: 5/10
2018-01-31 | reset count: 0 | final loss: 0.5700 at epoch 2
making sample. (inference: True, date:2018-02-28 00:00:00, date_number: 374)
making sample. (inference: False, date:2018-01-31 00:00:00, date_number: 373)
[AttentionModel/2018-02-28 | get_logger | INFO]: ====================AttentionModel/2018-02-28 start====================
making sample. (inference: True, date:2018-03-31 00:00:00, date_number: 375)
making sample. (inference: False, date:2018-02-28 00:00:00, date_number: 374)
[AttentionModel/2018-03-31 | get_logger | INFO]: ====================AttentionModel/2018-03-31 start====================
[strategy_integration.py] set_seed(2018-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5666)training: 2/10 (0.5666)validation : 2/10 (0.5666)training: 3/10 (0.5666)validation : 3/10 (0.5638)training: 4/10 (0.5638)validation : 4/10 (0.5638)training: 5/10 (0.5638)validation : 5/10 (0.5625)training: 6/10 (0.5625)validation : 6/10 (0.5625)early stopping at 6 with loss 0.5625
AttentionModel-training is done: 6/10
2018-02-28 | reset count: 0 | final loss: 0.5625 at epoch 5
[strategy_integration.py] set_seed(2018-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5672)training: 2/10 (0.5672)validation : 2/10 (0.5672)training: 3/10 (0.5672)validation : 3/10 (0.5635)training: 4/10 (0.5635)validation : 4/10 (0.5635)training: 5/10 (0.5635)validation : 5/10 (0.5635)training: 6/10 (0.5635)validation : 6/10 (0.5635)early stopping at 6 with loss 0.5635
AttentionModel-training is done: 6/10
2018-03-31 | reset count: 0 | final loss: 0.5635 at epoch 3
making sample. (inference: True, date:2018-04-30 00:00:00, date_number: 376)
making sample. (inference: False, date:2018-03-31 00:00:00, date_number: 375)
[AttentionModel/2018-04-30 | get_logger | INFO]: ====================AttentionModel/2018-04-30 start====================
making sample. (inference: True, date:2018-05-31 00:00:00, date_number: 377)
making sample. (inference: False, date:2018-04-30 00:00:00, date_number: 376)
[AttentionModel/2018-05-31 | get_logger | INFO]: ====================AttentionModel/2018-05-31 start====================
[strategy_integration.py] set_seed(2018-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5595)training: 2/10 (0.5595)validation : 2/10 (0.5595)training: 3/10 (0.5595)validation : 3/10 (0.5595)training: 4/10 (0.5595)validation : 4/10 (0.5595)training: 5/10 (0.5595)validation : 5/10 (0.5595)training: 6/10 (0.5595)validation : 6/10 (0.5576)training: 7/10 (0.5576)validation : 7/10 (0.5576)training: 8/10 (0.5576)validation : 8/10 (0.5576)early stopping at 8 with loss 0.5576
AttentionModel-training is done: 8/10
2018-04-30 | reset count: 0 | final loss: 0.5576 at epoch 6
[strategy_integration.py] set_seed(2018-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5768)training: 2/10 (0.5768)validation : 2/10 (0.5751)training: 3/10 (0.5751)validation : 3/10 (0.5728)training: 4/10 (0.5728)validation : 4/10 (0.5728)training: 5/10 (0.5728)validation : 5/10 (0.5728)early stopping at 5 with loss 0.5728
AttentionModel-training is done: 5/10
2018-05-31 | reset count: 0 | final loss: 0.5728 at epoch 3
making sample. (inference: True, date:2018-06-30 00:00:00, date_number: 378)
making sample. (inference: False, date:2018-05-31 00:00:00, date_number: 377)
[AttentionModel/2018-06-30 | get_logger | INFO]: ====================AttentionModel/2018-06-30 start====================
making sample. (inference: True, date:2018-07-31 00:00:00, date_number: 379)
making sample. (inference: False, date:2018-06-30 00:00:00, date_number: 378)
[AttentionModel/2018-07-31 | get_logger | INFO]: ====================AttentionModel/2018-07-31 start====================
[strategy_integration.py] set_seed(2018-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5676)training: 2/10 (0.5676)validation : 2/10 (0.5676)training: 3/10 (0.5676)validation : 3/10 (0.5676)training: 4/10 (0.5676)validation : 4/10 (0.5636)training: 5/10 (0.5636)validation : 5/10 (0.5636)training: 6/10 (0.5636)validation : 6/10 (0.5636)training: 7/10 (0.5636)validation : 7/10 (0.5636)early stopping at 7 with loss 0.5636
AttentionModel-training is done: 7/10
2018-06-30 | reset count: 0 | final loss: 0.5636 at epoch 4
[strategy_integration.py] set_seed(2018-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5719)training: 2/10 (0.5719)validation : 2/10 (0.5719)training: 3/10 (0.5719)validation : 3/10 (0.5713)training: 4/10 (0.5713)validation : 4/10 (0.5713)training: 5/10 (0.5713)validation : 5/10 (0.5713)early stopping at 5 with loss 0.5713
AttentionModel-training is done: 5/10
2018-07-31 | reset count: 0 | final loss: 0.5713 at epoch 3
making sample. (inference: True, date:2018-08-31 00:00:00, date_number: 380)
making sample. (inference: False, date:2018-07-31 00:00:00, date_number: 379)
[AttentionModel/2018-08-31 | get_logger | INFO]: ====================AttentionModel/2018-08-31 start====================
making sample. (inference: True, date:2018-09-30 00:00:00, date_number: 381)
making sample. (inference: False, date:2018-08-31 00:00:00, date_number: 380)
[AttentionModel/2018-09-30 | get_logger | INFO]: ====================AttentionModel/2018-09-30 start====================
[strategy_integration.py] set_seed(2018-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5731)training: 2/10 (0.5731)validation : 2/10 (0.5729)training: 3/10 (0.5729)validation : 3/10 (0.5708)training: 4/10 (0.5708)validation : 4/10 (0.5690)training: 5/10 (0.5690)validation : 5/10 (0.5690)training: 6/10 (0.5690)validation : 6/10 (0.5670)training: 7/10 (0.5670)validation : 7/10 (0.5670)early stopping at 7 with loss 0.5670
AttentionModel-training is done: 7/10
2018-08-31 | reset count: 0 | final loss: 0.5670 at epoch 6
[strategy_integration.py] set_seed(2018-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5733)training: 2/10 (0.5733)validation : 2/10 (0.5678)training: 3/10 (0.5678)validation : 3/10 (0.5678)training: 4/10 (0.5678)validation : 4/10 (0.5678)training: 5/10 (0.5678)validation : 5/10 (0.5678)early stopping at 5 with loss 0.5678
AttentionModel-training is done: 5/10
2018-09-30 | reset count: 0 | final loss: 0.5678 at epoch 2
making sample. (inference: True, date:2018-10-31 00:00:00, date_number: 382)
making sample. (inference: False, date:2018-09-30 00:00:00, date_number: 381)
[AttentionModel/2018-10-31 | get_logger | INFO]: ====================AttentionModel/2018-10-31 start====================
making sample. (inference: True, date:2018-11-30 00:00:00, date_number: 383)
making sample. (inference: False, date:2018-10-31 00:00:00, date_number: 382)
[AttentionModel/2018-11-30 | get_logger | INFO]: ====================AttentionModel/2018-11-30 start====================
[strategy_integration.py] set_seed(2018-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5769)training: 2/10 (0.5769)validation : 2/10 (0.5769)training: 3/10 (0.5769)validation : 3/10 (0.5768)training: 4/10 (0.5768)validation : 4/10 (0.5767)training: 5/10 (0.5767)validation : 5/10 (0.5767)training: 6/10 (0.5767)validation : 6/10 (0.5759)training: 7/10 (0.5759)validation : 7/10 (0.5759)early stopping at 7 with loss 0.5759
AttentionModel-training is done: 7/10
2018-10-31 | reset count: 0 | final loss: 0.5759 at epoch 6
[strategy_integration.py] set_seed(2018-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5808)training: 2/10 (0.5808)validation : 2/10 (0.5759)training: 3/10 (0.5759)validation : 3/10 (0.5759)training: 4/10 (0.5759)validation : 4/10 (0.5759)training: 5/10 (0.5759)validation : 5/10 (0.5756)training: 6/10 (0.5756)validation : 6/10 (0.5756)early stopping at 6 with loss 0.5756
AttentionModel-training is done: 6/10
2018-11-30 | reset count: 0 | final loss: 0.5756 at epoch 5
making sample. (inference: True, date:2018-12-31 00:00:00, date_number: 384)
making sample. (inference: False, date:2018-11-30 00:00:00, date_number: 383)
[AttentionModel/2018-12-31 | get_logger | INFO]: ====================AttentionModel/2018-12-31 start====================
making sample. (inference: True, date:2019-01-31 00:00:00, date_number: 385)
making sample. (inference: False, date:2018-12-31 00:00:00, date_number: 384)
[AttentionModel/2019-01-31 | get_logger | INFO]: ====================AttentionModel/2019-01-31 start====================
[strategy_integration.py] set_seed(2018-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5723)training: 2/10 (0.5723)validation : 2/10 (0.5723)training: 3/10 (0.5723)validation : 3/10 (0.5699)training: 4/10 (0.5699)validation : 4/10 (0.5699)training: 5/10 (0.5699)validation : 5/10 (0.5699)training: 6/10 (0.5699)validation : 6/10 (0.5699)early stopping at 6 with loss 0.5699
AttentionModel-training is done: 6/10
2018-12-31 | reset count: 0 | final loss: 0.5699 at epoch 3
making sample. (inference: True, date:2019-02-28 00:00:00, date_number: 386)
making sample. (inference: False, date:2019-01-31 00:00:00, date_number: 385)
[AttentionModel/2019-02-28 | get_logger | INFO]: ====================AttentionModel/2019-02-28 start====================
[strategy_integration.py] set_seed(2019-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5856)training: 2/10 (0.5856)validation : 2/10 (0.5839)training: 3/10 (0.5839)validation : 3/10 (0.5839)training: 4/10 (0.5839)validation : 4/10 (0.5833)training: 5/10 (0.5833)validation : 5/10 (0.5804)training: 6/10 (0.5804)validation : 6/10 (0.5801)training: 7/10 (0.5801)validation : 7/10 (0.5801)training: 8/10 (0.5801)validation : 8/10 (0.5801)early stopping at 8 with loss 0.5801
AttentionModel-training is done: 8/10
2019-01-31 | reset count: 0 | final loss: 0.5801 at epoch 6
making sample. (inference: True, date:2019-03-31 00:00:00, date_number: 387)
making sample. (inference: False, date:2019-02-28 00:00:00, date_number: 386)
[AttentionModel/2019-03-31 | get_logger | INFO]: ====================AttentionModel/2019-03-31 start====================
[strategy_integration.py] set_seed(2019-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5813)training: 2/10 (0.5813)validation : 2/10 (0.5805)training: 3/10 (0.5805)validation : 3/10 (0.5797)training: 4/10 (0.5797)validation : 4/10 (0.5761)training: 5/10 (0.5761)validation : 5/10 (0.5761)training: 6/10 (0.5761)validation : 6/10 (0.5761)early stopping at 6 with loss 0.5761
AttentionModel-training is done: 6/10
2019-02-28 | reset count: 0 | final loss: 0.5761 at epoch 4
making sample. (inference: True, date:2019-04-30 00:00:00, date_number: 388)
making sample. (inference: False, date:2019-03-31 00:00:00, date_number: 387)
[AttentionModel/2019-04-30 | get_logger | INFO]: ====================AttentionModel/2019-04-30 start====================
[strategy_integration.py] set_seed(2019-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5752)training: 2/10 (0.5752)validation : 2/10 (0.5752)training: 3/10 (0.5752)validation : 3/10 (0.5752)training: 4/10 (0.5752)validation : 4/10 (0.5716)training: 5/10 (0.5716)validation : 5/10 (0.5716)training: 6/10 (0.5716)validation : 6/10 (0.5716)early stopping at 6 with loss 0.5716
AttentionModel-training is done: 6/10
2019-03-31 | reset count: 0 | final loss: 0.5716 at epoch 4
making sample. (inference: True, date:2019-05-31 00:00:00, date_number: 389)
making sample. (inference: False, date:2019-04-30 00:00:00, date_number: 388)
[AttentionModel/2019-05-31 | get_logger | INFO]: ====================AttentionModel/2019-05-31 start====================
[strategy_integration.py] set_seed(2019-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5835)training: 2/10 (0.5835)validation : 2/10 (0.5811)training: 3/10 (0.5811)validation : 3/10 (0.5811)training: 4/10 (0.5811)validation : 4/10 (0.5804)training: 5/10 (0.5804)validation : 5/10 (0.5750)training: 6/10 (0.5750)validation : 6/10 (0.5750)training: 7/10 (0.5750)validation : 7/10 (0.5750)training: 8/10 (0.5750)validation : 8/10 (0.5750)early stopping at 8 with loss 0.5750
AttentionModel-training is done: 8/10
2019-04-30 | reset count: 0 | final loss: 0.5750 at epoch 5
[strategy_integration.py] set_seed(2019-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5855)training: 2/10 (0.5855)validation : 2/10 (0.5790)training: 3/10 (0.5790)validation : 3/10 (0.5789)training: 4/10 (0.5789)validation : 4/10 (0.5789)training: 5/10 (0.5789)validation : 5/10 (0.5789)training: 6/10 (0.5789)validation : 6/10 (0.5789)early stopping at 6 with loss 0.5789
AttentionModel-training is done: 6/10
2019-05-31 | reset count: 0 | final loss: 0.5789 at epoch 3
making sample. (inference: True, date:2019-06-30 00:00:00, date_number: 390)
making sample. (inference: False, date:2019-05-31 00:00:00, date_number: 389)
[AttentionModel/2019-06-30 | get_logger | INFO]: ====================AttentionModel/2019-06-30 start====================
making sample. (inference: True, date:2019-07-31 00:00:00, date_number: 391)
making sample. (inference: False, date:2019-06-30 00:00:00, date_number: 390)
[AttentionModel/2019-07-31 | get_logger | INFO]: ====================AttentionModel/2019-07-31 start====================
[strategy_integration.py] set_seed(2019-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5762)training: 2/10 (0.5762)validation : 2/10 (0.5751)training: 3/10 (0.5751)validation : 3/10 (0.5700)training: 4/10 (0.5700)validation : 4/10 (0.5700)training: 5/10 (0.5700)validation : 5/10 (0.5694)training: 6/10 (0.5694)validation : 6/10 (0.5694)early stopping at 6 with loss 0.5694
AttentionModel-training is done: 6/10
2019-06-30 | reset count: 0 | final loss: 0.5694 at epoch 5
[strategy_integration.py] set_seed(2019-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5838)training: 2/10 (0.5838)validation : 2/10 (0.5838)training: 3/10 (0.5838)validation : 3/10 (0.5819)training: 4/10 (0.5819)validation : 4/10 (0.5819)training: 5/10 (0.5819)validation : 5/10 (0.5819)early stopping at 5 with loss 0.5819
AttentionModel-training is done: 5/10
2019-07-31 | reset count: 0 | final loss: 0.5819 at epoch 3
making sample. (inference: True, date:2019-08-31 00:00:00, date_number: 392)
making sample. (inference: False, date:2019-07-31 00:00:00, date_number: 391)
[AttentionModel/2019-08-31 | get_logger | INFO]: ====================AttentionModel/2019-08-31 start====================
making sample. (inference: True, date:2019-09-30 00:00:00, date_number: 393)
making sample. (inference: False, date:2019-08-31 00:00:00, date_number: 392)
[AttentionModel/2019-09-30 | get_logger | INFO]: ====================AttentionModel/2019-09-30 start====================
[strategy_integration.py] set_seed(2019-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5844)training: 2/10 (0.5844)validation : 2/10 (0.5835)training: 3/10 (0.5835)validation : 3/10 (0.5832)training: 4/10 (0.5832)validation : 4/10 (0.5831)training: 5/10 (0.5831)validation : 5/10 (0.5823)training: 6/10 (0.5823)validation : 6/10 (0.5823)early stopping at 6 with loss 0.5823
AttentionModel-training is done: 6/10
2019-08-31 | reset count: 0 | final loss: 0.5823 at epoch 5
[strategy_integration.py] set_seed(2019-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5777)training: 2/10 (0.5777)validation : 2/10 (0.5749)training: 3/10 (0.5749)validation : 3/10 (0.5749)training: 4/10 (0.5749)validation : 4/10 (0.5749)training: 5/10 (0.5749)validation : 5/10 (0.5726)training: 6/10 (0.5726)validation : 6/10 (0.5726)early stopping at 6 with loss 0.5726
AttentionModel-training is done: 6/10
2019-09-30 | reset count: 0 | final loss: 0.5726 at epoch 5
making sample. (inference: True, date:2019-10-31 00:00:00, date_number: 394)
making sample. (inference: False, date:2019-09-30 00:00:00, date_number: 393)
[AttentionModel/2019-10-31 | get_logger | INFO]: ====================AttentionModel/2019-10-31 start====================
making sample. (inference: True, date:2019-11-30 00:00:00, date_number: 395)
making sample. (inference: False, date:2019-10-31 00:00:00, date_number: 394)
[AttentionModel/2019-11-30 | get_logger | INFO]: ====================AttentionModel/2019-11-30 start====================
[strategy_integration.py] set_seed(2019-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5841)training: 2/10 (0.5841)validation : 2/10 (0.5827)training: 3/10 (0.5827)validation : 3/10 (0.5822)training: 4/10 (0.5822)validation : 4/10 (0.5822)training: 5/10 (0.5822)validation : 5/10 (0.5812)training: 6/10 (0.5812)validation : 6/10 (0.5812)training: 7/10 (0.5812)validation : 7/10 (0.5812)training: 8/10 (0.5812)validation : 8/10 (0.5788)training: 9/10 (0.5788)validation : 9/10 (0.5788)training: 10/10 (0.5788)validation : 10/10 (0.5788)AttentionModel-training is done: 10/10
2019-10-31 | reset count: 0 | final loss: 0.5788 at epoch 8
[strategy_integration.py] set_seed(2019-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5907)training: 2/10 (0.5907)validation : 2/10 (0.5897)training: 3/10 (0.5897)validation : 3/10 (0.5857)training: 4/10 (0.5857)validation : 4/10 (0.5857)training: 5/10 (0.5857)validation : 5/10 (0.5827)training: 6/10 (0.5827)validation : 6/10 (0.5827)early stopping at 6 with loss 0.5827
AttentionModel-training is done: 6/10
2019-11-30 | reset count: 0 | final loss: 0.5827 at epoch 5
making sample. (inference: True, date:2019-12-31 00:00:00, date_number: 396)
making sample. (inference: False, date:2019-11-30 00:00:00, date_number: 395)
[AttentionModel/2019-12-31 | get_logger | INFO]: ====================AttentionModel/2019-12-31 start====================
making sample. (inference: True, date:2020-01-31 00:00:00, date_number: 397)
making sample. (inference: False, date:2019-12-31 00:00:00, date_number: 396)
[AttentionModel/2020-01-31 | get_logger | INFO]: ====================AttentionModel/2020-01-31 start====================
[strategy_integration.py] set_seed(2019-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5727)training: 2/10 (0.5727)validation : 2/10 (0.5727)training: 3/10 (0.5727)validation : 3/10 (0.5727)training: 4/10 (0.5727)validation : 4/10 (0.5727)training: 5/10 (0.5727)validation : 5/10 (0.5727)early stopping at 5 with loss 0.5727
AttentionModel-training is done: 5/10
2019-12-31 | reset count: 0 | final loss: 0.5727 at epoch 1
[strategy_integration.py] set_seed(2020-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5758)training: 2/10 (0.5758)validation : 2/10 (0.5758)training: 3/10 (0.5758)validation : 3/10 (0.5758)training: 4/10 (0.5758)validation : 4/10 (0.5758)training: 5/10 (0.5758)validation : 5/10 (0.5758)early stopping at 5 with loss 0.5758
AttentionModel-training is done: 5/10
2020-01-31 | reset count: 0 | final loss: 0.5758 at epoch 1
making sample. (inference: True, date:2020-02-29 00:00:00, date_number: 398)
making sample. (inference: False, date:2020-01-31 00:00:00, date_number: 397)
[AttentionModel/2020-02-29 | get_logger | INFO]: ====================AttentionModel/2020-02-29 start====================
making sample. (inference: True, date:2020-03-31 00:00:00, date_number: 399)
making sample. (inference: False, date:2020-02-29 00:00:00, date_number: 398)
[AttentionModel/2020-03-31 | get_logger | INFO]: ====================AttentionModel/2020-03-31 start====================
[strategy_integration.py] set_seed(2020-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5724)training: 2/10 (0.5724)validation : 2/10 (0.5681)training: 3/10 (0.5681)validation : 3/10 (0.5681)training: 4/10 (0.5681)validation : 4/10 (0.5681)training: 5/10 (0.5681)validation : 5/10 (0.5681)early stopping at 5 with loss 0.5681
AttentionModel-training is done: 5/10
2020-03-31 | reset count: 0 | final loss: 0.5681 at epoch 2
[strategy_integration.py] set_seed(2020-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5762)training: 2/10 (0.5762)validation : 2/10 (0.5735)training: 3/10 (0.5735)validation : 3/10 (0.5735)training: 4/10 (0.5735)validation : 4/10 (0.5735)training: 5/10 (0.5735)validation : 5/10 (0.5724)training: 6/10 (0.5724)validation : 6/10 (0.5724)training: 7/10 (0.5724)validation : 7/10 (0.5713)training: 8/10 (0.5713)validation : 8/10 (0.5711)training: 9/10 (0.5711)validation : 9/10 (0.5711)training: 10/10 (0.5711)validation : 10/10 (0.5707)AttentionModel-training is done: 10/10
2020-02-29 | reset count: 0 | final loss: 0.5707 at epoch 10
making sample. (inference: True, date:2020-04-30 00:00:00, date_number: 400)
making sample. (inference: False, date:2020-03-31 00:00:00, date_number: 399)
[AttentionModel/2020-04-30 | get_logger | INFO]: ====================AttentionModel/2020-04-30 start====================
making sample. (inference: True, date:2020-05-31 00:00:00, date_number: 401)
making sample. (inference: False, date:2020-04-30 00:00:00, date_number: 400)
[AttentionModel/2020-05-31 | get_logger | INFO]: ====================AttentionModel/2020-05-31 start====================
[strategy_integration.py] set_seed(2020-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5745)training: 2/10 (0.5745)validation : 2/10 (0.5735)training: 3/10 (0.5735)validation : 3/10 (0.5668)training: 4/10 (0.5668)validation : 4/10 (0.5668)training: 5/10 (0.5668)validation : 5/10 (0.5668)training: 6/10 (0.5668)validation : 6/10 (0.5668)early stopping at 6 with loss 0.5668
AttentionModel-training is done: 6/10
2020-04-30 | reset count: 0 | final loss: 0.5668 at epoch 3
[strategy_integration.py] set_seed(2020-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5718)training: 2/10 (0.5718)validation : 2/10 (0.5716)training: 3/10 (0.5716)validation : 3/10 (0.5716)training: 4/10 (0.5716)validation : 4/10 (0.5716)training: 5/10 (0.5716)validation : 5/10 (0.5716)early stopping at 5 with loss 0.5716
AttentionModel-training is done: 5/10
2020-05-31 | reset count: 0 | final loss: 0.5716 at epoch 2
making sample. (inference: True, date:2020-06-30 00:00:00, date_number: 402)
making sample. (inference: False, date:2020-05-31 00:00:00, date_number: 401)
[AttentionModel/2020-06-30 | get_logger | INFO]: ====================AttentionModel/2020-06-30 start====================
making sample. (inference: True, date:2020-07-31 00:00:00, date_number: 403)
making sample. (inference: False, date:2020-06-30 00:00:00, date_number: 402)
[AttentionModel/2020-07-31 | get_logger | INFO]: ====================AttentionModel/2020-07-31 start====================
[strategy_integration.py] set_seed(2020-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5807)training: 2/10 (0.5807)validation : 2/10 (0.5780)training: 3/10 (0.5780)validation : 3/10 (0.5780)training: 4/10 (0.5780)validation : 4/10 (0.5780)training: 5/10 (0.5780)validation : 5/10 (0.5778)early stopping at 5 with loss 0.5778
AttentionModel-training is done: 5/10
2020-06-30 | reset count: 0 | final loss: 0.5778 at epoch 5
making sample. (inference: True, date:2020-08-31 00:00:00, date_number: 404)
making sample. (inference: False, date:2020-07-31 00:00:00, date_number: 403)
[AttentionModel/2020-08-31 | get_logger | INFO]: ====================AttentionModel/2020-08-31 start====================
[strategy_integration.py] set_seed(2020-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5737)training: 2/10 (0.5737)validation : 2/10 (0.5691)training: 3/10 (0.5691)validation : 3/10 (0.5687)training: 4/10 (0.5687)validation : 4/10 (0.5687)training: 5/10 (0.5687)validation : 5/10 (0.5687)training: 6/10 (0.5687)validation : 6/10 (0.5687)early stopping at 6 with loss 0.5687
AttentionModel-training is done: 6/10
2020-07-31 | reset count: 0 | final loss: 0.5687 at epoch 3
making sample. (inference: True, date:2020-09-30 00:00:00, date_number: 405)
making sample. (inference: False, date:2020-08-31 00:00:00, date_number: 404)
[AttentionModel/2020-09-30 | get_logger | INFO]: ====================AttentionModel/2020-09-30 start====================
[strategy_integration.py] set_seed(2020-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5662)training: 2/10 (0.5662)validation : 2/10 (0.5659)training: 3/10 (0.5659)validation : 3/10 (0.5646)training: 4/10 (0.5646)validation : 4/10 (0.5615)training: 5/10 (0.5615)validation : 5/10 (0.5615)training: 6/10 (0.5615)validation : 6/10 (0.5615)early stopping at 6 with loss 0.5615
AttentionModel-training is done: 6/10
2020-08-31 | reset count: 0 | final loss: 0.5615 at epoch 4
[strategy_integration.py] set_seed(2020-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5788)training: 2/10 (0.5788)validation : 2/10 (0.5688)training: 3/10 (0.5688)validation : 3/10 (0.5688)training: 4/10 (0.5688)validation : 4/10 (0.5688)training: 5/10 (0.5688)validation : 5/10 (0.5688)early stopping at 5 with loss 0.5688
AttentionModel-training is done: 5/10
2020-09-30 | reset count: 0 | final loss: 0.5688 at epoch 2
making sample. (inference: True, date:2020-10-31 00:00:00, date_number: 406)
making sample. (inference: False, date:2020-09-30 00:00:00, date_number: 405)
[AttentionModel/2020-10-31 | get_logger | INFO]: ====================AttentionModel/2020-10-31 start====================
making sample. (inference: True, date:2020-11-30 00:00:00, date_number: 407)
making sample. (inference: False, date:2020-10-31 00:00:00, date_number: 406)
[AttentionModel/2020-11-30 | get_logger | INFO]: ====================AttentionModel/2020-11-30 start====================
[strategy_integration.py] set_seed(2020-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5647)training: 2/10 (0.5647)validation : 2/10 (0.5647)training: 3/10 (0.5647)validation : 3/10 (0.5647)training: 4/10 (0.5647)validation : 4/10 (0.5647)training: 5/10 (0.5647)validation : 5/10 (0.5647)training: 6/10 (0.5647)validation : 6/10 (0.5647)training: 7/10 (0.5647)validation : 7/10 (0.5647)early stopping at 7 with loss 0.5647
AttentionModel-training is done: 7/10
2020-10-31 | reset count: 0 | final loss: 0.5647 at epoch 1
[strategy_integration.py] set_seed(2020-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5653)training: 2/10 (0.5653)validation : 2/10 (0.5644)training: 3/10 (0.5644)validation : 3/10 (0.5636)training: 4/10 (0.5636)validation : 4/10 (0.5623)training: 5/10 (0.5623)validation : 5/10 (0.5623)training: 6/10 (0.5623)validation : 6/10 (0.5623)early stopping at 6 with loss 0.5623
AttentionModel-training is done: 6/10
2020-11-30 | reset count: 0 | final loss: 0.5623 at epoch 4
making sample. (inference: True, date:2020-12-31 00:00:00, date_number: 408)
making sample. (inference: False, date:2020-11-30 00:00:00, date_number: 407)
[AttentionModel/2020-12-31 | get_logger | INFO]: ====================AttentionModel/2020-12-31 start====================
making sample. (inference: True, date:2021-01-31 00:00:00, date_number: 409)
making sample. (inference: False, date:2020-12-31 00:00:00, date_number: 408)
[AttentionModel/2021-01-31 | get_logger | INFO]: ====================AttentionModel/2021-01-31 start====================
[strategy_integration.py] set_seed(2021-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5573)training: 2/10 (0.5573)validation : 2/10 (0.5523)training: 3/10 (0.5523)validation : 3/10 (0.5501)training: 4/10 (0.5501)validation : 4/10 (0.5501)training: 5/10 (0.5501)validation : 5/10 (0.5501)early stopping at 5 with loss 0.5501
AttentionModel-training is done: 5/10
2021-01-31 | reset count: 0 | final loss: 0.5501 at epoch 3
[strategy_integration.py] set_seed(2020-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5755)training: 2/10 (0.5755)validation : 2/10 (0.5686)training: 3/10 (0.5686)validation : 3/10 (0.5686)training: 4/10 (0.5686)validation : 4/10 (0.5686)training: 5/10 (0.5686)validation : 5/10 (0.5686)training: 6/10 (0.5686)validation : 6/10 (0.5686)training: 7/10 (0.5686)validation : 7/10 (0.5671)training: 8/10 (0.5671)validation : 8/10 (0.5671)training: 9/10 (0.5671)validation : 9/10 (0.5671)training: 10/10 (0.5671)validation : 10/10 (0.5671)early stopping at 10 with loss 0.5671
AttentionModel-training is done: 10/10
2020-12-31 | reset count: 0 | final loss: 0.5671 at epoch 7
making sample. (inference: True, date:2021-02-28 00:00:00, date_number: 410)
making sample. (inference: False, date:2021-01-31 00:00:00, date_number: 409)
[AttentionModel/2021-02-28 | get_logger | INFO]: ====================AttentionModel/2021-02-28 start====================
making sample. (inference: True, date:2021-03-31 00:00:00, date_number: 411)
making sample. (inference: False, date:2021-02-28 00:00:00, date_number: 410)
[AttentionModel/2021-03-31 | get_logger | INFO]: ====================AttentionModel/2021-03-31 start====================
[strategy_integration.py] set_seed(2021-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5695)training: 2/10 (0.5695)validation : 2/10 (0.5691)training: 3/10 (0.5691)validation : 3/10 (0.5662)training: 4/10 (0.5662)validation : 4/10 (0.5662)training: 5/10 (0.5662)validation : 5/10 (0.5651)training: 6/10 (0.5651)validation : 6/10 (0.5651)early stopping at 6 with loss 0.5651
AttentionModel-training is done: 6/10
2021-02-28 | reset count: 0 | final loss: 0.5651 at epoch 5
[strategy_integration.py] set_seed(2021-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5561)training: 2/10 (0.5561)validation : 2/10 (0.5549)training: 3/10 (0.5549)validation : 3/10 (0.5536)training: 4/10 (0.5536)validation : 4/10 (0.5536)training: 5/10 (0.5536)validation : 5/10 (0.5536)early stopping at 5 with loss 0.5536
AttentionModel-training is done: 5/10
2021-03-31 | reset count: 0 | final loss: 0.5536 at epoch 3
making sample. (inference: True, date:2021-04-30 00:00:00, date_number: 412)
making sample. (inference: False, date:2021-03-31 00:00:00, date_number: 411)
[AttentionModel/2021-04-30 | get_logger | INFO]: ====================AttentionModel/2021-04-30 start====================
making sample. (inference: True, date:2021-05-31 00:00:00, date_number: 413)
making sample. (inference: False, date:2021-04-30 00:00:00, date_number: 412)
[AttentionModel/2021-05-31 | get_logger | INFO]: ====================AttentionModel/2021-05-31 start====================
[strategy_integration.py] set_seed(2021-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5613)training: 2/10 (0.5613)validation : 2/10 (0.5613)training: 3/10 (0.5613)validation : 3/10 (0.5613)training: 4/10 (0.5613)validation : 4/10 (0.5591)training: 5/10 (0.5591)validation : 5/10 (0.5577)training: 6/10 (0.5577)validation : 6/10 (0.5577)early stopping at 6 with loss 0.5577
AttentionModel-training is done: 6/10
2021-04-30 | reset count: 0 | final loss: 0.5577 at epoch 5
making sample. (inference: True, date:2021-06-30 00:00:00, date_number: 414)
making sample. (inference: False, date:2021-05-31 00:00:00, date_number: 413)
[AttentionModel/2021-06-30 | get_logger | INFO]: ====================AttentionModel/2021-06-30 start====================
[strategy_integration.py] set_seed(2021-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5667)training: 2/10 (0.5667)validation : 2/10 (0.5621)training: 3/10 (0.5621)validation : 3/10 (0.5621)training: 4/10 (0.5621)validation : 4/10 (0.5621)training: 5/10 (0.5621)validation : 5/10 (0.5598)training: 6/10 (0.5598)validation : 6/10 (0.5598)training: 7/10 (0.5598)validation : 7/10 (0.5598)early stopping at 7 with loss 0.5598
AttentionModel-training is done: 7/10
2021-05-31 | reset count: 0 | final loss: 0.5598 at epoch 5
making sample. (inference: True, date:2021-07-31 00:00:00, date_number: 415)
making sample. (inference: False, date:2021-06-30 00:00:00, date_number: 414)
[AttentionModel/2021-07-31 | get_logger | INFO]: ====================AttentionModel/2021-07-31 start====================
[strategy_integration.py] set_seed(2021-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5725)training: 2/10 (0.5725)validation : 2/10 (0.5725)training: 3/10 (0.5725)validation : 3/10 (0.5725)training: 4/10 (0.5725)validation : 4/10 (0.5725)training: 5/10 (0.5725)validation : 5/10 (0.5685)training: 6/10 (0.5685)validation : 6/10 (0.5685)early stopping at 6 with loss 0.5685
AttentionModel-training is done: 6/10
2021-06-30 | reset count: 0 | final loss: 0.5685 at epoch 5
making sample. (inference: True, date:2021-08-31 00:00:00, date_number: 416)
making sample. (inference: False, date:2021-07-31 00:00:00, date_number: 415)
[strategy_integration.py] set_seed(2021-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5867)training: 2/10 (0.5867)validation : 2/10 (0.5852)training: 3/10 (0.5852)validation : 3/10 (0.5824)training: 4/10 (0.5824)validation : 4/10 (0.5824)training: 5/10 (0.5824)validation : 5/10 (0.5824)training: 6/10 (0.5824)validation : 6/10 (0.5824)early stopping at 6 with loss 0.5824
AttentionModel-training is done: 6/10
2021-07-31 | reset count: 0 | final loss: 0.5824 at epoch 3
[AttentionModel/2021-08-31 | get_logger | INFO]: ====================AttentionModel/2021-08-31 start====================
making sample. (inference: True, date:2021-09-30 00:00:00, date_number: 417)
making sample. (inference: False, date:2021-08-31 00:00:00, date_number: 416)
[AttentionModel/2021-09-30 | get_logger | INFO]: ====================AttentionModel/2021-09-30 start====================
[strategy_integration.py] set_seed(2021-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5780)training: 2/10 (0.5780)validation : 2/10 (0.5749)training: 3/10 (0.5749)validation : 3/10 (0.5749)training: 4/10 (0.5749)validation : 4/10 (0.5749)training: 5/10 (0.5749)validation : 5/10 (0.5749)early stopping at 5 with loss 0.5749
AttentionModel-training is done: 5/10
2021-08-31 | reset count: 0 | final loss: 0.5749 at epoch 2
[strategy_integration.py] set_seed(2021-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5651)training: 2/10 (0.5651)validation : 2/10 (0.5638)training: 3/10 (0.5638)validation : 3/10 (0.5638)training: 4/10 (0.5638)validation : 4/10 (0.5638)training: 5/10 (0.5638)validation : 5/10 (0.5638)early stopping at 5 with loss 0.5638
AttentionModel-training is done: 5/10
2021-09-30 | reset count: 0 | final loss: 0.5638 at epoch 2
making sample. (inference: True, date:2021-10-31 00:00:00, date_number: 418)
making sample. (inference: False, date:2021-09-30 00:00:00, date_number: 417)
[AttentionModel/2021-10-31 | get_logger | INFO]: ====================AttentionModel/2021-10-31 start====================
[strategy_integration.py] set_seed(2021-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5760)training: 2/10 (0.5760)validation : 2/10 (0.5760)training: 3/10 (0.5760)validation : 3/10 (0.5760)training: 4/10 (0.5760)validation : 4/10 (0.5760)training: 5/10 (0.5760)validation : 5/10 (0.5760)early stopping at 5 with loss 0.5760
AttentionModel-training is done: 5/10
2021-10-31 | reset count: 0 | final loss: 0.5760 at epoch 1
[strategy | get_logger | INFO]: ====================strategy start====================
[strategy | _get_infer_dirs | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick250_test03/infer 이 strategy 대상에 추가됨
[strategy | _backtest | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick250_test03/infer is starting
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.155 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading masking data⠦ [Compustat API] : Loading get_historic_universe from cache⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading get_historic_universe from cache done in 1.953 secs
⠧ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.254 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.064 secs
⠋ [Compustat API] : Loading get_monthly_total_return from cache✓ [Compustat API] : Loading get_monthly_total_return from cache done in 0.063 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.077 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.072 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.07 secs
⠋ [Compustat API] : Loading get_monthly_volume_data from cache✓ [Compustat API] : Loading get_monthly_volume_data from cache done in 0.08 secs
[strategy | _allocating | INFO]: [STRATEGY] allocating start
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_weight_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] performance.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] combined.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] weight.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] universe.csv is saved.
[comparison | get_logger | INFO]: ====================comparison start====================
[comparison | compare | INFO]: [5. COMPARISON] comparison is saved.
[comparison | compare | INFO]:                        Harvest_Green_concept001_pick250_test03_price_return  ...  SPCOMP_total_win
2016-01-31_2021-04-30                                           0.234735     ...          0.765625

[1 rows x 27 columns]
2015-12-31
2016-01-31
2016-02-29
2016-03-31
2016-04-30
2016-05-31
2016-06-30
2016-07-31
2016-08-31
2016-09-30
2016-10-31
2016-11-30
2016-12-31
2017-01-31
2017-02-28
2017-03-31
2017-04-30
2017-05-31
2017-06-30
2017-07-31
2017-08-31
2017-09-30
2017-10-31
2017-11-30
2017-12-31
2018-01-31
2018-02-28
2018-03-31
2018-04-30
2018-05-31
2018-06-30
2018-07-31
2018-08-31
2018-09-30
2018-10-31
2018-11-30
2018-12-31
2019-01-31
2019-02-28
2019-03-31
2019-04-30
2019-05-31
2019-06-30
2019-07-31
2019-08-31
2019-09-30
2019-10-31
2019-11-30
2019-12-31
2020-01-31
2020-02-29
2020-03-31
2020-04-30
2020-05-31
2020-06-30
2020-07-31
2020-08-31
2020-09-30
2020-10-31
2020-11-30
2020-12-31
2021-01-31
2021-02-28
2021-03-31
2021-04-30
2021-05-31
2021-06-30
2021-07-31
2021-08-31
2021-09-30
2021-10-31
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.049 secs
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.046 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.065 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.081 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.075 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.683 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.68 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.678 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.684 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.685 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.076 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.67 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.67 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.671 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.664 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.661 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.079 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.684 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.076 secs
⠋ [Compustat API] : Cleaning past days cache✓ [Compustat API] : Cleaning past days cache done in 0.0 secs
auto caching이 활성화 되었습니다. /home/sronly/sr-storage/kirin_cache/kirin_api_cache/2.19.17_2021-11-23에 캐쉬를 저장합니다.
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.148 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache✓ [Compustat API] : Loading get_historic_universe from cache done in 2.182 secs
⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.483 secs
setting tensorflow random seed failed
load_data: op_lev
load_data: roe
load_data: std_u_e
load_data: retail_mfr
load_data: m1
load_data: m2
load_data: ae_m
load_data: ia_ta
load_data: rc_a
load_data: t1y_ff
load_data: t5y_ff
load_data: t10y_t2y
load_data: export_growth
load_data: import_growth
load_data: real_gig
load_data: pr_1m_0m
load_data: mv
load_data: btm
load_data: t3y
load_data: t5y
load_data: t7y
load_data: aaa_t10y
load_data: baa_t10y
load_data: aaa_ff
load_data: real_pig
load_data: federal_tg
load_data: real_gdp
load_data: res_mom_12m_1m_0m
load_data: res_vol_6m_3m_0m
load_data: at
load_data: gpa
load_data: rev_surp
load_data: cash_at
load_data: baa_ff
load_data: core_cpi
load_data: core_pce
load_data: corporate_tg
load_data: industrial_prod
load_data: home_pr
load_data: mom_12m_1m
load_data: ram_12m_0m
load_data: vol_3m
load_data: snp500_pr
load_data: wilshire500_pr
load_data: ted
load_data: ppi
load_data: trimmed_pce
load_data: unemploy
load_data: snp500_vol
load_data: r_s
load_data: r_a
load_data: fred_ff
load_data: t3m
load_data: t6m
load_data: t2y
load_data: t10y
load_data: aaa
load_data: baa
load_data: core_ppi
load_data: cpi
load_data: pce
load_data: wti
load_data: capa_util
load_data: snp500_pe
load_data: sector_values
load_data: mv
load_data: pr_1m_0m
load_data: mv
making sample. (inference: True, date:2015-12-31 00:00:00, date_number: 348)
making sample. (inference: False, date:2012-12-31 00:00:00, date_number: 312)
making sample. (inference: False, date:2013-01-31 00:00:00, date_number: 313)
making sample. (inference: False, date:2013-02-28 00:00:00, date_number: 314)
making sample. (inference: False, date:2013-03-31 00:00:00, date_number: 315)
making sample. (inference: False, date:2013-04-30 00:00:00, date_number: 316)
making sample. (inference: False, date:2013-05-31 00:00:00, date_number: 317)
making sample. (inference: False, date:2013-06-30 00:00:00, date_number: 318)
making sample. (inference: False, date:2013-07-31 00:00:00, date_number: 319)
making sample. (inference: False, date:2013-08-31 00:00:00, date_number: 320)
making sample. (inference: False, date:2013-09-30 00:00:00, date_number: 321)
making sample. (inference: False, date:2013-10-31 00:00:00, date_number: 322)
making sample. (inference: False, date:2013-11-30 00:00:00, date_number: 323)
making sample. (inference: False, date:2013-12-31 00:00:00, date_number: 324)
making sample. (inference: False, date:2014-01-31 00:00:00, date_number: 325)
making sample. (inference: False, date:2014-02-28 00:00:00, date_number: 326)
making sample. (inference: False, date:2014-03-31 00:00:00, date_number: 327)
making sample. (inference: False, date:2014-04-30 00:00:00, date_number: 328)
making sample. (inference: False, date:2014-05-31 00:00:00, date_number: 329)
making sample. (inference: False, date:2014-06-30 00:00:00, date_number: 330)
making sample. (inference: False, date:2014-07-31 00:00:00, date_number: 331)
making sample. (inference: False, date:2014-08-31 00:00:00, date_number: 332)
making sample. (inference: False, date:2014-09-30 00:00:00, date_number: 333)
making sample. (inference: False, date:2014-10-31 00:00:00, date_number: 334)
making sample. (inference: False, date:2014-11-30 00:00:00, date_number: 335)
making sample. (inference: False, date:2014-12-31 00:00:00, date_number: 336)
making sample. (inference: False, date:2015-01-31 00:00:00, date_number: 337)
making sample. (inference: False, date:2015-02-28 00:00:00, date_number: 338)
making sample. (inference: False, date:2015-03-31 00:00:00, date_number: 339)
making sample. (inference: False, date:2015-04-30 00:00:00, date_number: 340)
making sample. (inference: False, date:2015-05-31 00:00:00, date_number: 341)
making sample. (inference: False, date:2015-06-30 00:00:00, date_number: 342)
making sample. (inference: False, date:2015-07-31 00:00:00, date_number: 343)
making sample. (inference: False, date:2015-08-31 00:00:00, date_number: 344)
making sample. (inference: False, date:2015-09-30 00:00:00, date_number: 345)
making sample. (inference: False, date:2015-10-31 00:00:00, date_number: 346)
making sample. (inference: False, date:2015-11-30 00:00:00, date_number: 347)
[AttentionModel/2015-12-31 | get_logger | INFO]: ====================AttentionModel/2015-12-31 start====================
making sample. (inference: True, date:2016-01-31 00:00:00, date_number: 349)
making sample. (inference: False, date:2015-12-31 00:00:00, date_number: 348)
[AttentionModel/2016-01-31 | get_logger | INFO]: ====================AttentionModel/2016-01-31 start====================
[strategy_integration.py] set_seed(2015-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5761)training: 2/10 (0.5761)validation : 2/10 (0.5757)training: 3/10 (0.5757)validation : 3/10 (0.5757)training: 4/10 (0.5757)validation : 4/10 (0.5757)training: 5/10 (0.5757)validation : 5/10 (0.5757)early stopping at 5 with loss 0.5757
AttentionModel-training is done: 5/10
2015-12-31 | reset count: 0 | final loss: 0.5757 at epoch 2
making sample. (inference: True, date:2016-02-29 00:00:00, date_number: 350)
making sample. (inference: False, date:2016-01-31 00:00:00, date_number: 349)
[AttentionModel/2016-02-29 | get_logger | INFO]: ====================AttentionModel/2016-02-29 start====================
[strategy_integration.py] set_seed(2016-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5761)training: 2/10 (0.5761)validation : 2/10 (0.5726)training: 3/10 (0.5726)validation : 3/10 (0.5726)training: 4/10 (0.5726)validation : 4/10 (0.5726)training: 5/10 (0.5726)validation : 5/10 (0.5726)training: 6/10 (0.5726)validation : 6/10 (0.5707)training: 7/10 (0.5707)validation : 7/10 (0.5707)early stopping at 7 with loss 0.5707
AttentionModel-training is done: 7/10
2016-01-31 | reset count: 0 | final loss: 0.5707 at epoch 6
making sample. (inference: True, date:2016-03-31 00:00:00, date_number: 351)
making sample. (inference: False, date:2016-02-29 00:00:00, date_number: 350)
[AttentionModel/2016-03-31 | get_logger | INFO]: ====================AttentionModel/2016-03-31 start====================
[strategy_integration.py] set_seed(2016-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5814)training: 2/10 (0.5814)validation : 2/10 (0.5768)training: 3/10 (0.5768)validation : 3/10 (0.5768)training: 4/10 (0.5768)validation : 4/10 (0.5749)training: 5/10 (0.5749)validation : 5/10 (0.5734)training: 6/10 (0.5734)validation : 6/10 (0.5722)training: 7/10 (0.5722)validation : 7/10 (0.5722)early stopping at 7 with loss 0.5722
AttentionModel-training is done: 7/10
2016-02-29 | reset count: 0 | final loss: 0.5722 at epoch 6
making sample. (inference: True, date:2016-04-30 00:00:00, date_number: 352)
making sample. (inference: False, date:2016-03-31 00:00:00, date_number: 351)
[AttentionModel/2016-04-30 | get_logger | INFO]: ====================AttentionModel/2016-04-30 start====================
[strategy_integration.py] set_seed(2016-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5674)training: 2/10 (0.5674)validation : 2/10 (0.5674)training: 3/10 (0.5674)validation : 3/10 (0.5651)training: 4/10 (0.5651)validation : 4/10 (0.5651)training: 5/10 (0.5651)validation : 5/10 (0.5651)early stopping at 5 with loss 0.5651
AttentionModel-training is done: 5/10
2016-03-31 | reset count: 0 | final loss: 0.5651 at epoch 3
making sample. (inference: True, date:2016-05-31 00:00:00, date_number: 353)
making sample. (inference: False, date:2016-04-30 00:00:00, date_number: 352)
[AttentionModel/2016-05-31 | get_logger | INFO]: ====================AttentionModel/2016-05-31 start====================
[strategy_integration.py] set_seed(2016-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5776)training: 2/10 (0.5776)validation : 2/10 (0.5760)training: 3/10 (0.5760)validation : 3/10 (0.5760)training: 4/10 (0.5760)validation : 4/10 (0.5731)training: 5/10 (0.5731)validation : 5/10 (0.5731)training: 6/10 (0.5731)validation : 6/10 (0.5731)training: 7/10 (0.5731)validation : 7/10 (0.5731)early stopping at 7 with loss 0.5731
AttentionModel-training is done: 7/10
2016-04-30 | reset count: 0 | final loss: 0.5731 at epoch 4
making sample. (inference: True, date:2016-06-30 00:00:00, date_number: 354)
making sample. (inference: False, date:2016-05-31 00:00:00, date_number: 353)
[AttentionModel/2016-06-30 | get_logger | INFO]: ====================AttentionModel/2016-06-30 start====================
[strategy_integration.py] set_seed(2016-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5709)training: 2/10 (0.5709)validation : 2/10 (0.5709)training: 3/10 (0.5709)validation : 3/10 (0.5691)training: 4/10 (0.5691)validation : 4/10 (0.5685)training: 5/10 (0.5685)validation : 5/10 (0.5685)training: 6/10 (0.5685)validation : 6/10 (0.5646)training: 7/10 (0.5646)validation : 7/10 (0.5646)early stopping at 7 with loss 0.5646
AttentionModel-training is done: 7/10
2016-05-31 | reset count: 0 | final loss: 0.5646 at epoch 6
making sample. (inference: True, date:2016-07-31 00:00:00, date_number: 355)
making sample. (inference: False, date:2016-06-30 00:00:00, date_number: 354)
[AttentionModel/2016-07-31 | get_logger | INFO]: ====================AttentionModel/2016-07-31 start====================
[strategy_integration.py] set_seed(2016-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5660)training: 2/10 (0.5660)validation : 2/10 (0.5615)training: 3/10 (0.5615)validation : 3/10 (0.5597)training: 4/10 (0.5597)validation : 4/10 (0.5597)training: 5/10 (0.5597)validation : 5/10 (0.5593)training: 6/10 (0.5593)validation : 6/10 (0.5593)early stopping at 6 with loss 0.5593
AttentionModel-training is done: 6/10
2016-06-30 | reset count: 0 | final loss: 0.5593 at epoch 5
making sample. (inference: True, date:2016-08-31 00:00:00, date_number: 356)
making sample. (inference: False, date:2016-07-31 00:00:00, date_number: 355)
[AttentionModel/2016-08-31 | get_logger | INFO]: ====================AttentionModel/2016-08-31 start====================
[strategy_integration.py] set_seed(2016-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5718)training: 2/10 (0.5718)validation : 2/10 (0.5685)training: 3/10 (0.5685)validation : 3/10 (0.5685)training: 4/10 (0.5685)validation : 4/10 (0.5685)training: 5/10 (0.5685)validation : 5/10 (0.5685)early stopping at 5 with loss 0.5685
AttentionModel-training is done: 5/10
2016-07-31 | reset count: 0 | final loss: 0.5685 at epoch 2
making sample. (inference: True, date:2016-09-30 00:00:00, date_number: 357)
making sample. (inference: False, date:2016-08-31 00:00:00, date_number: 356)
[AttentionModel/2016-09-30 | get_logger | INFO]: ====================AttentionModel/2016-09-30 start====================
[strategy_integration.py] set_seed(2016-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5739)training: 2/10 (0.5739)validation : 2/10 (0.5716)training: 3/10 (0.5716)validation : 3/10 (0.5716)training: 4/10 (0.5716)validation : 4/10 (0.5687)training: 5/10 (0.5687)validation : 5/10 (0.5687)training: 6/10 (0.5687)validation : 6/10 (0.5687)training: 7/10 (0.5687)validation : 7/10 (0.5687)early stopping at 7 with loss 0.5687
AttentionModel-training is done: 7/10
2016-08-31 | reset count: 0 | final loss: 0.5687 at epoch 4
[strategy_integration.py] set_seed(2016-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5683)training: 2/10 (0.5683)validation : 2/10 (0.5673)training: 3/10 (0.5673)validation : 3/10 (0.5673)training: 4/10 (0.5673)validation : 4/10 (0.5673)training: 5/10 (0.5673)validation : 5/10 (0.5673)early stopping at 5 with loss 0.5673
AttentionModel-training is done: 5/10
2016-09-30 | reset count: 0 | final loss: 0.5673 at epoch 2
making sample. (inference: True, date:2016-10-31 00:00:00, date_number: 358)
making sample. (inference: False, date:2016-09-30 00:00:00, date_number: 357)
[AttentionModel/2016-10-31 | get_logger | INFO]: ====================AttentionModel/2016-10-31 start====================
making sample. (inference: True, date:2016-11-30 00:00:00, date_number: 359)
making sample. (inference: False, date:2016-10-31 00:00:00, date_number: 358)
[AttentionModel/2016-11-30 | get_logger | INFO]: ====================AttentionModel/2016-11-30 start====================
[strategy_integration.py] set_seed(2016-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5661)training: 2/10 (0.5661)validation : 2/10 (0.5661)training: 3/10 (0.5661)validation : 3/10 (0.5661)training: 4/10 (0.5661)validation : 4/10 (0.5661)training: 5/10 (0.5661)validation : 5/10 (0.5661)early stopping at 5 with loss 0.5661
AttentionModel-training is done: 5/10
2016-10-31 | reset count: 0 | final loss: 0.5661 at epoch 1
[strategy_integration.py] set_seed(2016-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5738)training: 2/10 (0.5738)validation : 2/10 (0.5697)training: 3/10 (0.5697)validation : 3/10 (0.5697)training: 4/10 (0.5697)validation : 4/10 (0.5697)training: 5/10 (0.5697)validation : 5/10 (0.5697)early stopping at 5 with loss 0.5697
AttentionModel-training is done: 5/10
2016-11-30 | reset count: 0 | final loss: 0.5697 at epoch 2
making sample. (inference: True, date:2016-12-31 00:00:00, date_number: 360)
making sample. (inference: False, date:2016-11-30 00:00:00, date_number: 359)
[AttentionModel/2016-12-31 | get_logger | INFO]: ====================AttentionModel/2016-12-31 start====================
making sample. (inference: True, date:2017-01-31 00:00:00, date_number: 361)
making sample. (inference: False, date:2016-12-31 00:00:00, date_number: 360)
[AttentionModel/2017-01-31 | get_logger | INFO]: ====================AttentionModel/2017-01-31 start====================
[strategy_integration.py] set_seed(2016-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5657)training: 2/10 (0.5657)validation : 2/10 (0.5620)training: 3/10 (0.5620)validation : 3/10 (0.5615)training: 4/10 (0.5615)validation : 4/10 (0.5609)training: 5/10 (0.5609)validation : 5/10 (0.5609)training: 6/10 (0.5609)validation : 6/10 (0.5609)early stopping at 6 with loss 0.5609
AttentionModel-training is done: 6/10
2016-12-31 | reset count: 0 | final loss: 0.5609 at epoch 4
[strategy_integration.py] set_seed(2017-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5766)training: 2/10 (0.5766)validation : 2/10 (0.5709)training: 3/10 (0.5709)validation : 3/10 (0.5709)training: 4/10 (0.5709)validation : 4/10 (0.5709)training: 5/10 (0.5709)validation : 5/10 (0.5686)training: 6/10 (0.5686)validation : 6/10 (0.5686)training: 7/10 (0.5686)validation : 7/10 (0.5686)early stopping at 7 with loss 0.5686
AttentionModel-training is done: 7/10
2017-01-31 | reset count: 0 | final loss: 0.5686 at epoch 5
making sample. (inference: True, date:2017-02-28 00:00:00, date_number: 362)
making sample. (inference: False, date:2017-01-31 00:00:00, date_number: 361)
[AttentionModel/2017-02-28 | get_logger | INFO]: ====================AttentionModel/2017-02-28 start====================
making sample. (inference: True, date:2017-03-31 00:00:00, date_number: 363)
making sample. (inference: False, date:2017-02-28 00:00:00, date_number: 362)
[AttentionModel/2017-03-31 | get_logger | INFO]: ====================AttentionModel/2017-03-31 start====================
[strategy_integration.py] set_seed(2017-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5676)training: 2/10 (0.5676)validation : 2/10 (0.5639)training: 3/10 (0.5639)validation : 3/10 (0.5617)training: 4/10 (0.5617)validation : 4/10 (0.5617)training: 5/10 (0.5617)validation : 5/10 (0.5617)training: 6/10 (0.5617)validation : 6/10 (0.5617)early stopping at 6 with loss 0.5617
AttentionModel-training is done: 6/10
2017-02-28 | reset count: 0 | final loss: 0.5617 at epoch 3
making sample. (inference: True, date:2017-04-30 00:00:00, date_number: 364)
making sample. (inference: False, date:2017-03-31 00:00:00, date_number: 363)
[AttentionModel/2017-04-30 | get_logger | INFO]: ====================AttentionModel/2017-04-30 start====================
[strategy_integration.py] set_seed(2017-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5675)training: 2/10 (0.5675)validation : 2/10 (0.5626)training: 3/10 (0.5626)validation : 3/10 (0.5626)training: 4/10 (0.5626)validation : 4/10 (0.5595)training: 5/10 (0.5595)validation : 5/10 (0.5595)training: 6/10 (0.5595)validation : 6/10 (0.5595)training: 7/10 (0.5595)validation : 7/10 (0.5595)early stopping at 7 with loss 0.5595
AttentionModel-training is done: 7/10
2017-03-31 | reset count: 0 | final loss: 0.5595 at epoch 4
making sample. (inference: True, date:2017-05-31 00:00:00, date_number: 365)
making sample. (inference: False, date:2017-04-30 00:00:00, date_number: 364)
[strategy_integration.py] set_seed(2017-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5535)training: 2/10 (0.5535)validation : 2/10 (0.5535)training: 3/10 (0.5535)validation : 3/10 (0.5535)training: 4/10 (0.5535)validation : 4/10 (0.5535)training: 5/10 (0.5535)validation : 5/10 (0.5535)early stopping at 5 with loss 0.5535
AttentionModel-training is done: 5/10
2017-04-30 | reset count: 0 | final loss: 0.5535 at epoch 1
[AttentionModel/2017-05-31 | get_logger | INFO]: ====================AttentionModel/2017-05-31 start====================
making sample. (inference: True, date:2017-06-30 00:00:00, date_number: 366)
making sample. (inference: False, date:2017-05-31 00:00:00, date_number: 365)
[AttentionModel/2017-06-30 | get_logger | INFO]: ====================AttentionModel/2017-06-30 start====================
[strategy_integration.py] set_seed(2017-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5487)training: 2/10 (0.5487)validation : 2/10 (0.5487)training: 3/10 (0.5487)validation : 3/10 (0.5487)training: 4/10 (0.5487)validation : 4/10 (0.5449)training: 5/10 (0.5449)validation : 5/10 (0.5430)training: 6/10 (0.5430)validation : 6/10 (0.5430)training: 7/10 (0.5430)validation : 7/10 (0.5430)training: 8/10 (0.5430)validation : 8/10 (0.5430)early stopping at 8 with loss 0.5430
AttentionModel-training is done: 8/10
2017-05-31 | reset count: 0 | final loss: 0.5430 at epoch 5
[strategy_integration.py] set_seed(2017-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5535)training: 2/10 (0.5535)validation : 2/10 (0.5535)training: 3/10 (0.5535)validation : 3/10 (0.5491)training: 4/10 (0.5491)validation : 4/10 (0.5491)training: 5/10 (0.5491)validation : 5/10 (0.5491)training: 6/10 (0.5491)validation : 6/10 (0.5491)early stopping at 6 with loss 0.5491
AttentionModel-training is done: 6/10
2017-06-30 | reset count: 0 | final loss: 0.5491 at epoch 3
making sample. (inference: True, date:2017-07-31 00:00:00, date_number: 367)
making sample. (inference: False, date:2017-06-30 00:00:00, date_number: 366)
[AttentionModel/2017-07-31 | get_logger | INFO]: ====================AttentionModel/2017-07-31 start====================
making sample. (inference: True, date:2017-08-31 00:00:00, date_number: 368)
making sample. (inference: False, date:2017-07-31 00:00:00, date_number: 367)
[AttentionModel/2017-08-31 | get_logger | INFO]: ====================AttentionModel/2017-08-31 start====================
[strategy_integration.py] set_seed(2017-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5616)training: 2/10 (0.5616)validation : 2/10 (0.5616)training: 3/10 (0.5616)validation : 3/10 (0.5616)training: 4/10 (0.5616)validation : 4/10 (0.5616)training: 5/10 (0.5616)validation : 5/10 (0.5605)training: 6/10 (0.5605)validation : 6/10 (0.5605)training: 7/10 (0.5605)validation : 7/10 (0.5605)training: 8/10 (0.5605)validation : 8/10 (0.5605)early stopping at 8 with loss 0.5605
AttentionModel-training is done: 8/10
2017-07-31 | reset count: 0 | final loss: 0.5605 at epoch 5
[strategy_integration.py] set_seed(2017-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5665)training: 2/10 (0.5665)validation : 2/10 (0.5648)training: 3/10 (0.5648)validation : 3/10 (0.5648)training: 4/10 (0.5648)validation : 4/10 (0.5640)training: 5/10 (0.5640)validation : 5/10 (0.5640)early stopping at 5 with loss 0.5640
AttentionModel-training is done: 5/10
2017-08-31 | reset count: 0 | final loss: 0.5640 at epoch 4
making sample. (inference: True, date:2017-09-30 00:00:00, date_number: 369)
making sample. (inference: False, date:2017-08-31 00:00:00, date_number: 368)
[AttentionModel/2017-09-30 | get_logger | INFO]: ====================AttentionModel/2017-09-30 start====================
making sample. (inference: True, date:2017-10-31 00:00:00, date_number: 370)
making sample. (inference: False, date:2017-09-30 00:00:00, date_number: 369)
[AttentionModel/2017-10-31 | get_logger | INFO]: ====================AttentionModel/2017-10-31 start====================
[strategy_integration.py] set_seed(2017-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5586)training: 2/10 (0.5586)validation : 2/10 (0.5572)training: 3/10 (0.5572)validation : 3/10 (0.5544)training: 4/10 (0.5544)validation : 4/10 (0.5544)training: 5/10 (0.5544)validation : 5/10 (0.5544)early stopping at 5 with loss 0.5544
AttentionModel-training is done: 5/10
2017-09-30 | reset count: 0 | final loss: 0.5544 at epoch 3
making sample. (inference: True, date:2017-11-30 00:00:00, date_number: 371)
making sample. (inference: False, date:2017-10-31 00:00:00, date_number: 370)
[AttentionModel/2017-11-30 | get_logger | INFO]: ====================AttentionModel/2017-11-30 start====================
[strategy_integration.py] set_seed(2017-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5591)training: 2/10 (0.5591)validation : 2/10 (0.5585)training: 3/10 (0.5585)validation : 3/10 (0.5569)training: 4/10 (0.5569)validation : 4/10 (0.5544)training: 5/10 (0.5544)validation : 5/10 (0.5544)training: 6/10 (0.5544)validation : 6/10 (0.5544)training: 7/10 (0.5544)validation : 7/10 (0.5544)early stopping at 7 with loss 0.5544
AttentionModel-training is done: 7/10
2017-10-31 | reset count: 0 | final loss: 0.5544 at epoch 4
[strategy_integration.py] set_seed(2017-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5551)training: 2/10 (0.5551)validation : 2/10 (0.5504)training: 3/10 (0.5504)validation : 3/10 (0.5498)training: 4/10 (0.5498)validation : 4/10 (0.5498)training: 5/10 (0.5498)validation : 5/10 (0.5498)training: 6/10 (0.5498)validation : 6/10 (0.5498)early stopping at 6 with loss 0.5498
AttentionModel-training is done: 6/10
2017-11-30 | reset count: 0 | final loss: 0.5498 at epoch 3
making sample. (inference: True, date:2017-12-31 00:00:00, date_number: 372)
making sample. (inference: False, date:2017-11-30 00:00:00, date_number: 371)
[AttentionModel/2017-12-31 | get_logger | INFO]: ====================AttentionModel/2017-12-31 start====================
making sample. (inference: True, date:2018-01-31 00:00:00, date_number: 373)
making sample. (inference: False, date:2017-12-31 00:00:00, date_number: 372)
[AttentionModel/2018-01-31 | get_logger | INFO]: ====================AttentionModel/2018-01-31 start====================
[strategy_integration.py] set_seed(2017-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5653)training: 2/10 (0.5653)validation : 2/10 (0.5609)training: 3/10 (0.5609)validation : 3/10 (0.5607)training: 4/10 (0.5607)validation : 4/10 (0.5607)training: 5/10 (0.5607)validation : 5/10 (0.5574)training: 6/10 (0.5574)validation : 6/10 (0.5574)early stopping at 6 with loss 0.5574
AttentionModel-training is done: 6/10
2017-12-31 | reset count: 0 | final loss: 0.5574 at epoch 5
making sample. (inference: True, date:2018-02-28 00:00:00, date_number: 374)
making sample. (inference: False, date:2018-01-31 00:00:00, date_number: 373)
[AttentionModel/2018-02-28 | get_logger | INFO]: ====================AttentionModel/2018-02-28 start====================
[strategy_integration.py] set_seed(2018-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5622)training: 2/10 (0.5622)validation : 2/10 (0.5582)training: 3/10 (0.5582)validation : 3/10 (0.5582)training: 4/10 (0.5582)validation : 4/10 (0.5582)training: 5/10 (0.5582)validation : 5/10 (0.5582)training: 6/10 (0.5582)validation : 6/10 (0.5582)training: 7/10 (0.5582)validation : 7/10 (0.5582)training: 8/10 (0.5582)validation : 8/10 (0.5582)early stopping at 8 with loss 0.5582
AttentionModel-training is done: 8/10
2018-01-31 | reset count: 0 | final loss: 0.5582 at epoch 2
making sample. (inference: True, date:2018-03-31 00:00:00, date_number: 375)
making sample. (inference: False, date:2018-02-28 00:00:00, date_number: 374)
[AttentionModel/2018-03-31 | get_logger | INFO]: ====================AttentionModel/2018-03-31 start====================
[strategy_integration.py] set_seed(2018-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5662)training: 2/10 (0.5662)validation : 2/10 (0.5630)training: 3/10 (0.5630)validation : 3/10 (0.5630)training: 4/10 (0.5630)validation : 4/10 (0.5630)training: 5/10 (0.5630)validation : 5/10 (0.5630)training: 6/10 (0.5630)validation : 6/10 (0.5627)training: 7/10 (0.5627)validation : 7/10 (0.5627)early stopping at 7 with loss 0.5627
AttentionModel-training is done: 7/10
2018-02-28 | reset count: 0 | final loss: 0.5627 at epoch 6
[strategy_integration.py] set_seed(2018-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5691)training: 2/10 (0.5691)validation : 2/10 (0.5687)training: 3/10 (0.5687)validation : 3/10 (0.5680)training: 4/10 (0.5680)validation : 4/10 (0.5670)training: 5/10 (0.5670)validation : 5/10 (0.5670)early stopping at 5 with loss 0.5670
AttentionModel-training is done: 5/10
2018-03-31 | reset count: 0 | final loss: 0.5670 at epoch 4
making sample. (inference: True, date:2018-04-30 00:00:00, date_number: 376)
making sample. (inference: False, date:2018-03-31 00:00:00, date_number: 375)
[AttentionModel/2018-04-30 | get_logger | INFO]: ====================AttentionModel/2018-04-30 start====================
making sample. (inference: True, date:2018-05-31 00:00:00, date_number: 377)
making sample. (inference: False, date:2018-04-30 00:00:00, date_number: 376)
[AttentionModel/2018-05-31 | get_logger | INFO]: ====================AttentionModel/2018-05-31 start====================
[strategy_integration.py] set_seed(2018-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5727)training: 2/10 (0.5727)validation : 2/10 (0.5709)training: 3/10 (0.5709)validation : 3/10 (0.5698)training: 4/10 (0.5698)validation : 4/10 (0.5698)training: 5/10 (0.5698)validation : 5/10 (0.5698)early stopping at 5 with loss 0.5698
AttentionModel-training is done: 5/10
2018-04-30 | reset count: 0 | final loss: 0.5698 at epoch 3
making sample. (inference: True, date:2018-06-30 00:00:00, date_number: 378)
making sample. (inference: False, date:2018-05-31 00:00:00, date_number: 377)
[strategy_integration.py] set_seed(2018-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5728)training: 2/10 (0.5728)validation : 2/10 (0.5728)training: 3/10 (0.5728)validation : 3/10 (0.5693)training: 4/10 (0.5693)validation : 4/10 (0.5673)training: 5/10 (0.5673)validation : 5/10 (0.5673)training: 6/10 (0.5673)validation : 6/10 (0.5673)early stopping at 6 with loss 0.5673
AttentionModel-training is done: 6/10
2018-05-31 | reset count: 0 | final loss: 0.5673 at epoch 4
[AttentionModel/2018-06-30 | get_logger | INFO]: ====================AttentionModel/2018-06-30 start====================
making sample. (inference: True, date:2018-07-31 00:00:00, date_number: 379)
making sample. (inference: False, date:2018-06-30 00:00:00, date_number: 378)
[AttentionModel/2018-07-31 | get_logger | INFO]: ====================AttentionModel/2018-07-31 start====================
[strategy_integration.py] set_seed(2018-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5678)training: 2/10 (0.5678)validation : 2/10 (0.5665)training: 3/10 (0.5665)validation : 3/10 (0.5665)training: 4/10 (0.5665)validation : 4/10 (0.5615)training: 5/10 (0.5615)validation : 5/10 (0.5615)training: 6/10 (0.5615)validation : 6/10 (0.5615)training: 7/10 (0.5615)validation : 7/10 (0.5615)early stopping at 7 with loss 0.5615
AttentionModel-training is done: 7/10
2018-06-30 | reset count: 0 | final loss: 0.5615 at epoch 4
[strategy_integration.py] set_seed(2018-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5691)training: 2/10 (0.5691)validation : 2/10 (0.5691)training: 3/10 (0.5691)validation : 3/10 (0.5691)training: 4/10 (0.5691)validation : 4/10 (0.5661)training: 5/10 (0.5661)validation : 5/10 (0.5661)training: 6/10 (0.5661)validation : 6/10 (0.5661)training: 7/10 (0.5661)validation : 7/10 (0.5661)early stopping at 7 with loss 0.5661
AttentionModel-training is done: 7/10
2018-07-31 | reset count: 0 | final loss: 0.5661 at epoch 4
making sample. (inference: True, date:2018-08-31 00:00:00, date_number: 380)
making sample. (inference: False, date:2018-07-31 00:00:00, date_number: 379)
[AttentionModel/2018-08-31 | get_logger | INFO]: ====================AttentionModel/2018-08-31 start====================
making sample. (inference: True, date:2018-09-30 00:00:00, date_number: 381)
making sample. (inference: False, date:2018-08-31 00:00:00, date_number: 380)
[AttentionModel/2018-09-30 | get_logger | INFO]: ====================AttentionModel/2018-09-30 start====================
[strategy_integration.py] set_seed(2018-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5736)training: 2/10 (0.5736)validation : 2/10 (0.5699)training: 3/10 (0.5699)validation : 3/10 (0.5679)training: 4/10 (0.5679)validation : 4/10 (0.5672)training: 5/10 (0.5672)validation : 5/10 (0.5672)training: 6/10 (0.5672)validation : 6/10 (0.5672)early stopping at 6 with loss 0.5672
AttentionModel-training is done: 6/10
2018-08-31 | reset count: 0 | final loss: 0.5672 at epoch 4
[strategy_integration.py] set_seed(2018-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5755)training: 2/10 (0.5755)validation : 2/10 (0.5707)training: 3/10 (0.5707)validation : 3/10 (0.5701)training: 4/10 (0.5701)validation : 4/10 (0.5683)training: 5/10 (0.5683)validation : 5/10 (0.5683)training: 6/10 (0.5683)validation : 6/10 (0.5683)early stopping at 6 with loss 0.5683
AttentionModel-training is done: 6/10
2018-09-30 | reset count: 0 | final loss: 0.5683 at epoch 4
making sample. (inference: True, date:2018-10-31 00:00:00, date_number: 382)
making sample. (inference: False, date:2018-09-30 00:00:00, date_number: 381)
[AttentionModel/2018-10-31 | get_logger | INFO]: ====================AttentionModel/2018-10-31 start====================
making sample. (inference: True, date:2018-11-30 00:00:00, date_number: 383)
making sample. (inference: False, date:2018-10-31 00:00:00, date_number: 382)
[AttentionModel/2018-11-30 | get_logger | INFO]: ====================AttentionModel/2018-11-30 start====================
[strategy_integration.py] set_seed(2018-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5768)training: 2/10 (0.5768)validation : 2/10 (0.5768)training: 3/10 (0.5768)validation : 3/10 (0.5768)training: 4/10 (0.5768)validation : 4/10 (0.5768)training: 5/10 (0.5768)validation : 5/10 (0.5768)early stopping at 5 with loss 0.5768
AttentionModel-training is done: 5/10
2018-10-31 | reset count: 0 | final loss: 0.5768 at epoch 1
making sample. (inference: True, date:2018-12-31 00:00:00, date_number: 384)
making sample. (inference: False, date:2018-11-30 00:00:00, date_number: 383)
[AttentionModel/2018-12-31 | get_logger | INFO]: ====================AttentionModel/2018-12-31 start====================
[strategy_integration.py] set_seed(2018-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5847)training: 2/10 (0.5847)validation : 2/10 (0.5847)training: 3/10 (0.5847)validation : 3/10 (0.5797)training: 4/10 (0.5797)validation : 4/10 (0.5797)training: 5/10 (0.5797)validation : 5/10 (0.5797)training: 6/10 (0.5797)validation : 6/10 (0.5791)training: 7/10 (0.5791)validation : 7/10 (0.5791)training: 8/10 (0.5791)validation : 8/10 (0.5791)early stopping at 8 with loss 0.5791
AttentionModel-training is done: 8/10
2018-11-30 | reset count: 0 | final loss: 0.5791 at epoch 6
making sample. (inference: True, date:2019-01-31 00:00:00, date_number: 385)
making sample. (inference: False, date:2018-12-31 00:00:00, date_number: 384)
[AttentionModel/2019-01-31 | get_logger | INFO]: ====================AttentionModel/2019-01-31 start====================
[strategy_integration.py] set_seed(2018-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5819)training: 2/10 (0.5819)validation : 2/10 (0.5797)training: 3/10 (0.5797)validation : 3/10 (0.5748)training: 4/10 (0.5748)validation : 4/10 (0.5748)training: 5/10 (0.5748)validation : 5/10 (0.5748)training: 6/10 (0.5748)validation : 6/10 (0.5741)training: 7/10 (0.5741)validation : 7/10 (0.5741)early stopping at 7 with loss 0.5741
AttentionModel-training is done: 7/10
2018-12-31 | reset count: 0 | final loss: 0.5741 at epoch 6
making sample. (inference: True, date:2019-02-28 00:00:00, date_number: 386)
making sample. (inference: False, date:2019-01-31 00:00:00, date_number: 385)
[AttentionModel/2019-02-28 | get_logger | INFO]: ====================AttentionModel/2019-02-28 start====================
[strategy_integration.py] set_seed(2019-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5873)training: 2/10 (0.5873)validation : 2/10 (0.5873)training: 3/10 (0.5873)validation : 3/10 (0.5869)training: 4/10 (0.5869)validation : 4/10 (0.5869)training: 5/10 (0.5869)validation : 5/10 (0.5869)training: 6/10 (0.5869)validation : 6/10 (0.5853)training: 7/10 (0.5853)validation : 7/10 (0.5853)early stopping at 7 with loss 0.5853
AttentionModel-training is done: 7/10
2019-01-31 | reset count: 0 | final loss: 0.5853 at epoch 6
[strategy_integration.py] set_seed(2019-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5848)training: 2/10 (0.5848)validation : 2/10 (0.5775)training: 3/10 (0.5775)validation : 3/10 (0.5720)training: 4/10 (0.5720)validation : 4/10 (0.5720)training: 5/10 (0.5720)validation : 5/10 (0.5705)training: 6/10 (0.5705)validation : 6/10 (0.5705)early stopping at 6 with loss 0.5705
AttentionModel-training is done: 6/10
2019-02-28 | reset count: 0 | final loss: 0.5705 at epoch 5
making sample. (inference: True, date:2019-03-31 00:00:00, date_number: 387)
making sample. (inference: False, date:2019-02-28 00:00:00, date_number: 386)
[AttentionModel/2019-03-31 | get_logger | INFO]: ====================AttentionModel/2019-03-31 start====================
making sample. (inference: True, date:2019-04-30 00:00:00, date_number: 388)
making sample. (inference: False, date:2019-03-31 00:00:00, date_number: 387)
[AttentionModel/2019-04-30 | get_logger | INFO]: ====================AttentionModel/2019-04-30 start====================
[strategy_integration.py] set_seed(2019-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5887)training: 2/10 (0.5887)validation : 2/10 (0.5848)training: 3/10 (0.5848)validation : 3/10 (0.5839)training: 4/10 (0.5839)validation : 4/10 (0.5839)training: 5/10 (0.5839)validation : 5/10 (0.5839)early stopping at 5 with loss 0.5839
AttentionModel-training is done: 5/10
2019-03-31 | reset count: 0 | final loss: 0.5839 at epoch 3
making sample. (inference: True, date:2019-05-31 00:00:00, date_number: 389)
making sample. (inference: False, date:2019-04-30 00:00:00, date_number: 388)
[AttentionModel/2019-05-31 | get_logger | INFO]: ====================AttentionModel/2019-05-31 start====================
[strategy_integration.py] set_seed(2019-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5864)training: 2/10 (0.5864)validation : 2/10 (0.5860)training: 3/10 (0.5860)validation : 3/10 (0.5857)training: 4/10 (0.5857)validation : 4/10 (0.5827)training: 5/10 (0.5827)validation : 5/10 (0.5827)training: 6/10 (0.5827)validation : 6/10 (0.5827)training: 7/10 (0.5827)validation : 7/10 (0.5827)early stopping at 7 with loss 0.5827
AttentionModel-training is done: 7/10
2019-04-30 | reset count: 0 | final loss: 0.5827 at epoch 4
[strategy_integration.py] set_seed(2019-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5711)training: 2/10 (0.5711)validation : 2/10 (0.5663)training: 3/10 (0.5663)validation : 3/10 (0.5663)training: 4/10 (0.5663)validation : 4/10 (0.5663)training: 5/10 (0.5663)validation : 5/10 (0.5663)early stopping at 5 with loss 0.5663
AttentionModel-training is done: 5/10
2019-05-31 | reset count: 0 | final loss: 0.5663 at epoch 2
making sample. (inference: True, date:2019-06-30 00:00:00, date_number: 390)
making sample. (inference: False, date:2019-05-31 00:00:00, date_number: 389)
[AttentionModel/2019-06-30 | get_logger | INFO]: ====================AttentionModel/2019-06-30 start====================
making sample. (inference: True, date:2019-07-31 00:00:00, date_number: 391)
making sample. (inference: False, date:2019-06-30 00:00:00, date_number: 390)
[AttentionModel/2019-07-31 | get_logger | INFO]: ====================AttentionModel/2019-07-31 start====================
[strategy_integration.py] set_seed(2019-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5811)training: 2/10 (0.5811)validation : 2/10 (0.5746)training: 3/10 (0.5746)validation : 3/10 (0.5746)training: 4/10 (0.5746)validation : 4/10 (0.5746)training: 5/10 (0.5746)validation : 5/10 (0.5746)early stopping at 5 with loss 0.5746
AttentionModel-training is done: 5/10
2019-06-30 | reset count: 0 | final loss: 0.5746 at epoch 2
making sample. (inference: True, date:2019-08-31 00:00:00, date_number: 392)
making sample. (inference: False, date:2019-07-31 00:00:00, date_number: 391)
[AttentionModel/2019-08-31 | get_logger | INFO]: ====================AttentionModel/2019-08-31 start====================
[strategy_integration.py] set_seed(2019-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5829)training: 2/10 (0.5829)validation : 2/10 (0.5788)training: 3/10 (0.5788)validation : 3/10 (0.5788)training: 4/10 (0.5788)validation : 4/10 (0.5788)training: 5/10 (0.5788)validation : 5/10 (0.5700)training: 6/10 (0.5700)validation : 6/10 (0.5700)training: 7/10 (0.5700)validation : 7/10 (0.5700)training: 8/10 (0.5700)validation : 8/10 (0.5700)early stopping at 8 with loss 0.5700
AttentionModel-training is done: 8/10
2019-07-31 | reset count: 0 | final loss: 0.5700 at epoch 5
making sample. (inference: True, date:2019-09-30 00:00:00, date_number: 393)
making sample. (inference: False, date:2019-08-31 00:00:00, date_number: 392)
[AttentionModel/2019-09-30 | get_logger | INFO]: ====================AttentionModel/2019-09-30 start====================
[strategy_integration.py] set_seed(2019-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5759)training: 2/10 (0.5759)validation : 2/10 (0.5756)training: 3/10 (0.5756)validation : 3/10 (0.5756)training: 4/10 (0.5756)validation : 4/10 (0.5756)training: 5/10 (0.5756)validation : 5/10 (0.5747)training: 6/10 (0.5747)validation : 6/10 (0.5747)early stopping at 6 with loss 0.5747
AttentionModel-training is done: 6/10
2019-08-31 | reset count: 0 | final loss: 0.5747 at epoch 5
making sample. (inference: True, date:2019-10-31 00:00:00, date_number: 394)
making sample. (inference: False, date:2019-09-30 00:00:00, date_number: 393)
[AttentionModel/2019-10-31 | get_logger | INFO]: ====================AttentionModel/2019-10-31 start====================
[strategy_integration.py] set_seed(2019-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5780)training: 2/10 (0.5780)validation : 2/10 (0.5774)training: 3/10 (0.5774)validation : 3/10 (0.5774)training: 4/10 (0.5774)validation : 4/10 (0.5774)training: 5/10 (0.5774)validation : 5/10 (0.5737)training: 6/10 (0.5737)validation : 6/10 (0.5737)training: 7/10 (0.5737)validation : 7/10 (0.5737)training: 8/10 (0.5737)validation : 8/10 (0.5737)early stopping at 8 with loss 0.5737
AttentionModel-training is done: 8/10
2019-09-30 | reset count: 0 | final loss: 0.5737 at epoch 5
[strategy_integration.py] set_seed(2019-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5784)training: 2/10 (0.5784)validation : 2/10 (0.5782)training: 3/10 (0.5782)validation : 3/10 (0.5782)training: 4/10 (0.5782)validation : 4/10 (0.5782)training: 5/10 (0.5782)validation : 5/10 (0.5782)early stopping at 5 with loss 0.5782
AttentionModel-training is done: 5/10
2019-10-31 | reset count: 0 | final loss: 0.5782 at epoch 2
making sample. (inference: True, date:2019-11-30 00:00:00, date_number: 395)
making sample. (inference: False, date:2019-10-31 00:00:00, date_number: 394)
[AttentionModel/2019-11-30 | get_logger | INFO]: ====================AttentionModel/2019-11-30 start====================
making sample. (inference: True, date:2019-12-31 00:00:00, date_number: 396)
making sample. (inference: False, date:2019-11-30 00:00:00, date_number: 395)
[AttentionModel/2019-12-31 | get_logger | INFO]: ====================AttentionModel/2019-12-31 start====================
[strategy_integration.py] set_seed(2019-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5867)training: 2/10 (0.5867)validation : 2/10 (0.5867)training: 3/10 (0.5867)validation : 3/10 (0.5867)training: 4/10 (0.5867)validation : 4/10 (0.5845)training: 5/10 (0.5845)validation : 5/10 (0.5845)training: 6/10 (0.5845)validation : 6/10 (0.5845)training: 7/10 (0.5845)validation : 7/10 (0.5845)early stopping at 7 with loss 0.5845
AttentionModel-training is done: 7/10
2019-11-30 | reset count: 0 | final loss: 0.5845 at epoch 4
[strategy_integration.py] set_seed(2019-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5819)training: 2/10 (0.5819)validation : 2/10 (0.5817)training: 3/10 (0.5817)validation : 3/10 (0.5817)training: 4/10 (0.5817)validation : 4/10 (0.5817)training: 5/10 (0.5817)validation : 5/10 (0.5817)early stopping at 5 with loss 0.5817
AttentionModel-training is done: 5/10
2019-12-31 | reset count: 0 | final loss: 0.5817 at epoch 2
making sample. (inference: True, date:2020-01-31 00:00:00, date_number: 397)
making sample. (inference: False, date:2019-12-31 00:00:00, date_number: 396)
[AttentionModel/2020-01-31 | get_logger | INFO]: ====================AttentionModel/2020-01-31 start====================
making sample. (inference: True, date:2020-02-29 00:00:00, date_number: 398)
making sample. (inference: False, date:2020-01-31 00:00:00, date_number: 397)
[AttentionModel/2020-02-29 | get_logger | INFO]: ====================AttentionModel/2020-02-29 start====================
[strategy_integration.py] set_seed(2020-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5897)training: 2/10 (0.5897)validation : 2/10 (0.5879)training: 3/10 (0.5879)validation : 3/10 (0.5838)training: 4/10 (0.5838)validation : 4/10 (0.5838)training: 5/10 (0.5838)validation : 5/10 (0.5831)training: 6/10 (0.5831)validation : 6/10 (0.5831)early stopping at 6 with loss 0.5831
AttentionModel-training is done: 6/10
2020-01-31 | reset count: 0 | final loss: 0.5831 at epoch 5
making sample. (inference: True, date:2020-03-31 00:00:00, date_number: 399)
making sample. (inference: False, date:2020-02-29 00:00:00, date_number: 398)
[AttentionModel/2020-03-31 | get_logger | INFO]: ====================AttentionModel/2020-03-31 start====================
[strategy_integration.py] set_seed(2020-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5831)training: 2/10 (0.5831)validation : 2/10 (0.5813)training: 3/10 (0.5813)validation : 3/10 (0.5792)training: 4/10 (0.5792)validation : 4/10 (0.5792)training: 5/10 (0.5792)validation : 5/10 (0.5770)training: 6/10 (0.5770)validation : 6/10 (0.5770)training: 7/10 (0.5770)validation : 7/10 (0.5770)early stopping at 7 with loss 0.5770
AttentionModel-training is done: 7/10
2020-02-29 | reset count: 0 | final loss: 0.5770 at epoch 5
making sample. (inference: True, date:2020-04-30 00:00:00, date_number: 400)
making sample. (inference: False, date:2020-03-31 00:00:00, date_number: 399)
[AttentionModel/2020-04-30 | get_logger | INFO]: ====================AttentionModel/2020-04-30 start====================
[strategy_integration.py] set_seed(2020-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5808)training: 2/10 (0.5808)validation : 2/10 (0.5801)training: 3/10 (0.5801)validation : 3/10 (0.5771)training: 4/10 (0.5771)validation : 4/10 (0.5771)training: 5/10 (0.5771)validation : 5/10 (0.5771)early stopping at 5 with loss 0.5771
AttentionModel-training is done: 5/10
2020-03-31 | reset count: 0 | final loss: 0.5771 at epoch 3
making sample. (inference: True, date:2020-05-31 00:00:00, date_number: 401)
making sample. (inference: False, date:2020-04-30 00:00:00, date_number: 400)
[AttentionModel/2020-05-31 | get_logger | INFO]: ====================AttentionModel/2020-05-31 start====================
[strategy_integration.py] set_seed(2020-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5750)training: 2/10 (0.5750)validation : 2/10 (0.5716)training: 3/10 (0.5716)validation : 3/10 (0.5716)training: 4/10 (0.5716)validation : 4/10 (0.5716)training: 5/10 (0.5716)validation : 5/10 (0.5716)early stopping at 5 with loss 0.5716
AttentionModel-training is done: 5/10
2020-04-30 | reset count: 0 | final loss: 0.5716 at epoch 2
making sample. (inference: True, date:2020-06-30 00:00:00, date_number: 402)
making sample. (inference: False, date:2020-05-31 00:00:00, date_number: 401)
[AttentionModel/2020-06-30 | get_logger | INFO]: ====================AttentionModel/2020-06-30 start====================
[strategy_integration.py] set_seed(2020-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5787)training: 2/10 (0.5787)validation : 2/10 (0.5783)training: 3/10 (0.5783)validation : 3/10 (0.5757)training: 4/10 (0.5757)validation : 4/10 (0.5757)training: 5/10 (0.5757)validation : 5/10 (0.5746)training: 6/10 (0.5746)validation : 6/10 (0.5746)early stopping at 6 with loss 0.5746
AttentionModel-training is done: 6/10
2020-05-31 | reset count: 0 | final loss: 0.5746 at epoch 5
making sample. (inference: True, date:2020-07-31 00:00:00, date_number: 403)
making sample. (inference: False, date:2020-06-30 00:00:00, date_number: 402)
[AttentionModel/2020-07-31 | get_logger | INFO]: ====================AttentionModel/2020-07-31 start====================
[strategy_integration.py] set_seed(2020-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5880)training: 2/10 (0.5880)validation : 2/10 (0.5880)training: 3/10 (0.5880)validation : 3/10 (0.5861)training: 4/10 (0.5861)validation : 4/10 (0.5861)training: 5/10 (0.5861)validation : 5/10 (0.5851)training: 6/10 (0.5851)validation : 6/10 (0.5851)early stopping at 6 with loss 0.5851
AttentionModel-training is done: 6/10
2020-06-30 | reset count: 0 | final loss: 0.5851 at epoch 5
making sample. (inference: True, date:2020-08-31 00:00:00, date_number: 404)
making sample. (inference: False, date:2020-07-31 00:00:00, date_number: 403)
[AttentionModel/2020-08-31 | get_logger | INFO]: ====================AttentionModel/2020-08-31 start====================
[strategy_integration.py] set_seed(2020-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5793)training: 2/10 (0.5793)validation : 2/10 (0.5781)training: 3/10 (0.5781)validation : 3/10 (0.5780)training: 4/10 (0.5780)validation : 4/10 (0.5780)training: 5/10 (0.5780)validation : 5/10 (0.5768)training: 6/10 (0.5768)validation : 6/10 (0.5731)training: 7/10 (0.5731)validation : 7/10 (0.5731)training: 8/10 (0.5731)validation : 8/10 (0.5731)training: 9/10 (0.5731)validation : 9/10 (0.5731)early stopping at 9 with loss 0.5731
AttentionModel-training is done: 9/10
2020-07-31 | reset count: 0 | final loss: 0.5731 at epoch 6
[strategy_integration.py] set_seed(2020-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5749)training: 2/10 (0.5749)validation : 2/10 (0.5702)training: 3/10 (0.5702)validation : 3/10 (0.5702)training: 4/10 (0.5702)validation : 4/10 (0.5687)training: 5/10 (0.5687)validation : 5/10 (0.5687)training: 6/10 (0.5687)validation : 6/10 (0.5683)training: 7/10 (0.5683)validation : 7/10 (0.5683)early stopping at 7 with loss 0.5683
AttentionModel-training is done: 7/10
2020-08-31 | reset count: 0 | final loss: 0.5683 at epoch 6
making sample. (inference: True, date:2020-09-30 00:00:00, date_number: 405)
making sample. (inference: False, date:2020-08-31 00:00:00, date_number: 404)
[AttentionModel/2020-09-30 | get_logger | INFO]: ====================AttentionModel/2020-09-30 start====================
making sample. (inference: True, date:2020-10-31 00:00:00, date_number: 406)
making sample. (inference: False, date:2020-09-30 00:00:00, date_number: 405)
[AttentionModel/2020-10-31 | get_logger | INFO]: ====================AttentionModel/2020-10-31 start====================
[strategy_integration.py] set_seed(2020-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5709)training: 2/10 (0.5709)validation : 2/10 (0.5644)training: 3/10 (0.5644)validation : 3/10 (0.5644)training: 4/10 (0.5644)validation : 4/10 (0.5644)training: 5/10 (0.5644)validation : 5/10 (0.5644)early stopping at 5 with loss 0.5644
AttentionModel-training is done: 5/10
2020-09-30 | reset count: 0 | final loss: 0.5644 at epoch 2
making sample. (inference: True, date:2020-11-30 00:00:00, date_number: 407)
making sample. (inference: False, date:2020-10-31 00:00:00, date_number: 406)
[AttentionModel/2020-11-30 | get_logger | INFO]: ====================AttentionModel/2020-11-30 start====================
[strategy_integration.py] set_seed(2020-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5755)training: 2/10 (0.5755)validation : 2/10 (0.5698)training: 3/10 (0.5698)validation : 3/10 (0.5698)training: 4/10 (0.5698)validation : 4/10 (0.5698)training: 5/10 (0.5698)validation : 5/10 (0.5698)training: 6/10 (0.5698)validation : 6/10 (0.5688)training: 7/10 (0.5688)validation : 7/10 (0.5688)early stopping at 7 with loss 0.5688
AttentionModel-training is done: 7/10
2020-10-31 | reset count: 0 | final loss: 0.5688 at epoch 6
making sample. (inference: True, date:2020-12-31 00:00:00, date_number: 408)
making sample. (inference: False, date:2020-11-30 00:00:00, date_number: 407)
[AttentionModel/2020-12-31 | get_logger | INFO]: ====================AttentionModel/2020-12-31 start====================
[strategy_integration.py] set_seed(2020-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5661)training: 2/10 (0.5661)validation : 2/10 (0.5661)training: 3/10 (0.5661)validation : 3/10 (0.5641)training: 4/10 (0.5641)validation : 4/10 (0.5641)training: 5/10 (0.5641)validation : 5/10 (0.5641)training: 6/10 (0.5641)validation : 6/10 (0.5641)early stopping at 6 with loss 0.5641
AttentionModel-training is done: 6/10
2020-11-30 | reset count: 0 | final loss: 0.5641 at epoch 3
making sample. (inference: True, date:2021-01-31 00:00:00, date_number: 409)
making sample. (inference: False, date:2020-12-31 00:00:00, date_number: 408)
[AttentionModel/2021-01-31 | get_logger | INFO]: ====================AttentionModel/2021-01-31 start====================
[strategy_integration.py] set_seed(2020-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5755)training: 2/10 (0.5755)validation : 2/10 (0.5755)training: 3/10 (0.5755)validation : 3/10 (0.5725)training: 4/10 (0.5725)validation : 4/10 (0.5725)training: 5/10 (0.5725)validation : 5/10 (0.5723)training: 6/10 (0.5723)validation : 6/10 (0.5675)training: 7/10 (0.5675)validation : 7/10 (0.5675)training: 8/10 (0.5675)validation : 8/10 (0.5675)training: 9/10 (0.5675)validation : 9/10 (0.5675)early stopping at 9 with loss 0.5675
AttentionModel-training is done: 9/10
2020-12-31 | reset count: 0 | final loss: 0.5675 at epoch 6
[strategy_integration.py] set_seed(2021-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5623)training: 2/10 (0.5623)validation : 2/10 (0.5623)training: 3/10 (0.5623)validation : 3/10 (0.5623)training: 4/10 (0.5623)validation : 4/10 (0.5622)training: 5/10 (0.5622)validation : 5/10 (0.5622)early stopping at 5 with loss 0.5622
AttentionModel-training is done: 5/10
2021-01-31 | reset count: 0 | final loss: 0.5622 at epoch 4
making sample. (inference: True, date:2021-02-28 00:00:00, date_number: 410)
making sample. (inference: False, date:2021-01-31 00:00:00, date_number: 409)
[AttentionModel/2021-02-28 | get_logger | INFO]: ====================AttentionModel/2021-02-28 start====================
making sample. (inference: True, date:2021-03-31 00:00:00, date_number: 411)
making sample. (inference: False, date:2021-02-28 00:00:00, date_number: 410)
[AttentionModel/2021-03-31 | get_logger | INFO]: ====================AttentionModel/2021-03-31 start====================
[strategy_integration.py] set_seed(2021-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5625)training: 2/10 (0.5625)validation : 2/10 (0.5625)training: 3/10 (0.5625)validation : 3/10 (0.5572)training: 4/10 (0.5572)validation : 4/10 (0.5572)training: 5/10 (0.5572)validation : 5/10 (0.5572)training: 6/10 (0.5572)validation : 6/10 (0.5572)early stopping at 6 with loss 0.5572
AttentionModel-training is done: 6/10
2021-02-28 | reset count: 0 | final loss: 0.5572 at epoch 3
making sample. (inference: True, date:2021-04-30 00:00:00, date_number: 412)
making sample. (inference: False, date:2021-03-31 00:00:00, date_number: 411)
[AttentionModel/2021-04-30 | get_logger | INFO]: ====================AttentionModel/2021-04-30 start====================
[strategy_integration.py] set_seed(2021-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5603)training: 2/10 (0.5603)validation : 2/10 (0.5603)training: 3/10 (0.5603)validation : 3/10 (0.5583)training: 4/10 (0.5583)validation : 4/10 (0.5583)training: 5/10 (0.5583)validation : 5/10 (0.5562)training: 6/10 (0.5562)validation : 6/10 (0.5562)training: 7/10 (0.5562)validation : 7/10 (0.5562)early stopping at 7 with loss 0.5562
AttentionModel-training is done: 7/10
2021-03-31 | reset count: 0 | final loss: 0.5562 at epoch 5
making sample. (inference: True, date:2021-05-31 00:00:00, date_number: 413)
making sample. (inference: False, date:2021-04-30 00:00:00, date_number: 412)
[AttentionModel/2021-05-31 | get_logger | INFO]: ====================AttentionModel/2021-05-31 start====================
[strategy_integration.py] set_seed(2021-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5565)training: 2/10 (0.5565)validation : 2/10 (0.5512)training: 3/10 (0.5512)validation : 3/10 (0.5512)training: 4/10 (0.5512)validation : 4/10 (0.5510)training: 5/10 (0.5510)validation : 5/10 (0.5510)training: 6/10 (0.5510)validation : 6/10 (0.5510)training: 7/10 (0.5510)validation : 7/10 (0.5510)early stopping at 7 with loss 0.5510
AttentionModel-training is done: 7/10
2021-04-30 | reset count: 0 | final loss: 0.5510 at epoch 4
making sample. (inference: True, date:2021-06-30 00:00:00, date_number: 414)
making sample. (inference: False, date:2021-05-31 00:00:00, date_number: 413)
[AttentionModel/2021-06-30 | get_logger | INFO]: ====================AttentionModel/2021-06-30 start====================
[strategy_integration.py] set_seed(2021-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5710)training: 2/10 (0.5710)validation : 2/10 (0.5683)training: 3/10 (0.5683)validation : 3/10 (0.5683)training: 4/10 (0.5683)validation : 4/10 (0.5678)training: 5/10 (0.5678)validation : 5/10 (0.5678)early stopping at 5 with loss 0.5678
AttentionModel-training is done: 5/10
2021-05-31 | reset count: 0 | final loss: 0.5678 at epoch 4
making sample. (inference: True, date:2021-07-31 00:00:00, date_number: 415)
making sample. (inference: False, date:2021-06-30 00:00:00, date_number: 414)
[AttentionModel/2021-07-31 | get_logger | INFO]: ====================AttentionModel/2021-07-31 start====================
[strategy_integration.py] set_seed(2021-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5693)training: 2/10 (0.5693)validation : 2/10 (0.5658)training: 3/10 (0.5658)validation : 3/10 (0.5658)training: 4/10 (0.5658)validation : 4/10 (0.5658)training: 5/10 (0.5658)validation : 5/10 (0.5637)training: 6/10 (0.5637)validation : 6/10 (0.5637)training: 7/10 (0.5637)validation : 7/10 (0.5637)training: 8/10 (0.5637)validation : 8/10 (0.5637)early stopping at 8 with loss 0.5637
AttentionModel-training is done: 8/10
2021-06-30 | reset count: 0 | final loss: 0.5637 at epoch 5
making sample. (inference: True, date:2021-08-31 00:00:00, date_number: 416)
making sample. (inference: False, date:2021-07-31 00:00:00, date_number: 415)
[AttentionModel/2021-08-31 | get_logger | INFO]: ====================AttentionModel/2021-08-31 start====================
[strategy_integration.py] set_seed(2021-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5688)training: 2/10 (0.5688)validation : 2/10 (0.5665)training: 3/10 (0.5665)validation : 3/10 (0.5619)training: 4/10 (0.5619)validation : 4/10 (0.5606)training: 5/10 (0.5606)validation : 5/10 (0.5606)training: 6/10 (0.5606)validation : 6/10 (0.5595)training: 7/10 (0.5595)validation : 7/10 (0.5595)early stopping at 7 with loss 0.5595
AttentionModel-training is done: 7/10
2021-07-31 | reset count: 0 | final loss: 0.5595 at epoch 6
making sample. (inference: True, date:2021-09-30 00:00:00, date_number: 417)
making sample. (inference: False, date:2021-08-31 00:00:00, date_number: 416)
[AttentionModel/2021-09-30 | get_logger | INFO]: ====================AttentionModel/2021-09-30 start====================
[strategy_integration.py] set_seed(2021-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5771)training: 2/10 (0.5771)validation : 2/10 (0.5771)training: 3/10 (0.5771)validation : 3/10 (0.5751)training: 4/10 (0.5751)validation : 4/10 (0.5730)training: 5/10 (0.5730)validation : 5/10 (0.5730)training: 6/10 (0.5730)validation : 6/10 (0.5730)training: 7/10 (0.5730)validation : 7/10 (0.5730)early stopping at 7 with loss 0.5730
AttentionModel-training is done: 7/10
2021-08-31 | reset count: 0 | final loss: 0.5730 at epoch 4
[strategy_integration.py] set_seed(2021-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5751)training: 2/10 (0.5751)validation : 2/10 (0.5731)training: 3/10 (0.5731)validation : 3/10 (0.5731)training: 4/10 (0.5731)validation : 4/10 (0.5731)training: 5/10 (0.5731)validation : 5/10 (0.5731)early stopping at 5 with loss 0.5731
AttentionModel-training is done: 5/10
2021-09-30 | reset count: 0 | final loss: 0.5731 at epoch 2
making sample. (inference: True, date:2021-10-31 00:00:00, date_number: 418)
making sample. (inference: False, date:2021-09-30 00:00:00, date_number: 417)
[AttentionModel/2021-10-31 | get_logger | INFO]: ====================AttentionModel/2021-10-31 start====================
[strategy_integration.py] set_seed(2021-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5842)training: 2/10 (0.5842)validation : 2/10 (0.5788)training: 3/10 (0.5788)validation : 3/10 (0.5788)training: 4/10 (0.5788)validation : 4/10 (0.5788)training: 5/10 (0.5788)validation : 5/10 (0.5788)early stopping at 5 with loss 0.5788
AttentionModel-training is done: 5/10
2021-10-31 | reset count: 0 | final loss: 0.5788 at epoch 2
[strategy | get_logger | INFO]: ====================strategy start====================
[strategy | _get_infer_dirs | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick350_test01/infer 이 strategy 대상에 추가됨
[strategy | _backtest | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick350_test01/infer is starting
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.148 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠹ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache✓ [Compustat API] : Loading get_historic_universe from cache done in 1.921 secs
⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.222 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.062 secs
⠋ [Compustat API] : Loading get_monthly_total_return from cache✓ [Compustat API] : Loading get_monthly_total_return from cache done in 0.062 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.077 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.071 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.07 secs
⠋ [Compustat API] : Loading get_monthly_volume_data from cache✓ [Compustat API] : Loading get_monthly_volume_data from cache done in 0.08 secs
[strategy | _allocating | INFO]: [STRATEGY] allocating start
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_weight_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] performance.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] combined.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] weight.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] universe.csv is saved.
[comparison | get_logger | INFO]: ====================comparison start====================
[comparison | compare | INFO]: [5. COMPARISON] comparison is saved.
[comparison | compare | INFO]:                        Harvest_Green_concept001_pick350_test01_price_return  ...  SPCOMP_total_win
2016-01-31_2021-04-30                                           0.225002     ...          0.765625

[1 rows x 27 columns]
2015-12-31
2016-01-31
2016-02-29
2016-03-31
2016-04-30
2016-05-31
2016-06-30
2016-07-31
2016-08-31
2016-09-30
2016-10-31
2016-11-30
2016-12-31
2017-01-31
2017-02-28
2017-03-31
2017-04-30
2017-05-31
2017-06-30
2017-07-31
2017-08-31
2017-09-30
2017-10-31
2017-11-30
2017-12-31
2018-01-31
2018-02-28
2018-03-31
2018-04-30
2018-05-31
2018-06-30
2018-07-31
2018-08-31
2018-09-30
2018-10-31
2018-11-30
2018-12-31
2019-01-31
2019-02-28
2019-03-31
2019-04-30
2019-05-31
2019-06-30
2019-07-31
2019-08-31
2019-09-30
2019-10-31
2019-11-30
2019-12-31
2020-01-31
2020-02-29
2020-03-31
2020-04-30
2020-05-31
2020-06-30
2020-07-31
2020-08-31
2020-09-30
2020-10-31
2020-11-30
2020-12-31
2021-01-31
2021-02-28
2021-03-31
2021-04-30
2021-05-31
2021-06-30
2021-07-31
2021-08-31
2021-09-30
2021-10-31
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.047 secs
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.044 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.063 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.078 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.072 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.673 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.681 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.68 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.683 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.684 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.078 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.671 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.667 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.667 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.663 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.661 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.077 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.682 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.076 secs
⠋ [Compustat API] : Cleaning past days cache✓ [Compustat API] : Cleaning past days cache done in 0.0 secs
auto caching이 활성화 되었습니다. /home/sronly/sr-storage/kirin_cache/kirin_api_cache/2.19.17_2021-11-23에 캐쉬를 저장합니다.
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.148 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading masking data✓ [Compustat API] : Loading get_historic_universe from cache done in 2.009 secs
⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.31 secs
setting tensorflow random seed failed
load_data: mom_12m_1m
load_data: ram_12m_0m
load_data: vol_3m
load_data: snp500_pr
load_data: wilshire500_pr
load_data: ted
load_data: retail_mfr
load_data: m1
load_data: m2
load_data: ae_m
load_data: ia_ta
load_data: rc_a
load_data: t1y_ff
load_data: t5y_ff
load_data: t10y_t2y
load_data: export_growth
load_data: import_growth
load_data: real_gig
load_data: res_mom_12m_1m_0m
load_data: res_vol_6m_3m_0m
load_data: at
load_data: pr_1m_0m
load_data: mv
load_data: btm
load_data: t3y
load_data: t5y
load_data: t7y
load_data: baa_ff
load_data: core_cpi
load_data: core_pce
load_data: corporate_tg
load_data: industrial_prod
load_data: home_pr
load_data: r_s
load_data: r_a
load_data: fred_ff
load_data: t3m
load_data: t6m
load_data: t2y
load_data: t10y
load_data: aaa
load_data: baa
load_data: core_ppi
load_data: cpi
load_data: pce
load_data: wti
load_data: capa_util
load_data: snp500_pe
load_data: op_lev
load_data: roe
load_data: std_u_e
load_data: ppi
load_data: trimmed_pce
load_data: unemploy
load_data: snp500_vol
load_data: gpa
load_data: rev_surp
load_data: cash_at
load_data: aaa_t10y
load_data: baa_t10y
load_data: aaa_ff
load_data: real_pig
load_data: federal_tg
load_data: real_gdp
load_data: ret_noa
load_data: etm
load_data: ia_mv
load_data: sector_values
load_data: mv
load_data: pr_1m_0m
load_data: mv
making sample. (inference: True, date:2015-12-31 00:00:00, date_number: 348)
making sample. (inference: False, date:2012-12-31 00:00:00, date_number: 312)
making sample. (inference: False, date:2013-01-31 00:00:00, date_number: 313)
making sample. (inference: False, date:2013-02-28 00:00:00, date_number: 314)
making sample. (inference: False, date:2013-03-31 00:00:00, date_number: 315)
making sample. (inference: False, date:2013-04-30 00:00:00, date_number: 316)
making sample. (inference: False, date:2013-05-31 00:00:00, date_number: 317)
making sample. (inference: False, date:2013-06-30 00:00:00, date_number: 318)
making sample. (inference: False, date:2013-07-31 00:00:00, date_number: 319)
making sample. (inference: False, date:2013-08-31 00:00:00, date_number: 320)
making sample. (inference: False, date:2013-09-30 00:00:00, date_number: 321)
making sample. (inference: False, date:2013-10-31 00:00:00, date_number: 322)
making sample. (inference: False, date:2013-11-30 00:00:00, date_number: 323)
making sample. (inference: False, date:2013-12-31 00:00:00, date_number: 324)
making sample. (inference: False, date:2014-01-31 00:00:00, date_number: 325)
making sample. (inference: False, date:2014-02-28 00:00:00, date_number: 326)
making sample. (inference: False, date:2014-03-31 00:00:00, date_number: 327)
making sample. (inference: False, date:2014-04-30 00:00:00, date_number: 328)
making sample. (inference: False, date:2014-05-31 00:00:00, date_number: 329)
making sample. (inference: False, date:2014-06-30 00:00:00, date_number: 330)
making sample. (inference: False, date:2014-07-31 00:00:00, date_number: 331)
making sample. (inference: False, date:2014-08-31 00:00:00, date_number: 332)
making sample. (inference: False, date:2014-09-30 00:00:00, date_number: 333)
making sample. (inference: False, date:2014-10-31 00:00:00, date_number: 334)
making sample. (inference: False, date:2014-11-30 00:00:00, date_number: 335)
making sample. (inference: False, date:2014-12-31 00:00:00, date_number: 336)
making sample. (inference: False, date:2015-01-31 00:00:00, date_number: 337)
making sample. (inference: False, date:2015-02-28 00:00:00, date_number: 338)
making sample. (inference: False, date:2015-03-31 00:00:00, date_number: 339)
making sample. (inference: False, date:2015-04-30 00:00:00, date_number: 340)
making sample. (inference: False, date:2015-05-31 00:00:00, date_number: 341)
making sample. (inference: False, date:2015-06-30 00:00:00, date_number: 342)
making sample. (inference: False, date:2015-07-31 00:00:00, date_number: 343)
making sample. (inference: False, date:2015-08-31 00:00:00, date_number: 344)
making sample. (inference: False, date:2015-09-30 00:00:00, date_number: 345)
making sample. (inference: False, date:2015-10-31 00:00:00, date_number: 346)
making sample. (inference: False, date:2015-11-30 00:00:00, date_number: 347)
[AttentionModel/2015-12-31 | get_logger | INFO]: ====================AttentionModel/2015-12-31 start====================
making sample. (inference: True, date:2016-01-31 00:00:00, date_number: 349)
making sample. (inference: False, date:2015-12-31 00:00:00, date_number: 348)
[AttentionModel/2016-01-31 | get_logger | INFO]: ====================AttentionModel/2016-01-31 start====================
[strategy_integration.py] set_seed(2015-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5734)training: 2/10 (0.5734)validation : 2/10 (0.5719)training: 3/10 (0.5719)validation : 3/10 (0.5719)training: 4/10 (0.5719)validation : 4/10 (0.5692)training: 5/10 (0.5692)validation : 5/10 (0.5692)training: 6/10 (0.5692)validation : 6/10 (0.5692)early stopping at 6 with loss 0.5692
AttentionModel-training is done: 6/10
2015-12-31 | reset count: 0 | final loss: 0.5692 at epoch 4
[strategy_integration.py] set_seed(2016-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5823)training: 2/10 (0.5823)validation : 2/10 (0.5788)training: 3/10 (0.5788)validation : 3/10 (0.5788)training: 4/10 (0.5788)validation : 4/10 (0.5788)training: 5/10 (0.5788)validation : 5/10 (0.5788)early stopping at 5 with loss 0.5788
AttentionModel-training is done: 5/10
2016-01-31 | reset count: 0 | final loss: 0.5788 at epoch 2
making sample. (inference: True, date:2016-02-29 00:00:00, date_number: 350)
making sample. (inference: False, date:2016-01-31 00:00:00, date_number: 349)
[AttentionModel/2016-02-29 | get_logger | INFO]: ====================AttentionModel/2016-02-29 start====================
making sample. (inference: True, date:2016-03-31 00:00:00, date_number: 351)
making sample. (inference: False, date:2016-02-29 00:00:00, date_number: 350)
[AttentionModel/2016-03-31 | get_logger | INFO]: ====================AttentionModel/2016-03-31 start====================
[strategy_integration.py] set_seed(2016-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5772)training: 2/10 (0.5772)validation : 2/10 (0.5767)training: 3/10 (0.5767)validation : 3/10 (0.5751)training: 4/10 (0.5751)validation : 4/10 (0.5727)training: 5/10 (0.5727)validation : 5/10 (0.5727)training: 6/10 (0.5727)validation : 6/10 (0.5715)training: 7/10 (0.5715)validation : 7/10 (0.5697)training: 8/10 (0.5697)validation : 8/10 (0.5697)early stopping at 8 with loss 0.5697
AttentionModel-training is done: 8/10
2016-02-29 | reset count: 0 | final loss: 0.5697 at epoch 7
[strategy_integration.py] set_seed(2016-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5655)training: 2/10 (0.5655)validation : 2/10 (0.5597)training: 3/10 (0.5597)validation : 3/10 (0.5597)training: 4/10 (0.5597)validation : 4/10 (0.5597)training: 5/10 (0.5597)validation : 5/10 (0.5597)early stopping at 5 with loss 0.5597
AttentionModel-training is done: 5/10
2016-03-31 | reset count: 0 | final loss: 0.5597 at epoch 2
making sample. (inference: True, date:2016-04-30 00:00:00, date_number: 352)
making sample. (inference: False, date:2016-03-31 00:00:00, date_number: 351)
[AttentionModel/2016-04-30 | get_logger | INFO]: ====================AttentionModel/2016-04-30 start====================
making sample. (inference: True, date:2016-05-31 00:00:00, date_number: 353)
making sample. (inference: False, date:2016-04-30 00:00:00, date_number: 352)
[AttentionModel/2016-05-31 | get_logger | INFO]: ====================AttentionModel/2016-05-31 start====================
[strategy_integration.py] set_seed(2016-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5751)training: 2/10 (0.5751)validation : 2/10 (0.5751)training: 3/10 (0.5751)validation : 3/10 (0.5751)training: 4/10 (0.5751)validation : 4/10 (0.5751)training: 5/10 (0.5751)validation : 5/10 (0.5751)early stopping at 5 with loss 0.5751
AttentionModel-training is done: 5/10
2016-04-30 | reset count: 0 | final loss: 0.5751 at epoch 1
making sample. (inference: True, date:2016-06-30 00:00:00, date_number: 354)
making sample. (inference: False, date:2016-05-31 00:00:00, date_number: 353)
[AttentionModel/2016-06-30 | get_logger | INFO]: ====================AttentionModel/2016-06-30 start====================
[strategy_integration.py] set_seed(2016-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5874)training: 2/10 (0.5874)validation : 2/10 (0.5862)training: 3/10 (0.5862)validation : 3/10 (0.5862)training: 4/10 (0.5862)validation : 4/10 (0.5862)training: 5/10 (0.5862)validation : 5/10 (0.5821)training: 6/10 (0.5821)validation : 6/10 (0.5821)training: 7/10 (0.5821)validation : 7/10 (0.5821)training: 8/10 (0.5821)validation : 8/10 (0.5821)early stopping at 8 with loss 0.5821
AttentionModel-training is done: 8/10
2016-05-31 | reset count: 0 | final loss: 0.5821 at epoch 5
[strategy_integration.py] set_seed(2016-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5767)training: 2/10 (0.5767)validation : 2/10 (0.5759)training: 3/10 (0.5759)validation : 3/10 (0.5759)training: 4/10 (0.5759)validation : 4/10 (0.5759)training: 5/10 (0.5759)validation : 5/10 (0.5759)early stopping at 5 with loss 0.5759
AttentionModel-training is done: 5/10
2016-06-30 | reset count: 0 | final loss: 0.5759 at epoch 2
making sample. (inference: True, date:2016-07-31 00:00:00, date_number: 355)
making sample. (inference: False, date:2016-06-30 00:00:00, date_number: 354)
[AttentionModel/2016-07-31 | get_logger | INFO]: ====================AttentionModel/2016-07-31 start====================
making sample. (inference: True, date:2016-08-31 00:00:00, date_number: 356)
making sample. (inference: False, date:2016-07-31 00:00:00, date_number: 355)
[AttentionModel/2016-08-31 | get_logger | INFO]: ====================AttentionModel/2016-08-31 start====================
[strategy_integration.py] set_seed(2016-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5835)training: 2/10 (0.5835)validation : 2/10 (0.5835)training: 3/10 (0.5835)validation : 3/10 (0.5827)training: 4/10 (0.5827)validation : 4/10 (0.5827)training: 5/10 (0.5827)validation : 5/10 (0.5827)early stopping at 5 with loss 0.5827
AttentionModel-training is done: 5/10
2016-07-31 | reset count: 0 | final loss: 0.5827 at epoch 3
making sample. (inference: True, date:2016-09-30 00:00:00, date_number: 357)
making sample. (inference: False, date:2016-08-31 00:00:00, date_number: 356)
[AttentionModel/2016-09-30 | get_logger | INFO]: ====================AttentionModel/2016-09-30 start====================
[strategy_integration.py] set_seed(2016-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5799)training: 2/10 (0.5799)validation : 2/10 (0.5790)training: 3/10 (0.5790)validation : 3/10 (0.5758)training: 4/10 (0.5758)validation : 4/10 (0.5758)training: 5/10 (0.5758)validation : 5/10 (0.5758)training: 6/10 (0.5758)validation : 6/10 (0.5758)early stopping at 6 with loss 0.5758
AttentionModel-training is done: 6/10
2016-08-31 | reset count: 0 | final loss: 0.5758 at epoch 3
making sample. (inference: True, date:2016-10-31 00:00:00, date_number: 358)
making sample. (inference: False, date:2016-09-30 00:00:00, date_number: 357)
[AttentionModel/2016-10-31 | get_logger | INFO]: ====================AttentionModel/2016-10-31 start====================
[strategy_integration.py] set_seed(2016-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5621)training: 2/10 (0.5621)validation : 2/10 (0.5552)training: 3/10 (0.5552)validation : 3/10 (0.5552)training: 4/10 (0.5552)validation : 4/10 (0.5552)training: 5/10 (0.5552)validation : 5/10 (0.5552)training: 6/10 (0.5552)validation : 6/10 (0.5552)training: 7/10 (0.5552)validation : 7/10 (0.5552)early stopping at 7 with loss 0.5552
AttentionModel-training is done: 7/10
2016-09-30 | reset count: 0 | final loss: 0.5552 at epoch 2
making sample. (inference: True, date:2016-11-30 00:00:00, date_number: 359)
making sample. (inference: False, date:2016-10-31 00:00:00, date_number: 358)
[AttentionModel/2016-11-30 | get_logger | INFO]: ====================AttentionModel/2016-11-30 start====================
[strategy_integration.py] set_seed(2016-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5810)training: 2/10 (0.5810)validation : 2/10 (0.5810)training: 3/10 (0.5810)validation : 3/10 (0.5782)training: 4/10 (0.5782)validation : 4/10 (0.5782)training: 5/10 (0.5782)validation : 5/10 (0.5782)training: 6/10 (0.5782)validation : 6/10 (0.5782)early stopping at 6 with loss 0.5782
AttentionModel-training is done: 6/10
2016-10-31 | reset count: 0 | final loss: 0.5782 at epoch 3
making sample. (inference: True, date:2016-12-31 00:00:00, date_number: 360)
making sample. (inference: False, date:2016-11-30 00:00:00, date_number: 359)
[AttentionModel/2016-12-31 | get_logger | INFO]: ====================AttentionModel/2016-12-31 start====================
[strategy_integration.py] set_seed(2016-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5651)training: 2/10 (0.5651)validation : 2/10 (0.5651)training: 3/10 (0.5651)validation : 3/10 (0.5651)training: 4/10 (0.5651)validation : 4/10 (0.5651)training: 5/10 (0.5651)validation : 5/10 (0.5642)training: 6/10 (0.5642)validation : 6/10 (0.5628)training: 7/10 (0.5628)validation : 7/10 (0.5628)early stopping at 7 with loss 0.5628
AttentionModel-training is done: 7/10
2016-11-30 | reset count: 0 | final loss: 0.5628 at epoch 6
[strategy_integration.py] set_seed(2016-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5609)training: 2/10 (0.5609)validation : 2/10 (0.5563)training: 3/10 (0.5563)validation : 3/10 (0.5563)training: 4/10 (0.5563)validation : 4/10 (0.5563)training: 5/10 (0.5563)validation : 5/10 (0.5563)early stopping at 5 with loss 0.5563
AttentionModel-training is done: 5/10
2016-12-31 | reset count: 0 | final loss: 0.5563 at epoch 2
making sample. (inference: True, date:2017-01-31 00:00:00, date_number: 361)
making sample. (inference: False, date:2016-12-31 00:00:00, date_number: 360)
[AttentionModel/2017-01-31 | get_logger | INFO]: ====================AttentionModel/2017-01-31 start====================
making sample. (inference: True, date:2017-02-28 00:00:00, date_number: 362)
making sample. (inference: False, date:2017-01-31 00:00:00, date_number: 361)
[AttentionModel/2017-02-28 | get_logger | INFO]: ====================AttentionModel/2017-02-28 start====================
[strategy_integration.py] set_seed(2017-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5765)training: 2/10 (0.5765)validation : 2/10 (0.5765)training: 3/10 (0.5765)validation : 3/10 (0.5758)training: 4/10 (0.5758)validation : 4/10 (0.5700)training: 5/10 (0.5700)validation : 5/10 (0.5700)training: 6/10 (0.5700)validation : 6/10 (0.5700)early stopping at 6 with loss 0.5700
AttentionModel-training is done: 6/10
2017-01-31 | reset count: 0 | final loss: 0.5700 at epoch 4
[strategy_integration.py] set_seed(2017-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5692)training: 2/10 (0.5692)validation : 2/10 (0.5692)training: 3/10 (0.5692)validation : 3/10 (0.5672)training: 4/10 (0.5672)validation : 4/10 (0.5672)training: 5/10 (0.5672)validation : 5/10 (0.5672)early stopping at 5 with loss 0.5672
AttentionModel-training is done: 5/10
2017-02-28 | reset count: 0 | final loss: 0.5672 at epoch 3
making sample. (inference: True, date:2017-03-31 00:00:00, date_number: 363)
making sample. (inference: False, date:2017-02-28 00:00:00, date_number: 362)
[AttentionModel/2017-03-31 | get_logger | INFO]: ====================AttentionModel/2017-03-31 start====================
making sample. (inference: True, date:2017-04-30 00:00:00, date_number: 364)
making sample. (inference: False, date:2017-03-31 00:00:00, date_number: 363)
[AttentionModel/2017-04-30 | get_logger | INFO]: ====================AttentionModel/2017-04-30 start====================
[strategy_integration.py] set_seed(2017-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5670)training: 2/10 (0.5670)validation : 2/10 (0.5650)training: 3/10 (0.5650)validation : 3/10 (0.5632)training: 4/10 (0.5632)validation : 4/10 (0.5632)training: 5/10 (0.5632)validation : 5/10 (0.5632)training: 6/10 (0.5632)validation : 6/10 (0.5630)training: 7/10 (0.5630)validation : 7/10 (0.5630)training: 8/10 (0.5630)validation : 8/10 (0.5630)early stopping at 8 with loss 0.5630
AttentionModel-training is done: 8/10
2017-03-31 | reset count: 0 | final loss: 0.5630 at epoch 6
[strategy_integration.py] set_seed(2017-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5760)training: 2/10 (0.5760)validation : 2/10 (0.5712)training: 3/10 (0.5712)validation : 3/10 (0.5712)training: 4/10 (0.5712)validation : 4/10 (0.5712)training: 5/10 (0.5712)validation : 5/10 (0.5712)early stopping at 5 with loss 0.5712
AttentionModel-training is done: 5/10
2017-04-30 | reset count: 0 | final loss: 0.5712 at epoch 2
making sample. (inference: True, date:2017-05-31 00:00:00, date_number: 365)
making sample. (inference: False, date:2017-04-30 00:00:00, date_number: 364)
[AttentionModel/2017-05-31 | get_logger | INFO]: ====================AttentionModel/2017-05-31 start====================
making sample. (inference: True, date:2017-06-30 00:00:00, date_number: 366)
making sample. (inference: False, date:2017-05-31 00:00:00, date_number: 365)
[AttentionModel/2017-06-30 | get_logger | INFO]: ====================AttentionModel/2017-06-30 start====================
[strategy_integration.py] set_seed(2017-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5639)training: 2/10 (0.5639)validation : 2/10 (0.5607)training: 3/10 (0.5607)validation : 3/10 (0.5607)training: 4/10 (0.5607)validation : 4/10 (0.5603)training: 5/10 (0.5603)validation : 5/10 (0.5603)training: 6/10 (0.5603)validation : 6/10 (0.5575)training: 7/10 (0.5575)validation : 7/10 (0.5575)early stopping at 7 with loss 0.5575
AttentionModel-training is done: 7/10
2017-05-31 | reset count: 0 | final loss: 0.5575 at epoch 6
[strategy_integration.py] set_seed(2017-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5569)training: 2/10 (0.5569)validation : 2/10 (0.5569)training: 3/10 (0.5569)validation : 3/10 (0.5569)training: 4/10 (0.5569)validation : 4/10 (0.5555)training: 5/10 (0.5555)validation : 5/10 (0.5543)training: 6/10 (0.5543)validation : 6/10 (0.5543)training: 7/10 (0.5543)validation : 7/10 (0.5543)training: 8/10 (0.5543)validation : 8/10 (0.5543)early stopping at 8 with loss 0.5543
AttentionModel-training is done: 8/10
2017-06-30 | reset count: 0 | final loss: 0.5543 at epoch 5
making sample. (inference: True, date:2017-07-31 00:00:00, date_number: 367)
making sample. (inference: False, date:2017-06-30 00:00:00, date_number: 366)
[AttentionModel/2017-07-31 | get_logger | INFO]: ====================AttentionModel/2017-07-31 start====================
making sample. (inference: True, date:2017-08-31 00:00:00, date_number: 368)
making sample. (inference: False, date:2017-07-31 00:00:00, date_number: 367)
[AttentionModel/2017-08-31 | get_logger | INFO]: ====================AttentionModel/2017-08-31 start====================
[strategy_integration.py] set_seed(2017-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5816)training: 2/10 (0.5816)validation : 2/10 (0.5780)training: 3/10 (0.5780)validation : 3/10 (0.5771)training: 4/10 (0.5771)validation : 4/10 (0.5771)training: 5/10 (0.5771)validation : 5/10 (0.5771)early stopping at 5 with loss 0.5771
AttentionModel-training is done: 5/10
2017-07-31 | reset count: 0 | final loss: 0.5771 at epoch 3
[strategy_integration.py] set_seed(2017-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5640)training: 2/10 (0.5640)validation : 2/10 (0.5640)training: 3/10 (0.5640)validation : 3/10 (0.5638)training: 4/10 (0.5638)validation : 4/10 (0.5638)training: 5/10 (0.5638)validation : 5/10 (0.5638)early stopping at 5 with loss 0.5638
AttentionModel-training is done: 5/10
2017-08-31 | reset count: 0 | final loss: 0.5638 at epoch 3
making sample. (inference: True, date:2017-09-30 00:00:00, date_number: 369)
making sample. (inference: False, date:2017-08-31 00:00:00, date_number: 368)
[AttentionModel/2017-09-30 | get_logger | INFO]: ====================AttentionModel/2017-09-30 start====================
making sample. (inference: True, date:2017-10-31 00:00:00, date_number: 370)
making sample. (inference: False, date:2017-09-30 00:00:00, date_number: 369)
[AttentionModel/2017-10-31 | get_logger | INFO]: ====================AttentionModel/2017-10-31 start====================
[strategy_integration.py] set_seed(2017-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5558)training: 2/10 (0.5558)validation : 2/10 (0.5544)training: 3/10 (0.5544)validation : 3/10 (0.5544)training: 4/10 (0.5544)validation : 4/10 (0.5544)training: 5/10 (0.5544)validation : 5/10 (0.5544)early stopping at 5 with loss 0.5544
AttentionModel-training is done: 5/10
2017-09-30 | reset count: 0 | final loss: 0.5544 at epoch 2
[strategy_integration.py] set_seed(2017-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5707)training: 2/10 (0.5707)validation : 2/10 (0.5695)training: 3/10 (0.5695)validation : 3/10 (0.5695)training: 4/10 (0.5695)validation : 4/10 (0.5695)training: 5/10 (0.5695)validation : 5/10 (0.5695)early stopping at 5 with loss 0.5695
AttentionModel-training is done: 5/10
2017-10-31 | reset count: 0 | final loss: 0.5695 at epoch 2
making sample. (inference: True, date:2017-11-30 00:00:00, date_number: 371)
making sample. (inference: False, date:2017-10-31 00:00:00, date_number: 370)
[AttentionModel/2017-11-30 | get_logger | INFO]: ====================AttentionModel/2017-11-30 start====================
making sample. (inference: True, date:2017-12-31 00:00:00, date_number: 372)
making sample. (inference: False, date:2017-11-30 00:00:00, date_number: 371)
[AttentionModel/2017-12-31 | get_logger | INFO]: ====================AttentionModel/2017-12-31 start====================
[strategy_integration.py] set_seed(2017-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5665)training: 2/10 (0.5665)validation : 2/10 (0.5665)training: 3/10 (0.5665)validation : 3/10 (0.5665)training: 4/10 (0.5665)validation : 4/10 (0.5658)training: 5/10 (0.5658)validation : 5/10 (0.5658)training: 6/10 (0.5658)validation : 6/10 (0.5658)training: 7/10 (0.5658)validation : 7/10 (0.5658)early stopping at 7 with loss 0.5658
AttentionModel-training is done: 7/10
2017-11-30 | reset count: 0 | final loss: 0.5658 at epoch 4
[strategy_integration.py] set_seed(2017-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5704)training: 2/10 (0.5704)validation : 2/10 (0.5683)training: 3/10 (0.5683)validation : 3/10 (0.5659)training: 4/10 (0.5659)validation : 4/10 (0.5659)training: 5/10 (0.5659)validation : 5/10 (0.5659)early stopping at 5 with loss 0.5659
AttentionModel-training is done: 5/10
2017-12-31 | reset count: 0 | final loss: 0.5659 at epoch 3
making sample. (inference: True, date:2018-01-31 00:00:00, date_number: 373)
making sample. (inference: False, date:2017-12-31 00:00:00, date_number: 372)
[AttentionModel/2018-01-31 | get_logger | INFO]: ====================AttentionModel/2018-01-31 start====================
making sample. (inference: True, date:2018-02-28 00:00:00, date_number: 374)
making sample. (inference: False, date:2018-01-31 00:00:00, date_number: 373)
[AttentionModel/2018-02-28 | get_logger | INFO]: ====================AttentionModel/2018-02-28 start====================
[strategy_integration.py] set_seed(2018-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5701)training: 2/10 (0.5701)validation : 2/10 (0.5673)training: 3/10 (0.5673)validation : 3/10 (0.5673)training: 4/10 (0.5673)validation : 4/10 (0.5664)training: 5/10 (0.5664)validation : 5/10 (0.5664)training: 6/10 (0.5664)validation : 6/10 (0.5664)early stopping at 6 with loss 0.5664
AttentionModel-training is done: 6/10
2018-01-31 | reset count: 0 | final loss: 0.5664 at epoch 4
[strategy_integration.py] set_seed(2018-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5745)training: 2/10 (0.5745)validation : 2/10 (0.5659)training: 3/10 (0.5659)validation : 3/10 (0.5654)training: 4/10 (0.5654)validation : 4/10 (0.5654)training: 5/10 (0.5654)validation : 5/10 (0.5654)training: 6/10 (0.5654)validation : 6/10 (0.5654)early stopping at 6 with loss 0.5654
AttentionModel-training is done: 6/10
2018-02-28 | reset count: 0 | final loss: 0.5654 at epoch 3
making sample. (inference: True, date:2018-03-31 00:00:00, date_number: 375)
making sample. (inference: False, date:2018-02-28 00:00:00, date_number: 374)
[AttentionModel/2018-03-31 | get_logger | INFO]: ====================AttentionModel/2018-03-31 start====================
making sample. (inference: True, date:2018-04-30 00:00:00, date_number: 376)
making sample. (inference: False, date:2018-03-31 00:00:00, date_number: 375)
[AttentionModel/2018-04-30 | get_logger | INFO]: ====================AttentionModel/2018-04-30 start====================
[strategy_integration.py] set_seed(2018-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5622)training: 2/10 (0.5622)validation : 2/10 (0.5622)training: 3/10 (0.5622)validation : 3/10 (0.5593)training: 4/10 (0.5593)validation : 4/10 (0.5593)training: 5/10 (0.5593)validation : 5/10 (0.5593)training: 6/10 (0.5593)validation : 6/10 (0.5574)training: 7/10 (0.5574)validation : 7/10 (0.5574)training: 8/10 (0.5574)validation : 8/10 (0.5574)training: 9/10 (0.5574)validation : 9/10 (0.5574)early stopping at 9 with loss 0.5574
AttentionModel-training is done: 9/10
2018-03-31 | reset count: 0 | final loss: 0.5574 at epoch 6
[strategy_integration.py] set_seed(2018-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5766)training: 2/10 (0.5766)validation : 2/10 (0.5766)training: 3/10 (0.5766)validation : 3/10 (0.5766)training: 4/10 (0.5766)validation : 4/10 (0.5766)training: 5/10 (0.5766)validation : 5/10 (0.5766)early stopping at 5 with loss 0.5766
AttentionModel-training is done: 5/10
2018-04-30 | reset count: 0 | final loss: 0.5766 at epoch 1
making sample. (inference: True, date:2018-05-31 00:00:00, date_number: 377)
making sample. (inference: False, date:2018-04-30 00:00:00, date_number: 376)
[AttentionModel/2018-05-31 | get_logger | INFO]: ====================AttentionModel/2018-05-31 start====================
making sample. (inference: True, date:2018-06-30 00:00:00, date_number: 378)
making sample. (inference: False, date:2018-05-31 00:00:00, date_number: 377)
[AttentionModel/2018-06-30 | get_logger | INFO]: ====================AttentionModel/2018-06-30 start====================
[strategy_integration.py] set_seed(2018-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5692)training: 2/10 (0.5692)validation : 2/10 (0.5606)training: 3/10 (0.5606)validation : 3/10 (0.5606)training: 4/10 (0.5606)validation : 4/10 (0.5606)training: 5/10 (0.5606)validation : 5/10 (0.5606)early stopping at 5 with loss 0.5606
AttentionModel-training is done: 5/10
2018-05-31 | reset count: 0 | final loss: 0.5606 at epoch 2
making sample. (inference: True, date:2018-07-31 00:00:00, date_number: 379)
making sample. (inference: False, date:2018-06-30 00:00:00, date_number: 378)
[AttentionModel/2018-07-31 | get_logger | INFO]: ====================AttentionModel/2018-07-31 start====================
[strategy_integration.py] set_seed(2018-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5690)training: 2/10 (0.5690)validation : 2/10 (0.5690)training: 3/10 (0.5690)validation : 3/10 (0.5690)training: 4/10 (0.5690)validation : 4/10 (0.5690)training: 5/10 (0.5690)validation : 5/10 (0.5690)early stopping at 5 with loss 0.5690
AttentionModel-training is done: 5/10
2018-06-30 | reset count: 0 | final loss: 0.5690 at epoch 1
making sample. (inference: True, date:2018-08-31 00:00:00, date_number: 380)
making sample. (inference: False, date:2018-07-31 00:00:00, date_number: 379)
[strategy_integration.py] set_seed(2018-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5667)training: 2/10 (0.5667)validation : 2/10 (0.5660)training: 3/10 (0.5660)validation : 3/10 (0.5660)training: 4/10 (0.5660)validation : 4/10 (0.5660)training: 5/10 (0.5660)validation : 5/10 (0.5660)early stopping at 5 with loss 0.5660
AttentionModel-training is done: 5/10
2018-07-31 | reset count: 0 | final loss: 0.5660 at epoch 2
[AttentionModel/2018-08-31 | get_logger | INFO]: ====================AttentionModel/2018-08-31 start====================
making sample. (inference: True, date:2018-09-30 00:00:00, date_number: 381)
making sample. (inference: False, date:2018-08-31 00:00:00, date_number: 380)
[AttentionModel/2018-09-30 | get_logger | INFO]: ====================AttentionModel/2018-09-30 start====================
[strategy_integration.py] set_seed(2018-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5772)training: 2/10 (0.5772)validation : 2/10 (0.5697)training: 3/10 (0.5697)validation : 3/10 (0.5697)training: 4/10 (0.5697)validation : 4/10 (0.5697)training: 5/10 (0.5697)validation : 5/10 (0.5697)training: 6/10 (0.5697)validation : 6/10 (0.5697)training: 7/10 (0.5697)validation : 7/10 (0.5697)early stopping at 7 with loss 0.5697
AttentionModel-training is done: 7/10
2018-08-31 | reset count: 0 | final loss: 0.5697 at epoch 2
[strategy_integration.py] set_seed(2018-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5750)training: 2/10 (0.5750)validation : 2/10 (0.5686)training: 3/10 (0.5686)validation : 3/10 (0.5686)training: 4/10 (0.5686)validation : 4/10 (0.5686)training: 5/10 (0.5686)validation : 5/10 (0.5686)training: 6/10 (0.5686)validation : 6/10 (0.5686)early stopping at 6 with loss 0.5686
AttentionModel-training is done: 6/10
2018-09-30 | reset count: 0 | final loss: 0.5686 at epoch 2
making sample. (inference: True, date:2018-10-31 00:00:00, date_number: 382)
making sample. (inference: False, date:2018-09-30 00:00:00, date_number: 381)
[AttentionModel/2018-10-31 | get_logger | INFO]: ====================AttentionModel/2018-10-31 start====================
making sample. (inference: True, date:2018-11-30 00:00:00, date_number: 383)
making sample. (inference: False, date:2018-10-31 00:00:00, date_number: 382)
[AttentionModel/2018-11-30 | get_logger | INFO]: ====================AttentionModel/2018-11-30 start====================
[strategy_integration.py] set_seed(2018-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5775)training: 2/10 (0.5775)validation : 2/10 (0.5770)training: 3/10 (0.5770)validation : 3/10 (0.5767)training: 4/10 (0.5767)validation : 4/10 (0.5763)training: 5/10 (0.5763)validation : 5/10 (0.5740)training: 6/10 (0.5740)validation : 6/10 (0.5740)training: 7/10 (0.5740)validation : 7/10 (0.5740)training: 8/10 (0.5740)validation : 8/10 (0.5728)training: 9/10 (0.5728)validation : 9/10 (0.5728)early stopping at 9 with loss 0.5728
AttentionModel-training is done: 9/10
2018-10-31 | reset count: 0 | final loss: 0.5728 at epoch 8
making sample. (inference: True, date:2018-12-31 00:00:00, date_number: 384)
making sample. (inference: False, date:2018-11-30 00:00:00, date_number: 383)
[AttentionModel/2018-12-31 | get_logger | INFO]: ====================AttentionModel/2018-12-31 start====================
[strategy_integration.py] set_seed(2018-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5813)training: 2/10 (0.5813)validation : 2/10 (0.5813)training: 3/10 (0.5813)validation : 3/10 (0.5813)training: 4/10 (0.5813)validation : 4/10 (0.5810)training: 5/10 (0.5810)validation : 5/10 (0.5810)training: 6/10 (0.5810)validation : 6/10 (0.5810)training: 7/10 (0.5810)validation : 7/10 (0.5807)training: 8/10 (0.5807)validation : 8/10 (0.5798)training: 9/10 (0.5798)validation : 9/10 (0.5798)training: 10/10 (0.5798)validation : 10/10 (0.5798)early stopping at 10 with loss 0.5798
AttentionModel-training is done: 10/10
2018-11-30 | reset count: 0 | final loss: 0.5798 at epoch 8
making sample. (inference: True, date:2019-01-31 00:00:00, date_number: 385)
making sample. (inference: False, date:2018-12-31 00:00:00, date_number: 384)
[strategy_integration.py] set_seed(2018-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5689)training: 2/10 (0.5689)validation : 2/10 (0.5683)training: 3/10 (0.5683)validation : 3/10 (0.5662)training: 4/10 (0.5662)validation : 4/10 (0.5662)training: 5/10 (0.5662)validation : 5/10 (0.5662)training: 6/10 (0.5662)validation : 6/10 (0.5662)early stopping at 6 with loss 0.5662
AttentionModel-training is done: 6/10
2018-12-31 | reset count: 0 | final loss: 0.5662 at epoch 3
[AttentionModel/2019-01-31 | get_logger | INFO]: ====================AttentionModel/2019-01-31 start====================
making sample. (inference: True, date:2019-02-28 00:00:00, date_number: 386)
making sample. (inference: False, date:2019-01-31 00:00:00, date_number: 385)
[AttentionModel/2019-02-28 | get_logger | INFO]: ====================AttentionModel/2019-02-28 start====================
[strategy_integration.py] set_seed(2019-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5901)training: 2/10 (0.5901)validation : 2/10 (0.5901)training: 3/10 (0.5901)validation : 3/10 (0.5868)training: 4/10 (0.5868)validation : 4/10 (0.5868)training: 5/10 (0.5868)validation : 5/10 (0.5861)training: 6/10 (0.5861)validation : 6/10 (0.5861)early stopping at 6 with loss 0.5861
AttentionModel-training is done: 6/10
2019-01-31 | reset count: 0 | final loss: 0.5861 at epoch 5
[strategy_integration.py] set_seed(2019-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5926)training: 2/10 (0.5926)validation : 2/10 (0.5799)training: 3/10 (0.5799)validation : 3/10 (0.5799)training: 4/10 (0.5799)validation : 4/10 (0.5799)training: 5/10 (0.5799)validation : 5/10 (0.5799)early stopping at 5 with loss 0.5799
AttentionModel-training is done: 5/10
2019-02-28 | reset count: 0 | final loss: 0.5799 at epoch 2
making sample. (inference: True, date:2019-03-31 00:00:00, date_number: 387)
making sample. (inference: False, date:2019-02-28 00:00:00, date_number: 386)
[AttentionModel/2019-03-31 | get_logger | INFO]: ====================AttentionModel/2019-03-31 start====================
making sample. (inference: True, date:2019-04-30 00:00:00, date_number: 388)
making sample. (inference: False, date:2019-03-31 00:00:00, date_number: 387)
[AttentionModel/2019-04-30 | get_logger | INFO]: ====================AttentionModel/2019-04-30 start====================
[strategy_integration.py] set_seed(2019-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5858)training: 2/10 (0.5858)validation : 2/10 (0.5830)training: 3/10 (0.5830)validation : 3/10 (0.5830)training: 4/10 (0.5830)validation : 4/10 (0.5830)training: 5/10 (0.5830)validation : 5/10 (0.5818)training: 6/10 (0.5818)validation : 6/10 (0.5818)training: 7/10 (0.5818)validation : 7/10 (0.5784)training: 8/10 (0.5784)validation : 8/10 (0.5784)training: 9/10 (0.5784)validation : 9/10 (0.5784)early stopping at 9 with loss 0.5784
AttentionModel-training is done: 9/10
2019-03-31 | reset count: 0 | final loss: 0.5784 at epoch 7
[strategy_integration.py] set_seed(2019-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5867)training: 2/10 (0.5867)validation : 2/10 (0.5841)training: 3/10 (0.5841)validation : 3/10 (0.5841)training: 4/10 (0.5841)validation : 4/10 (0.5830)training: 5/10 (0.5830)validation : 5/10 (0.5830)training: 6/10 (0.5830)validation : 6/10 (0.5830)training: 7/10 (0.5830)validation : 7/10 (0.5830)early stopping at 7 with loss 0.5830
AttentionModel-training is done: 7/10
2019-04-30 | reset count: 0 | final loss: 0.5830 at epoch 4
making sample. (inference: True, date:2019-05-31 00:00:00, date_number: 389)
making sample. (inference: False, date:2019-04-30 00:00:00, date_number: 388)
[AttentionModel/2019-05-31 | get_logger | INFO]: ====================AttentionModel/2019-05-31 start====================
making sample. (inference: True, date:2019-06-30 00:00:00, date_number: 390)
making sample. (inference: False, date:2019-05-31 00:00:00, date_number: 389)
[AttentionModel/2019-06-30 | get_logger | INFO]: ====================AttentionModel/2019-06-30 start====================
[strategy_integration.py] set_seed(2019-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5914)training: 2/10 (0.5914)validation : 2/10 (0.5864)training: 3/10 (0.5864)validation : 3/10 (0.5864)training: 4/10 (0.5864)validation : 4/10 (0.5864)training: 5/10 (0.5864)validation : 5/10 (0.5864)early stopping at 5 with loss 0.5864
AttentionModel-training is done: 5/10
2019-05-31 | reset count: 0 | final loss: 0.5864 at epoch 2
making sample. (inference: True, date:2019-07-31 00:00:00, date_number: 391)
making sample. (inference: False, date:2019-06-30 00:00:00, date_number: 390)
[AttentionModel/2019-07-31 | get_logger | INFO]: ====================AttentionModel/2019-07-31 start====================
[strategy_integration.py] set_seed(2019-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5899)training: 2/10 (0.5899)validation : 2/10 (0.5895)training: 3/10 (0.5895)validation : 3/10 (0.5831)training: 4/10 (0.5831)validation : 4/10 (0.5831)training: 5/10 (0.5831)validation : 5/10 (0.5831)training: 6/10 (0.5831)validation : 6/10 (0.5831)early stopping at 6 with loss 0.5831
AttentionModel-training is done: 6/10
2019-06-30 | reset count: 0 | final loss: 0.5831 at epoch 3
[strategy_integration.py] set_seed(2019-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5838)training: 2/10 (0.5838)validation : 2/10 (0.5822)training: 3/10 (0.5822)validation : 3/10 (0.5814)training: 4/10 (0.5814)validation : 4/10 (0.5814)training: 5/10 (0.5814)validation : 5/10 (0.5814)early stopping at 5 with loss 0.5814
AttentionModel-training is done: 5/10
2019-07-31 | reset count: 0 | final loss: 0.5814 at epoch 3
making sample. (inference: True, date:2019-08-31 00:00:00, date_number: 392)
making sample. (inference: False, date:2019-07-31 00:00:00, date_number: 391)
[AttentionModel/2019-08-31 | get_logger | INFO]: ====================AttentionModel/2019-08-31 start====================
making sample. (inference: True, date:2019-09-30 00:00:00, date_number: 393)
making sample. (inference: False, date:2019-08-31 00:00:00, date_number: 392)
[AttentionModel/2019-09-30 | get_logger | INFO]: ====================AttentionModel/2019-09-30 start====================
[strategy_integration.py] set_seed(2019-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5889)training: 2/10 (0.5889)validation : 2/10 (0.5819)training: 3/10 (0.5819)validation : 3/10 (0.5819)training: 4/10 (0.5819)validation : 4/10 (0.5819)training: 5/10 (0.5819)validation : 5/10 (0.5819)early stopping at 5 with loss 0.5819
AttentionModel-training is done: 5/10
2019-08-31 | reset count: 0 | final loss: 0.5819 at epoch 2
making sample. (inference: True, date:2019-10-31 00:00:00, date_number: 394)
making sample. (inference: False, date:2019-09-30 00:00:00, date_number: 393)
[AttentionModel/2019-10-31 | get_logger | INFO]: ====================AttentionModel/2019-10-31 start====================
[strategy_integration.py] set_seed(2019-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5986)training: 2/10 (0.5986)validation : 2/10 (0.5949)training: 3/10 (0.5949)validation : 3/10 (0.5930)training: 4/10 (0.5930)validation : 4/10 (0.5916)training: 5/10 (0.5916)validation : 5/10 (0.5910)training: 6/10 (0.5910)validation : 6/10 (0.5910)training: 7/10 (0.5910)validation : 7/10 (0.5894)training: 8/10 (0.5894)validation : 8/10 (0.5894)early stopping at 8 with loss 0.5894
AttentionModel-training is done: 8/10
2019-09-30 | reset count: 0 | final loss: 0.5894 at epoch 7
[strategy_integration.py] set_seed(2019-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5863)training: 2/10 (0.5863)validation : 2/10 (0.5863)training: 3/10 (0.5863)validation : 3/10 (0.5863)training: 4/10 (0.5863)validation : 4/10 (0.5863)training: 5/10 (0.5863)validation : 5/10 (0.5863)early stopping at 5 with loss 0.5863
AttentionModel-training is done: 5/10
2019-10-31 | reset count: 0 | final loss: 0.5863 at epoch 1
making sample. (inference: True, date:2019-11-30 00:00:00, date_number: 395)
making sample. (inference: False, date:2019-10-31 00:00:00, date_number: 394)
[AttentionModel/2019-11-30 | get_logger | INFO]: ====================AttentionModel/2019-11-30 start====================
making sample. (inference: True, date:2019-12-31 00:00:00, date_number: 396)
making sample. (inference: False, date:2019-11-30 00:00:00, date_number: 395)
[AttentionModel/2019-12-31 | get_logger | INFO]: ====================AttentionModel/2019-12-31 start====================
[strategy_integration.py] set_seed(2019-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5851)training: 2/10 (0.5851)validation : 2/10 (0.5851)training: 3/10 (0.5851)validation : 3/10 (0.5851)training: 4/10 (0.5851)validation : 4/10 (0.5805)training: 5/10 (0.5805)validation : 5/10 (0.5805)training: 6/10 (0.5805)validation : 6/10 (0.5805)training: 7/10 (0.5805)validation : 7/10 (0.5805)early stopping at 7 with loss 0.5805
AttentionModel-training is done: 7/10
2019-11-30 | reset count: 0 | final loss: 0.5805 at epoch 4
[strategy_integration.py] set_seed(2019-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5818)training: 2/10 (0.5818)validation : 2/10 (0.5813)training: 3/10 (0.5813)validation : 3/10 (0.5793)training: 4/10 (0.5793)validation : 4/10 (0.5793)training: 5/10 (0.5793)validation : 5/10 (0.5793)early stopping at 5 with loss 0.5793
AttentionModel-training is done: 5/10
2019-12-31 | reset count: 0 | final loss: 0.5793 at epoch 3
making sample. (inference: True, date:2020-01-31 00:00:00, date_number: 397)
making sample. (inference: False, date:2019-12-31 00:00:00, date_number: 396)
[AttentionModel/2020-01-31 | get_logger | INFO]: ====================AttentionModel/2020-01-31 start====================
making sample. (inference: True, date:2020-02-29 00:00:00, date_number: 398)
making sample. (inference: False, date:2020-01-31 00:00:00, date_number: 397)
[AttentionModel/2020-02-29 | get_logger | INFO]: ====================AttentionModel/2020-02-29 start====================
[strategy_integration.py] set_seed(2020-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5875)training: 2/10 (0.5875)validation : 2/10 (0.5851)training: 3/10 (0.5851)validation : 3/10 (0.5851)training: 4/10 (0.5851)validation : 4/10 (0.5851)training: 5/10 (0.5851)validation : 5/10 (0.5851)early stopping at 5 with loss 0.5851
AttentionModel-training is done: 5/10
2020-01-31 | reset count: 0 | final loss: 0.5851 at epoch 2
[strategy_integration.py] set_seed(2020-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5784)training: 2/10 (0.5784)validation : 2/10 (0.5784)training: 3/10 (0.5784)validation : 3/10 (0.5784)training: 4/10 (0.5784)validation : 4/10 (0.5784)training: 5/10 (0.5784)validation : 5/10 (0.5784)early stopping at 5 with loss 0.5784
AttentionModel-training is done: 5/10
2020-02-29 | reset count: 0 | final loss: 0.5784 at epoch 1
making sample. (inference: True, date:2020-03-31 00:00:00, date_number: 399)
making sample. (inference: False, date:2020-02-29 00:00:00, date_number: 398)
[AttentionModel/2020-03-31 | get_logger | INFO]: ====================AttentionModel/2020-03-31 start====================
making sample. (inference: True, date:2020-04-30 00:00:00, date_number: 400)
making sample. (inference: False, date:2020-03-31 00:00:00, date_number: 399)
[AttentionModel/2020-04-30 | get_logger | INFO]: ====================AttentionModel/2020-04-30 start====================
[strategy_integration.py] set_seed(2020-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5858)training: 2/10 (0.5858)validation : 2/10 (0.5778)training: 3/10 (0.5778)validation : 3/10 (0.5778)training: 4/10 (0.5778)validation : 4/10 (0.5778)training: 5/10 (0.5778)validation : 5/10 (0.5778)training: 6/10 (0.5778)validation : 6/10 (0.5778)training: 7/10 (0.5778)validation : 7/10 (0.5778)early stopping at 7 with loss 0.5778
AttentionModel-training is done: 7/10
2020-03-31 | reset count: 0 | final loss: 0.5778 at epoch 2
[strategy_integration.py] set_seed(2020-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5745)training: 2/10 (0.5745)validation : 2/10 (0.5741)training: 3/10 (0.5741)validation : 3/10 (0.5741)training: 4/10 (0.5741)validation : 4/10 (0.5741)training: 5/10 (0.5741)validation : 5/10 (0.5741)early stopping at 5 with loss 0.5741
AttentionModel-training is done: 5/10
2020-04-30 | reset count: 0 | final loss: 0.5741 at epoch 2
making sample. (inference: True, date:2020-05-31 00:00:00, date_number: 401)
making sample. (inference: False, date:2020-04-30 00:00:00, date_number: 400)
[AttentionModel/2020-05-31 | get_logger | INFO]: ====================AttentionModel/2020-05-31 start====================
making sample. (inference: True, date:2020-06-30 00:00:00, date_number: 402)
making sample. (inference: False, date:2020-05-31 00:00:00, date_number: 401)
[AttentionModel/2020-06-30 | get_logger | INFO]: ====================AttentionModel/2020-06-30 start====================
[strategy_integration.py] set_seed(2020-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5831)training: 2/10 (0.5831)validation : 2/10 (0.5785)training: 3/10 (0.5785)validation : 3/10 (0.5769)training: 4/10 (0.5769)validation : 4/10 (0.5769)training: 5/10 (0.5769)validation : 5/10 (0.5769)training: 6/10 (0.5769)validation : 6/10 (0.5769)early stopping at 6 with loss 0.5769
AttentionModel-training is done: 6/10
2020-05-31 | reset count: 0 | final loss: 0.5769 at epoch 3
making sample. (inference: True, date:2020-07-31 00:00:00, date_number: 403)
making sample. (inference: False, date:2020-06-30 00:00:00, date_number: 402)
[strategy_integration.py] set_seed(2020-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5767)training: 2/10 (0.5767)validation : 2/10 (0.5767)training: 3/10 (0.5767)validation : 3/10 (0.5767)training: 4/10 (0.5767)validation : 4/10 (0.5730)training: 5/10 (0.5730)validation : 5/10 (0.5730)training: 6/10 (0.5730)validation : 6/10 (0.5730)training: 7/10 (0.5730)validation : 7/10 (0.5730)early stopping at 7 with loss 0.5730
AttentionModel-training is done: 7/10
2020-06-30 | reset count: 0 | final loss: 0.5730 at epoch 4
[AttentionModel/2020-07-31 | get_logger | INFO]: ====================AttentionModel/2020-07-31 start====================
making sample. (inference: True, date:2020-08-31 00:00:00, date_number: 404)
making sample. (inference: False, date:2020-07-31 00:00:00, date_number: 403)
[AttentionModel/2020-08-31 | get_logger | INFO]: ====================AttentionModel/2020-08-31 start====================
[strategy_integration.py] set_seed(2020-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5774)training: 2/10 (0.5774)validation : 2/10 (0.5774)training: 3/10 (0.5774)validation : 3/10 (0.5765)training: 4/10 (0.5765)validation : 4/10 (0.5765)training: 5/10 (0.5765)validation : 5/10 (0.5765)early stopping at 5 with loss 0.5765
AttentionModel-training is done: 5/10
2020-07-31 | reset count: 0 | final loss: 0.5765 at epoch 3
making sample. (inference: True, date:2020-09-30 00:00:00, date_number: 405)
making sample. (inference: False, date:2020-08-31 00:00:00, date_number: 404)
[AttentionModel/2020-09-30 | get_logger | INFO]: ====================AttentionModel/2020-09-30 start====================
[strategy_integration.py] set_seed(2020-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5749)training: 2/10 (0.5749)validation : 2/10 (0.5749)training: 3/10 (0.5749)validation : 3/10 (0.5726)training: 4/10 (0.5726)validation : 4/10 (0.5726)training: 5/10 (0.5726)validation : 5/10 (0.5726)training: 6/10 (0.5726)validation : 6/10 (0.5726)early stopping at 6 with loss 0.5726
AttentionModel-training is done: 6/10
2020-08-31 | reset count: 0 | final loss: 0.5726 at epoch 3
making sample. (inference: True, date:2020-10-31 00:00:00, date_number: 406)
making sample. (inference: False, date:2020-09-30 00:00:00, date_number: 405)
[AttentionModel/2020-10-31 | get_logger | INFO]: ====================AttentionModel/2020-10-31 start====================
[strategy_integration.py] set_seed(2020-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5750)training: 2/10 (0.5750)validation : 2/10 (0.5737)training: 3/10 (0.5737)validation : 3/10 (0.5737)training: 4/10 (0.5737)validation : 4/10 (0.5737)training: 5/10 (0.5737)validation : 5/10 (0.5737)early stopping at 5 with loss 0.5737
AttentionModel-training is done: 5/10
2020-09-30 | reset count: 0 | final loss: 0.5737 at epoch 2
making sample. (inference: True, date:2020-11-30 00:00:00, date_number: 407)
making sample. (inference: False, date:2020-10-31 00:00:00, date_number: 406)
[AttentionModel/2020-11-30 | get_logger | INFO]: ====================AttentionModel/2020-11-30 start====================
[strategy_integration.py] set_seed(2020-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5877)training: 2/10 (0.5877)validation : 2/10 (0.5821)training: 3/10 (0.5821)validation : 3/10 (0.5821)training: 4/10 (0.5821)validation : 4/10 (0.5821)training: 5/10 (0.5821)validation : 5/10 (0.5798)training: 6/10 (0.5798)validation : 6/10 (0.5798)training: 7/10 (0.5798)validation : 7/10 (0.5798)early stopping at 7 with loss 0.5798
AttentionModel-training is done: 7/10
2020-10-31 | reset count: 0 | final loss: 0.5798 at epoch 5
making sample. (inference: True, date:2020-12-31 00:00:00, date_number: 408)
making sample. (inference: False, date:2020-11-30 00:00:00, date_number: 407)
[strategy_integration.py] set_seed(2020-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5670)training: 2/10 (0.5670)validation : 2/10 (0.5670)training: 3/10 (0.5670)validation : 3/10 (0.5670)training: 4/10 (0.5670)validation : 4/10 (0.5650)training: 5/10 (0.5650)validation : 5/10 (0.5650)training: 6/10 (0.5650)validation : 6/10 (0.5650)training: 7/10 (0.5650)validation : 7/10 (0.5650)early stopping at 7 with loss 0.5650
AttentionModel-training is done: 7/10
2020-11-30 | reset count: 0 | final loss: 0.5650 at epoch 4
[AttentionModel/2020-12-31 | get_logger | INFO]: ====================AttentionModel/2020-12-31 start====================
making sample. (inference: True, date:2021-01-31 00:00:00, date_number: 409)
making sample. (inference: False, date:2020-12-31 00:00:00, date_number: 408)
[AttentionModel/2021-01-31 | get_logger | INFO]: ====================AttentionModel/2021-01-31 start====================
[strategy_integration.py] set_seed(2020-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5691)training: 2/10 (0.5691)validation : 2/10 (0.5691)training: 3/10 (0.5691)validation : 3/10 (0.5669)training: 4/10 (0.5669)validation : 4/10 (0.5635)training: 5/10 (0.5635)validation : 5/10 (0.5635)training: 6/10 (0.5635)validation : 6/10 (0.5635)early stopping at 6 with loss 0.5635
AttentionModel-training is done: 6/10
2020-12-31 | reset count: 0 | final loss: 0.5635 at epoch 4
[strategy_integration.py] set_seed(2021-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5631)training: 2/10 (0.5631)validation : 2/10 (0.5631)training: 3/10 (0.5631)validation : 3/10 (0.5605)training: 4/10 (0.5605)validation : 4/10 (0.5605)training: 5/10 (0.5605)validation : 5/10 (0.5570)training: 6/10 (0.5570)validation : 6/10 (0.5570)early stopping at 6 with loss 0.5570
AttentionModel-training is done: 6/10
2021-01-31 | reset count: 0 | final loss: 0.5570 at epoch 5
making sample. (inference: True, date:2021-02-28 00:00:00, date_number: 410)
making sample. (inference: False, date:2021-01-31 00:00:00, date_number: 409)
[AttentionModel/2021-02-28 | get_logger | INFO]: ====================AttentionModel/2021-02-28 start====================
making sample. (inference: True, date:2021-03-31 00:00:00, date_number: 411)
making sample. (inference: False, date:2021-02-28 00:00:00, date_number: 410)
[AttentionModel/2021-03-31 | get_logger | INFO]: ====================AttentionModel/2021-03-31 start====================
[strategy_integration.py] set_seed(2021-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5717)training: 2/10 (0.5717)validation : 2/10 (0.5675)training: 3/10 (0.5675)validation : 3/10 (0.5675)training: 4/10 (0.5675)validation : 4/10 (0.5675)training: 5/10 (0.5675)validation : 5/10 (0.5640)training: 6/10 (0.5640)validation : 6/10 (0.5625)training: 7/10 (0.5625)validation : 7/10 (0.5625)early stopping at 7 with loss 0.5625
AttentionModel-training is done: 7/10
2021-02-28 | reset count: 0 | final loss: 0.5625 at epoch 6
making sample. (inference: True, date:2021-04-30 00:00:00, date_number: 412)
making sample. (inference: False, date:2021-03-31 00:00:00, date_number: 411)
[AttentionModel/2021-04-30 | get_logger | INFO]: ====================AttentionModel/2021-04-30 start====================
[strategy_integration.py] set_seed(2021-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5574)training: 2/10 (0.5574)validation : 2/10 (0.5570)training: 3/10 (0.5570)validation : 3/10 (0.5552)training: 4/10 (0.5552)validation : 4/10 (0.5552)training: 5/10 (0.5552)validation : 5/10 (0.5533)training: 6/10 (0.5533)validation : 6/10 (0.5533)training: 7/10 (0.5533)validation : 7/10 (0.5533)training: 8/10 (0.5533)validation : 8/10 (0.5533)early stopping at 8 with loss 0.5533
AttentionModel-training is done: 8/10
2021-03-31 | reset count: 0 | final loss: 0.5533 at epoch 5
[strategy_integration.py] set_seed(2021-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5694)training: 2/10 (0.5694)validation : 2/10 (0.5622)training: 3/10 (0.5622)validation : 3/10 (0.5622)training: 4/10 (0.5622)validation : 4/10 (0.5622)training: 5/10 (0.5622)validation : 5/10 (0.5622)early stopping at 5 with loss 0.5622
AttentionModel-training is done: 5/10
2021-04-30 | reset count: 0 | final loss: 0.5622 at epoch 2
making sample. (inference: True, date:2021-05-31 00:00:00, date_number: 413)
making sample. (inference: False, date:2021-04-30 00:00:00, date_number: 412)
[AttentionModel/2021-05-31 | get_logger | INFO]: ====================AttentionModel/2021-05-31 start====================
making sample. (inference: True, date:2021-06-30 00:00:00, date_number: 414)
making sample. (inference: False, date:2021-05-31 00:00:00, date_number: 413)
[AttentionModel/2021-06-30 | get_logger | INFO]: ====================AttentionModel/2021-06-30 start====================
[strategy_integration.py] set_seed(2021-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5724)training: 2/10 (0.5724)validation : 2/10 (0.5654)training: 3/10 (0.5654)validation : 3/10 (0.5643)training: 4/10 (0.5643)validation : 4/10 (0.5643)training: 5/10 (0.5643)validation : 5/10 (0.5643)early stopping at 5 with loss 0.5643
AttentionModel-training is done: 5/10
2021-05-31 | reset count: 0 | final loss: 0.5643 at epoch 3
[strategy_integration.py] set_seed(2021-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5737)training: 2/10 (0.5737)validation : 2/10 (0.5697)training: 3/10 (0.5697)validation : 3/10 (0.5661)training: 4/10 (0.5661)validation : 4/10 (0.5655)training: 5/10 (0.5655)validation : 5/10 (0.5655)training: 6/10 (0.5655)validation : 6/10 (0.5655)early stopping at 6 with loss 0.5655
AttentionModel-training is done: 6/10
2021-06-30 | reset count: 0 | final loss: 0.5655 at epoch 4
making sample. (inference: True, date:2021-07-31 00:00:00, date_number: 415)
making sample. (inference: False, date:2021-06-30 00:00:00, date_number: 414)
[AttentionModel/2021-07-31 | get_logger | INFO]: ====================AttentionModel/2021-07-31 start====================
making sample. (inference: True, date:2021-08-31 00:00:00, date_number: 416)
making sample. (inference: False, date:2021-07-31 00:00:00, date_number: 415)
[AttentionModel/2021-08-31 | get_logger | INFO]: ====================AttentionModel/2021-08-31 start====================
[strategy_integration.py] set_seed(2021-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5666)training: 2/10 (0.5666)validation : 2/10 (0.5666)training: 3/10 (0.5666)validation : 3/10 (0.5666)training: 4/10 (0.5666)validation : 4/10 (0.5620)training: 5/10 (0.5620)validation : 5/10 (0.5620)training: 6/10 (0.5620)validation : 6/10 (0.5620)training: 7/10 (0.5620)validation : 7/10 (0.5620)early stopping at 7 with loss 0.5620
AttentionModel-training is done: 7/10
2021-07-31 | reset count: 0 | final loss: 0.5620 at epoch 4
[strategy_integration.py] set_seed(2021-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5712)training: 2/10 (0.5712)validation : 2/10 (0.5712)training: 3/10 (0.5712)validation : 3/10 (0.5689)training: 4/10 (0.5689)validation : 4/10 (0.5689)training: 5/10 (0.5689)validation : 5/10 (0.5667)training: 6/10 (0.5667)validation : 6/10 (0.5667)training: 7/10 (0.5667)validation : 7/10 (0.5667)training: 8/10 (0.5667)validation : 8/10 (0.5667)early stopping at 8 with loss 0.5667
AttentionModel-training is done: 8/10
2021-08-31 | reset count: 0 | final loss: 0.5667 at epoch 5
making sample. (inference: True, date:2021-09-30 00:00:00, date_number: 417)
making sample. (inference: False, date:2021-08-31 00:00:00, date_number: 416)
[AttentionModel/2021-09-30 | get_logger | INFO]: ====================AttentionModel/2021-09-30 start====================
making sample. (inference: True, date:2021-10-31 00:00:00, date_number: 418)
making sample. (inference: False, date:2021-09-30 00:00:00, date_number: 417)
[AttentionModel/2021-10-31 | get_logger | INFO]: ====================AttentionModel/2021-10-31 start====================
[strategy_integration.py] set_seed(2021-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5789)training: 2/10 (0.5789)validation : 2/10 (0.5777)training: 3/10 (0.5777)validation : 3/10 (0.5777)training: 4/10 (0.5777)validation : 4/10 (0.5733)training: 5/10 (0.5733)validation : 5/10 (0.5733)training: 6/10 (0.5733)validation : 6/10 (0.5733)training: 7/10 (0.5733)validation : 7/10 (0.5733)early stopping at 7 with loss 0.5733
AttentionModel-training is done: 7/10
2021-09-30 | reset count: 0 | final loss: 0.5733 at epoch 4
[strategy_integration.py] set_seed(2021-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5832)training: 2/10 (0.5832)validation : 2/10 (0.5824)training: 3/10 (0.5824)validation : 3/10 (0.5824)training: 4/10 (0.5824)validation : 4/10 (0.5804)training: 5/10 (0.5804)validation : 5/10 (0.5775)training: 6/10 (0.5775)validation : 6/10 (0.5767)training: 7/10 (0.5767)validation : 7/10 (0.5767)training: 8/10 (0.5767)validation : 8/10 (0.5767)early stopping at 8 with loss 0.5767
AttentionModel-training is done: 8/10
2021-10-31 | reset count: 0 | final loss: 0.5767 at epoch 6
[strategy | get_logger | INFO]: ====================strategy start====================
⠋ [Compustat API] : Cleaning past days cache✓ [Compustat API] : Cleaning past days cache done in 0.001 secs
auto caching이 활성화 되었습니다. /home/sronly/sr-storage/kirin_cache/kirin_api_cache/2.19.17_2021-11-24에 캐쉬를 저장합니다.
⠋ [Compustat API] : Loading meta table⠙ [Compustat API] : Loading meta table⠹ [Compustat API] : Loading meta table⠸ [Compustat API] : Loading meta table⠼ [Compustat API] : Loading meta table⠴ [Compustat API] : Loading meta table⠦ [Compustat API] : Loading meta table⠧ [Compustat API] : Loading meta table⠇ [Compustat API] : Loading meta table⠏ [Compustat API] : Loading meta table⠋ [Compustat API] : Loading meta table⠙ [Compustat API] : Loading meta table⠹ [Compustat API] : Loading meta table⠸ [Compustat API] : Loading meta table⠼ [Compustat API] : Loading meta table⠴ [Compustat API] : Loading meta table⠦ [Compustat API] : Loading meta table⠧ [Compustat API] : Loading meta table⠇ [Compustat API] : Loading meta table⠏ [Compustat API] : Loading meta table⠋ [Compustat API] : Loading meta table⠙ [Compustat API] : Loading meta table⠹ [Compustat API] : Loading meta table⠸ [Compustat API] : Loading meta table⠼ [Compustat API] : Loading meta table⠴ [Compustat API] : Loading meta table⠦ [Compustat API] : Loading meta table⠧ [Compustat API] : Loading meta table⠇ [Compustat API] : Loading meta table⠏ [Compustat API] : Loading meta table⠋ [Compustat API] : Loading meta table⠙ [Compustat API] : Loading meta table⠹ [Compustat API] : Loading meta table⠸ [Compustat API] : Loading meta table⠼ [Compustat API] : Loading meta table⠴ [Compustat API] : Loading meta table⠦ [Compustat API] : Loading meta table⠧ [Compustat API] : Loading meta table⠇ [Compustat API] : Loading meta table⠏ [Compustat API] : Loading meta table⠋ [Compustat API] : Loading meta table⠙ [Compustat API] : Loading meta table⠹ [Compustat API] : Loading meta table⠸ [Compustat API] : Loading meta table⠼ [Compustat API] : Loading meta table⠴ [Compustat API] : Loading meta table⠦ [Compustat API] : Loading meta table⠧ [Compustat API] : Loading meta table⠇ [Compustat API] : Loading meta table⠏ [Compustat API] : Loading meta table⠋ [Compustat API] : Loading meta table⠙ [Compustat API] : Loading meta table⠹ [Compustat API] : Loading meta table⠸ [Compustat API] : Loading meta table⠼ [Compustat API] : Loading meta table⠴ [Compustat API] : Loading meta table⠦ [Compustat API] : Loading meta table⠧ [Compustat API] : Loading meta table⠇ [Compustat API] : Loading meta table⠏ [Compustat API] : Loading meta table⠋ [Compustat API] : Loading meta table⠙ [Compustat API] : Loading meta table⠹ [Compustat API] : Loading meta table⠸ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 29.374 secs
⠋ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading masking data⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading masking data⠦ [Compustat API] : Loading masking data⠧ [Compustat API] : Loading masking data⠇ [Compustat API] : Loading masking data⠏ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading masking data⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading masking data⠦ [Compustat API] : Loading masking data⠧ [Compustat API] : Loading masking data⠇ [Compustat API] : Loading masking data⠏ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading masking data⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 8.812 secs
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.183 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading get_historic_universe from cache⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading masking data✓ [Compustat API] : Loading get_historic_universe from cache done in 1.925 secs
⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.225 secs
⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data✓ [Compustat API] : Loading monthly price data done in 19.577 secs
⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data✓ [Compustat API] : Loading monthly price data done in 25.385 secs
⠋ [Compustat API] : Loading monthly market value⠙ [Compustat API] : Loading monthly market value⠹ [Compustat API] : Loading monthly market value⠸ [Compustat API] : Loading monthly market value⠼ [Compustat API] : Loading monthly market value⠴ [Compustat API] : Loading monthly market value⠦ [Compustat API] : Loading monthly market value⠧ [Compustat API] : Loading monthly market value⠇ [Compustat API] : Loading monthly market value⠏ [Compustat API] : Loading monthly market value⠋ [Compustat API] : Loading monthly market value⠙ [Compustat API] : Loading monthly market value⠹ [Compustat API] : Loading monthly market value⠸ [Compustat API] : Loading monthly market value⠼ [Compustat API] : Loading monthly market value⠴ [Compustat API] : Loading monthly market value⠦ [Compustat API] : Loading monthly market value⠧ [Compustat API] : Loading monthly market value⠇ [Compustat API] : Loading monthly market value⠏ [Compustat API] : Loading monthly market value⠋ [Compustat API] : Loading monthly market value⠙ [Compustat API] : Loading monthly market value⠹ [Compustat API] : Loading monthly market value⠸ [Compustat API] : Loading monthly market value⠼ [Compustat API] : Loading monthly market value⠴ [Compustat API] : Loading monthly market value⠦ [Compustat API] : Loading monthly market value⠧ [Compustat API] : Loading monthly market value⠇ [Compustat API] : Loading monthly market value⠏ [Compustat API] : Loading monthly market value⠋ [Compustat API] : Loading monthly market value⠙ [Compustat API] : Loading monthly market value⠹ [Compustat API] : Loading monthly market value⠸ [Compustat API] : Loading monthly market value⠼ [Compustat API] : Loading monthly market value⠴ [Compustat API] : Loading monthly market value⠦ [Compustat API] : Loading monthly market value⠧ [Compustat API] : Loading monthly market value⠇ [Compustat API] : Loading monthly market value⠏ [Compustat API] : Loading monthly market value⠋ [Compustat API] : Loading monthly market value⠙ [Compustat API] : Loading monthly market value⠹ [Compustat API] : Loading monthly market value⠸ [Compustat API] : Loading monthly market value⠼ [Compustat API] : Loading monthly market value⠴ [Compustat API] : Loading monthly market value⠦ [Compustat API] : Loading monthly market value⠧ [Compustat API] : Loading monthly market value⠇ [Compustat API] : Loading monthly market value⠏ [Compustat API] : Loading monthly market value⠋ [Compustat API] : Loading monthly market value⠙ [Compustat API] : Loading monthly market value⠹ [Compustat API] : Loading monthly market value⠸ [Compustat API] : Loading monthly market value⠼ [Compustat API] : Loading monthly market value⠴ [Compustat API] : Loading monthly market value⠦ [Compustat API] : Loading monthly market value⠧ [Compustat API] : Loading monthly market value⠇ [Compustat API] : Loading monthly market value⠏ [Compustat API] : Loading monthly market value⠋ [Compustat API] : Loading monthly market value⠙ [Compustat API] : Loading monthly market value⠹ [Compustat API] : Loading monthly market value⠸ [Compustat API] : Loading monthly market value⠼ [Compustat API] : Loading monthly market value⠴ [Compustat API] : Loading monthly market value⠦ [Compustat API] : Loading monthly market value⠧ [Compustat API] : Loading monthly market value⠇ [Compustat API] : Loading monthly market value⠏ [Compustat API] : Loading monthly market value⠋ [Compustat API] : Loading monthly market value⠙ [Compustat API] : Loading monthly market value⠹ [Compustat API] : Loading monthly market value⠸ [Compustat API] : Loading monthly market value⠼ [Compustat API] : Loading monthly market value⠴ [Compustat API] : Loading monthly market value⠦ [Compustat API] : Loading monthly market value⠧ [Compustat API] : Loading monthly market value⠇ [Compustat API] : Loading monthly market value⠏ [Compustat API] : Loading monthly market value⠋ [Compustat API] : Loading monthly market value⠙ [Compustat API] : Loading monthly market value⠹ [Compustat API] : Loading monthly market value⠸ [Compustat API] : Loading monthly market value⠼ [Compustat API] : Loading monthly market value⠴ [Compustat API] : Loading monthly market value⠦ [Compustat API] : Loading monthly market value⠧ [Compustat API] : Loading monthly market value⠇ [Compustat API] : Loading monthly market value⠏ [Compustat API] : Loading monthly market value⠋ [Compustat API] : Loading monthly market value⠙ [Compustat API] : Loading monthly market value⠹ [Compustat API] : Loading monthly market value⠸ [Compustat API] : Loading monthly market value⠼ [Compustat API] : Loading monthly market value⠴ [Compustat API] : Loading monthly market value⠦ [Compustat API] : Loading monthly market value⠧ [Compustat API] : Loading monthly market value⠇ [Compustat API] : Loading monthly market value⠏ [Compustat API] : Loading monthly market value⠋ [Compustat API] : Loading monthly market value⠙ [Compustat API] : Loading monthly market value⠹ [Compustat API] : Loading monthly market value⠸ [Compustat API] : Loading monthly market value⠼ [Compustat API] : Loading monthly market value⠴ [Compustat API] : Loading monthly market value⠦ [Compustat API] : Loading monthly market value⠧ [Compustat API] : Loading monthly market valueOMP: Info #274: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.
[strategy | _get_infer_dirs | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick350_test02/infer 이 strategy 대상에 추가됨
[strategy | _backtest | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick350_test02/infer is starting
⠇ [Compustat API] : Loading monthly market value⠏ [Compustat API] : Loading monthly market value✓ [Compustat API] : Loading monthly market value done in 40.841 secs
⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data✓ [Compustat API] : Loading monthly price data done in 19.09 secs
⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data⠏ [Compustat API] : Loading monthly price data⠋ [Compustat API] : Loading monthly price data⠙ [Compustat API] : Loading monthly price data⠹ [Compustat API] : Loading monthly price data⠸ [Compustat API] : Loading monthly price data⠼ [Compustat API] : Loading monthly price data⠴ [Compustat API] : Loading monthly price data⠦ [Compustat API] : Loading monthly price data⠧ [Compustat API] : Loading monthly price data⠇ [Compustat API] : Loading monthly price data✓ [Compustat API] : Loading monthly price data done in 12.735 secs
⠋ [Compustat API] : Loading monthly volume data⠙ [Compustat API] : Loading monthly volume data⠹ [Compustat API] : Loading monthly volume data⠸ [Compustat API] : Loading monthly volume data⠼ [Compustat API] : Loading monthly volume data⠴ [Compustat API] : Loading monthly volume data⠦ [Compustat API] : Loading monthly volume data⠧ [Compustat API] : Loading monthly volume data⠇ [Compustat API] : Loading monthly volume data⠏ [Compustat API] : Loading monthly volume data⠋ [Compustat API] : Loading monthly volume data⠙ [Compustat API] : Loading monthly volume data⠹ [Compustat API] : Loading monthly volume data⠸ [Compustat API] : Loading monthly volume data⠼ [Compustat API] : Loading monthly volume data⠴ [Compustat API] : Loading monthly volume data⠦ [Compustat API] : Loading monthly volume data⠧ [Compustat API] : Loading monthly volume data⠇ [Compustat API] : Loading monthly volume data⠏ [Compustat API] : Loading monthly volume data⠋ [Compustat API] : Loading monthly volume data⠙ [Compustat API] : Loading monthly volume data⠹ [Compustat API] : Loading monthly volume data⠸ [Compustat API] : Loading monthly volume data⠼ [Compustat API] : Loading monthly volume data⠴ [Compustat API] : Loading monthly volume data⠦ [Compustat API] : Loading monthly volume data⠧ [Compustat API] : Loading monthly volume data⠇ [Compustat API] : Loading monthly volume data⠏ [Compustat API] : Loading monthly volume data✓ [Compustat API] : Loading monthly volume data done in 12.895 secs
[strategy | _allocating | INFO]: [STRATEGY] allocating start
2015-12-31
2016-01-31
2016-02-29
2016-03-31
2016-04-30
2016-05-31
2016-06-30
2016-07-31
2016-08-31
2016-09-30
2016-10-31
2016-11-30
2016-12-31
2017-01-31
2017-02-28
2017-03-31
2017-04-30
2017-05-31
2017-06-30
2017-07-31
2017-08-31
2017-09-30
2017-10-31
2017-11-30
2017-12-31
2018-01-31
2018-02-28
2018-03-31
2018-04-30
2018-05-31
2018-06-30
2018-07-31
2018-08-31
2018-09-30
2018-10-31
2018-11-30
2018-12-31
2019-01-31
2019-02-28
2019-03-31
2019-04-30
2019-05-31
2019-06-30
2019-07-31
2019-08-31
2019-09-30
2019-10-31
2019-11-30
2019-12-31
2020-01-31
2020-02-29
2020-03-31
2020-04-30
2020-05-31
2020-06-30
2020-07-31
2020-08-31
2020-09-30
2020-10-31
2020-11-30
2020-12-31
2021-01-31
2021-02-28
2021-03-31
2021-04-30
2021-05-31
2021-06-30
2021-07-31
2021-08-31
2021-09-30
2021-10-31
⠋ [Compustat API] : Loading GICS⠙ [Compustat API] : Loading GICS⠹ [Compustat API] : Loading GICS⠸ [Compustat API] : Loading GICS⠼ [Compustat API] : Loading GICS✓ [Compustat API] : Loading GICS done in 1.394 secs
⠋ [Compustat API] : Loading GICS⠙ [Compustat API] : Loading GICS⠹ [Compustat API] : Loading GICS⠸ [Compustat API] : Loading GICS⠼ [Compustat API] : Loading GICS✓ [Compustat API] : Loading GICS done in 1.364 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.07 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.083 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.079 secs
⠋ [Compustat API] : Loading fundamental data ni / seq as roe⠙ [Compustat API] : Loading fundamental data ni / seq as roe⠹ [Compustat API] : Loading fundamental data ni / seq as roe⠸ [Compustat API] : Loading fundamental data ni / seq as roe⠼ [Compustat API] : Loading fundamental data ni / seq as roe⠴ [Compustat API] : Loading fundamental data ni / seq as roe⠦ [Compustat API] : Loading fundamental data ni / seq as roe⠧ [Compustat API] : Loading fundamental data ni / seq as roe⠇ [Compustat API] : Loading fundamental data ni / seq as roe⠏ [Compustat API] : Loading fundamental data ni / seq as roe⠋ [Compustat API] : Loading fundamental data ni / seq as roe⠙ [Compustat API] : Loading fundamental data ni / seq as roe⠹ [Compustat API] : Loading fundamental data ni / seq as roe⠸ [Compustat API] : Loading fundamental data ni / seq as roe⠼ [Compustat API] : Loading fundamental data ni / seq as roe⠴ [Compustat API] : Loading fundamental data ni / seq as roe✓ [Compustat API] : Loading fundamental data ni / seq as roe done in 4.987 secs
⠋ [Compustat API] : Loading fundamental data ni / at as roa⠙ [Compustat API] : Loading fundamental data ni / at as roa⠹ [Compustat API] : Loading fundamental data ni / at as roa⠸ [Compustat API] : Loading fundamental data ni / at as roa⠼ [Compustat API] : Loading fundamental data ni / at as roa⠴ [Compustat API] : Loading fundamental data ni / at as roa⠦ [Compustat API] : Loading fundamental data ni / at as roa⠧ [Compustat API] : Loading fundamental data ni / at as roa⠇ [Compustat API] : Loading fundamental data ni / at as roa⠏ [Compustat API] : Loading fundamental data ni / at as roa⠋ [Compustat API] : Loading fundamental data ni / at as roa⠙ [Compustat API] : Loading fundamental data ni / at as roa⠹ [Compustat API] : Loading fundamental data ni / at as roa⠸ [Compustat API] : Loading fundamental data ni / at as roa⠼ [Compustat API] : Loading fundamental data ni / at as roa⠴ [Compustat API] : Loading fundamental data ni / at as roa⠦ [Compustat API] : Loading fundamental data ni / at as roa⠧ [Compustat API] : Loading fundamental data ni / at as roa⠇ [Compustat API] : Loading fundamental data ni / at as roa✓ [Compustat API] : Loading fundamental data ni / at as roa done in 5.827 secs
⠋ [Compustat API] : Loading fundamental data sale-cogs as gp⠙ [Compustat API] : Loading fundamental data sale-cogs as gp⠹ [Compustat API] : Loading fundamental data sale-cogs as gp⠸ [Compustat API] : Loading fundamental data sale-cogs as gp⠼ [Compustat API] : Loading fundamental data sale-cogs as gp⠴ [Compustat API] : Loading fundamental data sale-cogs as gp⠦ [Compustat API] : Loading fundamental data sale-cogs as gp⠧ [Compustat API] : Loading fundamental data sale-cogs as gp⠇ [Compustat API] : Loading fundamental data sale-cogs as gp⠏ [Compustat API] : Loading fundamental data sale-cogs as gp⠋ [Compustat API] : Loading fundamental data sale-cogs as gp⠙ [Compustat API] : Loading fundamental data sale-cogs as gp⠹ [Compustat API] : Loading fundamental data sale-cogs as gp⠸ [Compustat API] : Loading fundamental data sale-cogs as gp⠼ [Compustat API] : Loading fundamental data sale-cogs as gp⠴ [Compustat API] : Loading fundamental data sale-cogs as gp⠦ [Compustat API] : Loading fundamental data sale-cogs as gp⠧ [Compustat API] : Loading fundamental data sale-cogs as gp⠇ [Compustat API] : Loading fundamental data sale-cogs as gp✓ [Compustat API] : Loading fundamental data sale-cogs as gp done in 5.829 secs
⠋ [Compustat API] : Loading fundamental data at as at⠙ [Compustat API] : Loading fundamental data at as at⠹ [Compustat API] : Loading fundamental data at as at⠸ [Compustat API] : Loading fundamental data at as at⠼ [Compustat API] : Loading fundamental data at as at⠴ [Compustat API] : Loading fundamental data at as at⠦ [Compustat API] : Loading fundamental data at as at⠧ [Compustat API] : Loading fundamental data at as at⠇ [Compustat API] : Loading fundamental data at as at⠏ [Compustat API] : Loading fundamental data at as at⠋ [Compustat API] : Loading fundamental data at as at⠙ [Compustat API] : Loading fundamental data at as at⠹ [Compustat API] : Loading fundamental data at as at⠸ [Compustat API] : Loading fundamental data at as at⠼ [Compustat API] : Loading fundamental data at as at✓ [Compustat API] : Loading fundamental data at as at done in 4.626 secs
⠋ [Compustat API] : Loading fundamental data ib as ib⠙ [Compustat API] : Loading fundamental data ib as ib⠹ [Compustat API] : Loading fundamental data ib as ib⠸ [Compustat API] : Loading fundamental data ib as ib⠼ [Compustat API] : Loading fundamental data ib as ib⠴ [Compustat API] : Loading fundamental data ib as ib⠦ [Compustat API] : Loading fundamental data ib as ib⠧ [Compustat API] : Loading fundamental data ib as ib⠇ [Compustat API] : Loading fundamental data ib as ib⠏ [Compustat API] : Loading fundamental data ib as ib⠋ [Compustat API] : Loading fundamental data ib as ib⠙ [Compustat API] : Loading fundamental data ib as ib⠹ [Compustat API] : Loading fundamental data ib as ib⠸ [Compustat API] : Loading fundamental data ib as ib⠼ [Compustat API] : Loading fundamental data ib as ib⠴ [Compustat API] : Loading fundamental data ib as ib✓ [Compustat API] : Loading fundamental data ib as ib done in 4.712 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.081 secs
⠋ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠙ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠹ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠸ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠼ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠴ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠦ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠧ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠇ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠏ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠋ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠙ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠹ [Compustat API] : Loading fundamental data pstkrv as pstkrv[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_weight_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] performance.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] combined.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] weight.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] universe.csv is saved.
[comparison | get_logger | INFO]: ====================comparison start====================
[comparison | compare | INFO]: [5. COMPARISON] comparison is saved.
[comparison | compare | INFO]:                        Harvest_Green_concept001_pick350_test02_price_return  ...  SPCOMP_total_win
2016-01-31_2021-04-30                                           0.222292     ...          0.765625

[1 rows x 27 columns]
⠸ [Compustat API] : Loading fundamental data pstkrv as pstkrv⠼ [Compustat API] : Loading fundamental data pstkrv as pstkrv✓ [Compustat API] : Loading fundamental data pstkrv as pstkrv done in 4.538 secs
⠋ [Compustat API] : Loading fundamental data pstkl as pstkl⠙ [Compustat API] : Loading fundamental data pstkl as pstkl⠹ [Compustat API] : Loading fundamental data pstkl as pstkl⠸ [Compustat API] : Loading fundamental data pstkl as pstkl⠼ [Compustat API] : Loading fundamental data pstkl as pstkl⠴ [Compustat API] : Loading fundamental data pstkl as pstkl⠦ [Compustat API] : Loading fundamental data pstkl as pstkl⠧ [Compustat API] : Loading fundamental data pstkl as pstkl⠇ [Compustat API] : Loading fundamental data pstkl as pstkl⠏ [Compustat API] : Loading fundamental data pstkl as pstkl⠋ [Compustat API] : Loading fundamental data pstkl as pstkl⠙ [Compustat API] : Loading fundamental data pstkl as pstkl⠹ [Compustat API] : Loading fundamental data pstkl as pstkl⠸ [Compustat API] : Loading fundamental data pstkl as pstkl⠼ [Compustat API] : Loading fundamental data pstkl as pstkl✓ [Compustat API] : Loading fundamental data pstkl as pstkl done in 4.49 secs
⠋ [Compustat API] : Loading fundamental data pstk as pstk⠙ [Compustat API] : Loading fundamental data pstk as pstk⠹ [Compustat API] : Loading fundamental data pstk as pstk⠸ [Compustat API] : Loading fundamental data pstk as pstk⠼ [Compustat API] : Loading fundamental data pstk as pstk⠴ [Compustat API] : Loading fundamental data pstk as pstk⠦ [Compustat API] : Loading fundamental data pstk as pstk⠧ [Compustat API] : Loading fundamental data pstk as pstk⠇ [Compustat API] : Loading fundamental data pstk as pstk⠏ [Compustat API] : Loading fundamental data pstk as pstk⠋ [Compustat API] : Loading fundamental data pstk as pstk⠙ [Compustat API] : Loading fundamental data pstk as pstk⠹ [Compustat API] : Loading fundamental data pstk as pstk⠸ [Compustat API] : Loading fundamental data pstk as pstk⠼ [Compustat API] : Loading fundamental data pstk as pstk✓ [Compustat API] : Loading fundamental data pstk as pstk done in 4.494 secs
⠋ [Compustat API] : Loading fundamental data seq as seq⠙ [Compustat API] : Loading fundamental data seq as seq⠹ [Compustat API] : Loading fundamental data seq as seq⠸ [Compustat API] : Loading fundamental data seq as seq⠼ [Compustat API] : Loading fundamental data seq as seq⠴ [Compustat API] : Loading fundamental data seq as seq⠦ [Compustat API] : Loading fundamental data seq as seq⠧ [Compustat API] : Loading fundamental data seq as seq⠇ [Compustat API] : Loading fundamental data seq as seq⠏ [Compustat API] : Loading fundamental data seq as seq⠋ [Compustat API] : Loading fundamental data seq as seq⠙ [Compustat API] : Loading fundamental data seq as seq⠹ [Compustat API] : Loading fundamental data seq as seq⠸ [Compustat API] : Loading fundamental data seq as seq⠼ [Compustat API] : Loading fundamental data seq as seq✓ [Compustat API] : Loading fundamental data seq as seq done in 4.561 secs
⠋ [Compustat API] : Loading fundamental data txditc as txditc⠙ [Compustat API] : Loading fundamental data txditc as txditc⠹ [Compustat API] : Loading fundamental data txditc as txditc⠸ [Compustat API] : Loading fundamental data txditc as txditc⠼ [Compustat API] : Loading fundamental data txditc as txditc⠴ [Compustat API] : Loading fundamental data txditc as txditc⠦ [Compustat API] : Loading fundamental data txditc as txditc⠧ [Compustat API] : Loading fundamental data txditc as txditc⠇ [Compustat API] : Loading fundamental data txditc as txditc⠏ [Compustat API] : Loading fundamental data txditc as txditc⠋ [Compustat API] : Loading fundamental data txditc as txditc⠙ [Compustat API] : Loading fundamental data txditc as txditc⠹ [Compustat API] : Loading fundamental data txditc as txditc⠸ [Compustat API] : Loading fundamental data txditc as txditc⠼ [Compustat API] : Loading fundamental data txditc as txditc✓ [Compustat API] : Loading fundamental data txditc as txditc done in 4.633 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.083 secs
⠋ [Compustat API] : Loading fundamental data ib+dp as cf⠙ [Compustat API] : Loading fundamental data ib+dp as cf⠹ [Compustat API] : Loading fundamental data ib+dp as cf⠸ [Compustat API] : Loading fundamental data ib+dp as cf⠼ [Compustat API] : Loading fundamental data ib+dp as cf⠴ [Compustat API] : Loading fundamental data ib+dp as cf⠦ [Compustat API] : Loading fundamental data ib+dp as cf⠧ [Compustat API] : Loading fundamental data ib+dp as cf⠇ [Compustat API] : Loading fundamental data ib+dp as cf⠏ [Compustat API] : Loading fundamental data ib+dp as cf⠋ [Compustat API] : Loading fundamental data ib+dp as cf⠙ [Compustat API] : Loading fundamental data ib+dp as cf⠹ [Compustat API] : Loading fundamental data ib+dp as cf⠸ [Compustat API] : Loading fundamental data ib+dp as cf⠼ [Compustat API] : Loading fundamental data ib+dp as cf⠴ [Compustat API] : Loading fundamental data ib+dp as cf✓ [Compustat API] : Loading fundamental data ib+dp as cf done in 4.789 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.08 secs
⠋ [Compustat API] : Cleaning past days cache✓ [Compustat API] : Cleaning past days cache done in 0.0 secs
auto caching이 활성화 되었습니다. /home/sronly/sr-storage/kirin_cache/kirin_api_cache/2.19.17_2021-11-24에 캐쉬를 저장합니다.
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.149 secs
⠋ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading masking data⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading masking data⠦ [Compustat API] : Loading masking data⠧ [Compustat API] : Loading masking data⠇ [Compustat API] : Loading masking data⠏ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading masking data⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading masking data⠦ [Compustat API] : Loading masking data⠧ [Compustat API] : Loading masking data⠇ [Compustat API] : Loading masking data⠏ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading masking data⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading masking data⠴ [Compustat API] : Loading masking data⠦ [Compustat API] : Loading masking data⠧ [Compustat API] : Loading masking data⠇ [Compustat API] : Loading masking data⠏ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 9.112 secs
setting tensorflow random seed failed
load_data: ae_m
load_data: ia_ta
load_data: rc_a
load_data: t1y_ff
load_data: t5y_ff
load_data: t10y_t2y
load_data: export_growth
load_data: import_growth
load_data: real_gig
load_data: pr_1m_0m
load_data: mv
load_data: btm
load_data: snp500_pr
load_data: wilshire500_pr
load_data: ted
load_data: retail_mfr
load_data: m1
load_data: m2
load_data: ret_noa
load_data: etm
load_data: ia_mv
load_data: res_mom_12m_1m_0m
load_data: res_vol_6m_3m_0m
load_data: at
load_data: op_lev
load_data: roe
load_data: std_u_e
load_data: aaa_t10y
load_data: baa_t10y
load_data: aaa_ff
load_data: real_pig
load_data: federal_tg
load_data: real_gdp
load_data: gpa
load_data: rev_surp
load_data: cash_at
load_data: ppi
load_data: trimmed_pce
load_data: unemploy
load_data: snp500_vol
load_data: mom_12m_1m
load_data: ram_12m_0m
load_data: vol_3m
load_data: t3m
load_data: t6m
load_data: t2y
load_data: t10y
load_data: aaa
load_data: baa
load_data: core_ppi
load_data: cpi
load_data: pce
load_data: corporate_tg
load_data: industrial_prod
load_data: home_pr
load_data: r_s
load_data: r_a
load_data: fred_ff
load_data: t3y
load_data: t5y
load_data: t7y
load_data: baa_ff
load_data: core_cpi
load_data: core_pce
load_data: wti
load_data: capa_util
load_data: snp500_pe
load_data: sector_values
load_data: mv
load_data: pr_1m_0m
load_data: mv
making sample. (inference: True, date:2015-12-31 00:00:00, date_number: 348)
making sample. (inference: False, date:2012-12-31 00:00:00, date_number: 312)
making sample. (inference: False, date:2013-01-31 00:00:00, date_number: 313)
making sample. (inference: False, date:2013-02-28 00:00:00, date_number: 314)
making sample. (inference: False, date:2013-03-31 00:00:00, date_number: 315)
making sample. (inference: False, date:2013-04-30 00:00:00, date_number: 316)
making sample. (inference: False, date:2013-05-31 00:00:00, date_number: 317)
making sample. (inference: False, date:2013-06-30 00:00:00, date_number: 318)
making sample. (inference: False, date:2013-07-31 00:00:00, date_number: 319)
making sample. (inference: False, date:2013-08-31 00:00:00, date_number: 320)
making sample. (inference: False, date:2013-09-30 00:00:00, date_number: 321)
making sample. (inference: False, date:2013-10-31 00:00:00, date_number: 322)
making sample. (inference: False, date:2013-11-30 00:00:00, date_number: 323)
making sample. (inference: False, date:2013-12-31 00:00:00, date_number: 324)
making sample. (inference: False, date:2014-01-31 00:00:00, date_number: 325)
making sample. (inference: False, date:2014-02-28 00:00:00, date_number: 326)
making sample. (inference: False, date:2014-03-31 00:00:00, date_number: 327)
making sample. (inference: False, date:2014-04-30 00:00:00, date_number: 328)
making sample. (inference: False, date:2014-05-31 00:00:00, date_number: 329)
making sample. (inference: False, date:2014-06-30 00:00:00, date_number: 330)
making sample. (inference: False, date:2014-07-31 00:00:00, date_number: 331)
making sample. (inference: False, date:2014-08-31 00:00:00, date_number: 332)
making sample. (inference: False, date:2014-09-30 00:00:00, date_number: 333)
making sample. (inference: False, date:2014-10-31 00:00:00, date_number: 334)
making sample. (inference: False, date:2014-11-30 00:00:00, date_number: 335)
making sample. (inference: False, date:2014-12-31 00:00:00, date_number: 336)
making sample. (inference: False, date:2015-01-31 00:00:00, date_number: 337)
making sample. (inference: False, date:2015-02-28 00:00:00, date_number: 338)
making sample. (inference: False, date:2015-03-31 00:00:00, date_number: 339)
making sample. (inference: False, date:2015-04-30 00:00:00, date_number: 340)
making sample. (inference: False, date:2015-05-31 00:00:00, date_number: 341)
making sample. (inference: False, date:2015-06-30 00:00:00, date_number: 342)
making sample. (inference: False, date:2015-07-31 00:00:00, date_number: 343)
making sample. (inference: False, date:2015-08-31 00:00:00, date_number: 344)
making sample. (inference: False, date:2015-09-30 00:00:00, date_number: 345)
making sample. (inference: False, date:2015-10-31 00:00:00, date_number: 346)
making sample. (inference: False, date:2015-11-30 00:00:00, date_number: 347)
[AttentionModel/2015-12-31 | get_logger | INFO]: ====================AttentionModel/2015-12-31 start====================
making sample. (inference: True, date:2016-01-31 00:00:00, date_number: 349)
making sample. (inference: False, date:2015-12-31 00:00:00, date_number: 348)
[AttentionModel/2016-01-31 | get_logger | INFO]: ====================AttentionModel/2016-01-31 start====================
[strategy_integration.py] set_seed(2015-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5860)training: 2/10 (0.5860)validation : 2/10 (0.5817)training: 3/10 (0.5817)validation : 3/10 (0.5800)training: 4/10 (0.5800)validation : 4/10 (0.5800)training: 5/10 (0.5800)validation : 5/10 (0.5800)training: 6/10 (0.5800)validation : 6/10 (0.5800)early stopping at 6 with loss 0.5800
AttentionModel-training is done: 6/10
2015-12-31 | reset count: 0 | final loss: 0.5800 at epoch 3
[strategy_integration.py] set_seed(2016-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5952)training: 2/10 (0.5952)validation : 2/10 (0.5856)training: 3/10 (0.5856)validation : 3/10 (0.5856)training: 4/10 (0.5856)validation : 4/10 (0.5856)training: 5/10 (0.5856)validation : 5/10 (0.5856)early stopping at 5 with loss 0.5856
AttentionModel-training is done: 5/10
2016-01-31 | reset count: 0 | final loss: 0.5856 at epoch 2
making sample. (inference: True, date:2016-02-29 00:00:00, date_number: 350)
making sample. (inference: False, date:2016-01-31 00:00:00, date_number: 349)
[AttentionModel/2016-02-29 | get_logger | INFO]: ====================AttentionModel/2016-02-29 start====================
making sample. (inference: True, date:2016-03-31 00:00:00, date_number: 351)
making sample. (inference: False, date:2016-02-29 00:00:00, date_number: 350)
[AttentionModel/2016-03-31 | get_logger | INFO]: ====================AttentionModel/2016-03-31 start====================
[strategy_integration.py] set_seed(2016-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5778)training: 2/10 (0.5778)validation : 2/10 (0.5762)training: 3/10 (0.5762)validation : 3/10 (0.5762)training: 4/10 (0.5762)validation : 4/10 (0.5762)training: 5/10 (0.5762)validation : 5/10 (0.5762)early stopping at 5 with loss 0.5762
AttentionModel-training is done: 5/10
2016-02-29 | reset count: 0 | final loss: 0.5762 at epoch 2
[strategy_integration.py] set_seed(2016-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5746)training: 2/10 (0.5746)validation : 2/10 (0.5710)training: 3/10 (0.5710)validation : 3/10 (0.5710)training: 4/10 (0.5710)validation : 4/10 (0.5710)training: 5/10 (0.5710)validation : 5/10 (0.5710)early stopping at 5 with loss 0.5710
AttentionModel-training is done: 5/10
2016-03-31 | reset count: 0 | final loss: 0.5710 at epoch 2
making sample. (inference: True, date:2016-04-30 00:00:00, date_number: 352)
making sample. (inference: False, date:2016-03-31 00:00:00, date_number: 351)
[AttentionModel/2016-04-30 | get_logger | INFO]: ====================AttentionModel/2016-04-30 start====================
making sample. (inference: True, date:2016-05-31 00:00:00, date_number: 353)
making sample. (inference: False, date:2016-04-30 00:00:00, date_number: 352)
[AttentionModel/2016-05-31 | get_logger | INFO]: ====================AttentionModel/2016-05-31 start====================
[strategy_integration.py] set_seed(2016-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5793)training: 2/10 (0.5793)validation : 2/10 (0.5793)training: 3/10 (0.5793)validation : 3/10 (0.5786)training: 4/10 (0.5786)validation : 4/10 (0.5786)training: 5/10 (0.5786)validation : 5/10 (0.5785)training: 6/10 (0.5785)validation : 6/10 (0.5785)early stopping at 6 with loss 0.5785
AttentionModel-training is done: 6/10
2016-04-30 | reset count: 0 | final loss: 0.5785 at epoch 5
making sample. (inference: True, date:2016-06-30 00:00:00, date_number: 354)
making sample. (inference: False, date:2016-05-31 00:00:00, date_number: 353)
[AttentionModel/2016-06-30 | get_logger | INFO]: ====================AttentionModel/2016-06-30 start====================
[strategy_integration.py] set_seed(2016-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5823)training: 2/10 (0.5823)validation : 2/10 (0.5805)training: 3/10 (0.5805)validation : 3/10 (0.5798)training: 4/10 (0.5798)validation : 4/10 (0.5768)training: 5/10 (0.5768)validation : 5/10 (0.5768)training: 6/10 (0.5768)validation : 6/10 (0.5768)training: 7/10 (0.5768)validation : 7/10 (0.5768)early stopping at 7 with loss 0.5768
AttentionModel-training is done: 7/10
2016-05-31 | reset count: 0 | final loss: 0.5768 at epoch 4
making sample. (inference: True, date:2016-07-31 00:00:00, date_number: 355)
making sample. (inference: False, date:2016-06-30 00:00:00, date_number: 354)
[AttentionModel/2016-07-31 | get_logger | INFO]: ====================AttentionModel/2016-07-31 start====================
[strategy_integration.py] set_seed(2016-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5789)training: 2/10 (0.5789)validation : 2/10 (0.5776)training: 3/10 (0.5776)validation : 3/10 (0.5776)training: 4/10 (0.5776)validation : 4/10 (0.5769)training: 5/10 (0.5769)validation : 5/10 (0.5769)early stopping at 5 with loss 0.5769
AttentionModel-training is done: 5/10
2016-06-30 | reset count: 0 | final loss: 0.5769 at epoch 4
[strategy_integration.py] set_seed(2016-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5776)training: 2/10 (0.5776)validation : 2/10 (0.5749)training: 3/10 (0.5749)validation : 3/10 (0.5749)training: 4/10 (0.5749)validation : 4/10 (0.5749)training: 5/10 (0.5749)validation : 5/10 (0.5749)early stopping at 5 with loss 0.5749
AttentionModel-training is done: 5/10
2016-07-31 | reset count: 0 | final loss: 0.5749 at epoch 2
making sample. (inference: True, date:2016-08-31 00:00:00, date_number: 356)
making sample. (inference: False, date:2016-07-31 00:00:00, date_number: 355)
[AttentionModel/2016-08-31 | get_logger | INFO]: ====================AttentionModel/2016-08-31 start====================
making sample. (inference: True, date:2016-09-30 00:00:00, date_number: 357)
making sample. (inference: False, date:2016-08-31 00:00:00, date_number: 356)
[AttentionModel/2016-09-30 | get_logger | INFO]: ====================AttentionModel/2016-09-30 start====================
[strategy_integration.py] set_seed(2016-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5749)training: 2/10 (0.5749)validation : 2/10 (0.5749)training: 3/10 (0.5749)validation : 3/10 (0.5749)training: 4/10 (0.5749)validation : 4/10 (0.5749)training: 5/10 (0.5749)validation : 5/10 (0.5749)training: 6/10 (0.5749)validation : 6/10 (0.5749)training: 7/10 (0.5749)validation : 7/10 (0.5732)training: 8/10 (0.5732)validation : 8/10 (0.5716)training: 9/10 (0.5716)validation : 9/10 (0.5716)training: 10/10 (0.5716)validation : 10/10 (0.5716)early stopping at 10 with loss 0.5716
AttentionModel-training is done: 10/10
2016-08-31 | reset count: 0 | final loss: 0.5716 at epoch 8
[strategy_integration.py] set_seed(2016-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5800)training: 2/10 (0.5800)validation : 2/10 (0.5800)training: 3/10 (0.5800)validation : 3/10 (0.5790)training: 4/10 (0.5790)validation : 4/10 (0.5768)training: 5/10 (0.5768)validation : 5/10 (0.5735)training: 6/10 (0.5735)validation : 6/10 (0.5735)training: 7/10 (0.5735)validation : 7/10 (0.5735)early stopping at 7 with loss 0.5735
AttentionModel-training is done: 7/10
2016-09-30 | reset count: 0 | final loss: 0.5735 at epoch 5
making sample. (inference: True, date:2016-10-31 00:00:00, date_number: 358)
making sample. (inference: False, date:2016-09-30 00:00:00, date_number: 357)
[AttentionModel/2016-10-31 | get_logger | INFO]: ====================AttentionModel/2016-10-31 start====================
making sample. (inference: True, date:2016-11-30 00:00:00, date_number: 359)
making sample. (inference: False, date:2016-10-31 00:00:00, date_number: 358)
[AttentionModel/2016-11-30 | get_logger | INFO]: ====================AttentionModel/2016-11-30 start====================
[strategy_integration.py] set_seed(2016-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5691)training: 2/10 (0.5691)validation : 2/10 (0.5629)training: 3/10 (0.5629)validation : 3/10 (0.5629)training: 4/10 (0.5629)validation : 4/10 (0.5629)training: 5/10 (0.5629)validation : 5/10 (0.5629)training: 6/10 (0.5629)validation : 6/10 (0.5629)training: 7/10 (0.5629)validation : 7/10 (0.5629)early stopping at 7 with loss 0.5629
AttentionModel-training is done: 7/10
2016-10-31 | reset count: 0 | final loss: 0.5629 at epoch 2
making sample. (inference: True, date:2016-12-31 00:00:00, date_number: 360)
making sample. (inference: False, date:2016-11-30 00:00:00, date_number: 359)
[AttentionModel/2016-12-31 | get_logger | INFO]: ====================AttentionModel/2016-12-31 start====================
[strategy_integration.py] set_seed(2016-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5665)training: 2/10 (0.5665)validation : 2/10 (0.5665)training: 3/10 (0.5665)validation : 3/10 (0.5664)training: 4/10 (0.5664)validation : 4/10 (0.5664)training: 5/10 (0.5664)validation : 5/10 (0.5664)training: 6/10 (0.5664)validation : 6/10 (0.5635)training: 7/10 (0.5635)validation : 7/10 (0.5635)training: 8/10 (0.5635)validation : 8/10 (0.5606)training: 9/10 (0.5606)validation : 9/10 (0.5606)training: 10/10 (0.5606)validation : 10/10 (0.5606)AttentionModel-training is done: 10/10
2016-11-30 | reset count: 0 | final loss: 0.5606 at epoch 8
making sample. (inference: True, date:2017-01-31 00:00:00, date_number: 361)
making sample. (inference: False, date:2016-12-31 00:00:00, date_number: 360)
[AttentionModel/2017-01-31 | get_logger | INFO]: ====================AttentionModel/2017-01-31 start====================
[strategy_integration.py] set_seed(2016-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5763)training: 2/10 (0.5763)validation : 2/10 (0.5712)training: 3/10 (0.5712)validation : 3/10 (0.5712)training: 4/10 (0.5712)validation : 4/10 (0.5712)training: 5/10 (0.5712)validation : 5/10 (0.5712)training: 6/10 (0.5712)validation : 6/10 (0.5673)training: 7/10 (0.5673)validation : 7/10 (0.5673)training: 8/10 (0.5673)validation : 8/10 (0.5673)training: 9/10 (0.5673)validation : 9/10 (0.5673)early stopping at 9 with loss 0.5673
AttentionModel-training is done: 9/10
2016-12-31 | reset count: 0 | final loss: 0.5673 at epoch 6
making sample. (inference: True, date:2017-02-28 00:00:00, date_number: 362)
making sample. (inference: False, date:2017-01-31 00:00:00, date_number: 361)
[AttentionModel/2017-02-28 | get_logger | INFO]: ====================AttentionModel/2017-02-28 start====================
[strategy_integration.py] set_seed(2017-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5685)training: 2/10 (0.5685)validation : 2/10 (0.5683)training: 3/10 (0.5683)validation : 3/10 (0.5634)training: 4/10 (0.5634)validation : 4/10 (0.5634)training: 5/10 (0.5634)validation : 5/10 (0.5634)training: 6/10 (0.5634)validation : 6/10 (0.5634)early stopping at 6 with loss 0.5634
AttentionModel-training is done: 6/10
2017-01-31 | reset count: 0 | final loss: 0.5634 at epoch 3
making sample. (inference: True, date:2017-03-31 00:00:00, date_number: 363)
making sample. (inference: False, date:2017-02-28 00:00:00, date_number: 362)
[AttentionModel/2017-03-31 | get_logger | INFO]: ====================AttentionModel/2017-03-31 start====================
[strategy_integration.py] set_seed(2017-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5675)training: 2/10 (0.5675)validation : 2/10 (0.5636)training: 3/10 (0.5636)validation : 3/10 (0.5636)training: 4/10 (0.5636)validation : 4/10 (0.5615)training: 5/10 (0.5615)validation : 5/10 (0.5615)training: 6/10 (0.5615)validation : 6/10 (0.5615)training: 7/10 (0.5615)validation : 7/10 (0.5615)early stopping at 7 with loss 0.5615
AttentionModel-training is done: 7/10
2017-02-28 | reset count: 0 | final loss: 0.5615 at epoch 4
[strategy_integration.py] set_seed(2017-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5633)training: 2/10 (0.5633)validation : 2/10 (0.5633)training: 3/10 (0.5633)validation : 3/10 (0.5583)training: 4/10 (0.5583)validation : 4/10 (0.5583)training: 5/10 (0.5583)validation : 5/10 (0.5579)training: 6/10 (0.5579)validation : 6/10 (0.5579)early stopping at 6 with loss 0.5579
AttentionModel-training is done: 6/10
2017-03-31 | reset count: 0 | final loss: 0.5579 at epoch 5
making sample. (inference: True, date:2017-04-30 00:00:00, date_number: 364)
making sample. (inference: False, date:2017-03-31 00:00:00, date_number: 363)
[AttentionModel/2017-04-30 | get_logger | INFO]: ====================AttentionModel/2017-04-30 start====================
making sample. (inference: True, date:2017-05-31 00:00:00, date_number: 365)
making sample. (inference: False, date:2017-04-30 00:00:00, date_number: 364)
[AttentionModel/2017-05-31 | get_logger | INFO]: ====================AttentionModel/2017-05-31 start====================
[strategy_integration.py] set_seed(2017-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5757)training: 2/10 (0.5757)validation : 2/10 (0.5724)training: 3/10 (0.5724)validation : 3/10 (0.5688)training: 4/10 (0.5688)validation : 4/10 (0.5688)training: 5/10 (0.5688)validation : 5/10 (0.5688)training: 6/10 (0.5688)validation : 6/10 (0.5688)early stopping at 6 with loss 0.5688
AttentionModel-training is done: 6/10
2017-04-30 | reset count: 0 | final loss: 0.5688 at epoch 3
making sample. (inference: True, date:2017-06-30 00:00:00, date_number: 366)
making sample. (inference: False, date:2017-05-31 00:00:00, date_number: 365)
[AttentionModel/2017-06-30 | get_logger | INFO]: ====================AttentionModel/2017-06-30 start====================
[strategy_integration.py] set_seed(2017-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5658)training: 2/10 (0.5658)validation : 2/10 (0.5622)training: 3/10 (0.5622)validation : 3/10 (0.5622)training: 4/10 (0.5622)validation : 4/10 (0.5622)training: 5/10 (0.5622)validation : 5/10 (0.5588)training: 6/10 (0.5588)validation : 6/10 (0.5588)training: 7/10 (0.5588)validation : 7/10 (0.5588)training: 8/10 (0.5588)validation : 8/10 (0.5588)early stopping at 8 with loss 0.5588
AttentionModel-training is done: 8/10
2017-05-31 | reset count: 0 | final loss: 0.5588 at epoch 5
making sample. (inference: True, date:2017-07-31 00:00:00, date_number: 367)
making sample. (inference: False, date:2017-06-30 00:00:00, date_number: 366)
[AttentionModel/2017-07-31 | get_logger | INFO]: ====================AttentionModel/2017-07-31 start====================
[strategy_integration.py] set_seed(2017-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5684)training: 2/10 (0.5684)validation : 2/10 (0.5639)training: 3/10 (0.5639)validation : 3/10 (0.5639)training: 4/10 (0.5639)validation : 4/10 (0.5634)training: 5/10 (0.5634)validation : 5/10 (0.5611)training: 6/10 (0.5611)validation : 6/10 (0.5611)training: 7/10 (0.5611)validation : 7/10 (0.5611)early stopping at 7 with loss 0.5611
AttentionModel-training is done: 7/10
2017-06-30 | reset count: 0 | final loss: 0.5611 at epoch 5
making sample. (inference: True, date:2017-08-31 00:00:00, date_number: 368)
making sample. (inference: False, date:2017-07-31 00:00:00, date_number: 367)
[AttentionModel/2017-08-31 | get_logger | INFO]: ====================AttentionModel/2017-08-31 start====================
[strategy_integration.py] set_seed(2017-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5757)training: 2/10 (0.5757)validation : 2/10 (0.5719)training: 3/10 (0.5719)validation : 3/10 (0.5682)training: 4/10 (0.5682)validation : 4/10 (0.5682)training: 5/10 (0.5682)validation : 5/10 (0.5682)training: 6/10 (0.5682)validation : 6/10 (0.5682)early stopping at 6 with loss 0.5682
AttentionModel-training is done: 6/10
2017-07-31 | reset count: 0 | final loss: 0.5682 at epoch 3
making sample. (inference: True, date:2017-09-30 00:00:00, date_number: 369)
making sample. (inference: False, date:2017-08-31 00:00:00, date_number: 368)
[AttentionModel/2017-09-30 | get_logger | INFO]: ====================AttentionModel/2017-09-30 start====================
[strategy_integration.py] set_seed(2017-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5732)training: 2/10 (0.5732)validation : 2/10 (0.5732)training: 3/10 (0.5732)validation : 3/10 (0.5672)training: 4/10 (0.5672)validation : 4/10 (0.5672)training: 5/10 (0.5672)validation : 5/10 (0.5672)training: 6/10 (0.5672)validation : 6/10 (0.5672)early stopping at 6 with loss 0.5672
AttentionModel-training is done: 6/10
2017-08-31 | reset count: 0 | final loss: 0.5672 at epoch 3
making sample. (inference: True, date:2017-10-31 00:00:00, date_number: 370)
making sample. (inference: False, date:2017-09-30 00:00:00, date_number: 369)
[AttentionModel/2017-10-31 | get_logger | INFO]: ====================AttentionModel/2017-10-31 start====================
[strategy_integration.py] set_seed(2017-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5709)training: 2/10 (0.5709)validation : 2/10 (0.5656)training: 3/10 (0.5656)validation : 3/10 (0.5629)training: 4/10 (0.5629)validation : 4/10 (0.5629)training: 5/10 (0.5629)validation : 5/10 (0.5629)training: 6/10 (0.5629)validation : 6/10 (0.5629)early stopping at 6 with loss 0.5629
AttentionModel-training is done: 6/10
2017-09-30 | reset count: 0 | final loss: 0.5629 at epoch 3
making sample. (inference: True, date:2017-11-30 00:00:00, date_number: 371)
making sample. (inference: False, date:2017-10-31 00:00:00, date_number: 370)
[AttentionModel/2017-11-30 | get_logger | INFO]: ====================AttentionModel/2017-11-30 start====================
[strategy_integration.py] set_seed(2017-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5694)training: 2/10 (0.5694)validation : 2/10 (0.5680)training: 3/10 (0.5680)validation : 3/10 (0.5675)training: 4/10 (0.5675)validation : 4/10 (0.5639)training: 5/10 (0.5639)validation : 5/10 (0.5639)training: 6/10 (0.5639)validation : 6/10 (0.5639)early stopping at 6 with loss 0.5639
AttentionModel-training is done: 6/10
2017-10-31 | reset count: 0 | final loss: 0.5639 at epoch 4
making sample. (inference: True, date:2017-12-31 00:00:00, date_number: 372)
making sample. (inference: False, date:2017-11-30 00:00:00, date_number: 371)
[AttentionModel/2017-12-31 | get_logger | INFO]: ====================AttentionModel/2017-12-31 start====================
[strategy_integration.py] set_seed(2017-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5626)training: 2/10 (0.5626)validation : 2/10 (0.5626)training: 3/10 (0.5626)validation : 3/10 (0.5626)training: 4/10 (0.5626)validation : 4/10 (0.5626)training: 5/10 (0.5626)validation : 5/10 (0.5625)training: 6/10 (0.5625)validation : 6/10 (0.5618)training: 7/10 (0.5618)validation : 7/10 (0.5618)early stopping at 7 with loss 0.5618
AttentionModel-training is done: 7/10
2017-11-30 | reset count: 0 | final loss: 0.5618 at epoch 6
making sample. (inference: True, date:2018-01-31 00:00:00, date_number: 373)
making sample. (inference: False, date:2017-12-31 00:00:00, date_number: 372)
[AttentionModel/2018-01-31 | get_logger | INFO]: ====================AttentionModel/2018-01-31 start====================
[strategy_integration.py] set_seed(2017-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5724)training: 2/10 (0.5724)validation : 2/10 (0.5700)training: 3/10 (0.5700)validation : 3/10 (0.5686)training: 4/10 (0.5686)validation : 4/10 (0.5669)training: 5/10 (0.5669)validation : 5/10 (0.5669)training: 6/10 (0.5669)validation : 6/10 (0.5658)training: 7/10 (0.5658)validation : 7/10 (0.5658)early stopping at 7 with loss 0.5658
AttentionModel-training is done: 7/10
2017-12-31 | reset count: 0 | final loss: 0.5658 at epoch 6
making sample. (inference: True, date:2018-02-28 00:00:00, date_number: 374)
making sample. (inference: False, date:2018-01-31 00:00:00, date_number: 373)
[AttentionModel/2018-02-28 | get_logger | INFO]: ====================AttentionModel/2018-02-28 start====================
[strategy_integration.py] set_seed(2018-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5775)training: 2/10 (0.5775)validation : 2/10 (0.5729)training: 3/10 (0.5729)validation : 3/10 (0.5729)training: 4/10 (0.5729)validation : 4/10 (0.5705)training: 5/10 (0.5705)validation : 5/10 (0.5705)training: 6/10 (0.5705)validation : 6/10 (0.5705)training: 7/10 (0.5705)validation : 7/10 (0.5705)early stopping at 7 with loss 0.5705
AttentionModel-training is done: 7/10
2018-01-31 | reset count: 0 | final loss: 0.5705 at epoch 4
making sample. (inference: True, date:2018-03-31 00:00:00, date_number: 375)
making sample. (inference: False, date:2018-02-28 00:00:00, date_number: 374)
[AttentionModel/2018-03-31 | get_logger | INFO]: ====================AttentionModel/2018-03-31 start====================
[strategy_integration.py] set_seed(2018-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5729)training: 2/10 (0.5729)validation : 2/10 (0.5693)training: 3/10 (0.5693)validation : 3/10 (0.5681)training: 4/10 (0.5681)validation : 4/10 (0.5678)training: 5/10 (0.5678)validation : 5/10 (0.5660)training: 6/10 (0.5660)validation : 6/10 (0.5660)early stopping at 6 with loss 0.5660
AttentionModel-training is done: 6/10
2018-02-28 | reset count: 0 | final loss: 0.5660 at epoch 5
[strategy_integration.py] set_seed(2018-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5694)training: 2/10 (0.5694)validation : 2/10 (0.5676)training: 3/10 (0.5676)validation : 3/10 (0.5620)training: 4/10 (0.5620)validation : 4/10 (0.5620)training: 5/10 (0.5620)validation : 5/10 (0.5620)early stopping at 5 with loss 0.5620
AttentionModel-training is done: 5/10
2018-03-31 | reset count: 0 | final loss: 0.5620 at epoch 3
making sample. (inference: True, date:2018-04-30 00:00:00, date_number: 376)
making sample. (inference: False, date:2018-03-31 00:00:00, date_number: 375)
[AttentionModel/2018-04-30 | get_logger | INFO]: ====================AttentionModel/2018-04-30 start====================
making sample. (inference: True, date:2018-05-31 00:00:00, date_number: 377)
making sample. (inference: False, date:2018-04-30 00:00:00, date_number: 376)
[AttentionModel/2018-05-31 | get_logger | INFO]: ====================AttentionModel/2018-05-31 start====================
[strategy_integration.py] set_seed(2018-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5667)training: 2/10 (0.5667)validation : 2/10 (0.5667)training: 3/10 (0.5667)validation : 3/10 (0.5667)training: 4/10 (0.5667)validation : 4/10 (0.5667)training: 5/10 (0.5667)validation : 5/10 (0.5667)early stopping at 5 with loss 0.5667
AttentionModel-training is done: 5/10
2018-04-30 | reset count: 0 | final loss: 0.5667 at epoch 1
[strategy_integration.py] set_seed(2018-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5755)training: 2/10 (0.5755)validation : 2/10 (0.5724)training: 3/10 (0.5724)validation : 3/10 (0.5724)training: 4/10 (0.5724)validation : 4/10 (0.5719)training: 5/10 (0.5719)validation : 5/10 (0.5719)training: 6/10 (0.5719)validation : 6/10 (0.5719)early stopping at 6 with loss 0.5719
AttentionModel-training is done: 6/10
2018-05-31 | reset count: 0 | final loss: 0.5719 at epoch 4
making sample. (inference: True, date:2018-06-30 00:00:00, date_number: 378)
making sample. (inference: False, date:2018-05-31 00:00:00, date_number: 377)
[AttentionModel/2018-06-30 | get_logger | INFO]: ====================AttentionModel/2018-06-30 start====================
making sample. (inference: True, date:2018-07-31 00:00:00, date_number: 379)
making sample. (inference: False, date:2018-06-30 00:00:00, date_number: 378)
[AttentionModel/2018-07-31 | get_logger | INFO]: ====================AttentionModel/2018-07-31 start====================
[strategy_integration.py] set_seed(2018-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5739)training: 2/10 (0.5739)validation : 2/10 (0.5687)training: 3/10 (0.5687)validation : 3/10 (0.5687)training: 4/10 (0.5687)validation : 4/10 (0.5687)training: 5/10 (0.5687)validation : 5/10 (0.5687)early stopping at 5 with loss 0.5687
AttentionModel-training is done: 5/10
2018-07-31 | reset count: 0 | final loss: 0.5687 at epoch 2
[strategy_integration.py] set_seed(2018-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5728)training: 2/10 (0.5728)validation : 2/10 (0.5726)training: 3/10 (0.5726)validation : 3/10 (0.5696)training: 4/10 (0.5696)validation : 4/10 (0.5695)training: 5/10 (0.5695)validation : 5/10 (0.5695)training: 6/10 (0.5695)validation : 6/10 (0.5672)training: 7/10 (0.5672)validation : 7/10 (0.5671)training: 8/10 (0.5671)validation : 8/10 (0.5636)training: 9/10 (0.5636)validation : 9/10 (0.5636)training: 10/10 (0.5636)validation : 10/10 (0.5636)early stopping at 10 with loss 0.5636
AttentionModel-training is done: 10/10
2018-06-30 | reset count: 0 | final loss: 0.5636 at epoch 8
making sample. (inference: True, date:2018-08-31 00:00:00, date_number: 380)
making sample. (inference: False, date:2018-07-31 00:00:00, date_number: 379)
[AttentionModel/2018-08-31 | get_logger | INFO]: ====================AttentionModel/2018-08-31 start====================
making sample. (inference: True, date:2018-09-30 00:00:00, date_number: 381)
making sample. (inference: False, date:2018-08-31 00:00:00, date_number: 380)
[AttentionModel/2018-09-30 | get_logger | INFO]: ====================AttentionModel/2018-09-30 start====================
[strategy_integration.py] set_seed(2018-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5725)training: 2/10 (0.5725)validation : 2/10 (0.5725)training: 3/10 (0.5725)validation : 3/10 (0.5725)training: 4/10 (0.5725)validation : 4/10 (0.5707)training: 5/10 (0.5707)validation : 5/10 (0.5707)training: 6/10 (0.5707)validation : 6/10 (0.5707)early stopping at 6 with loss 0.5707
AttentionModel-training is done: 6/10
2018-09-30 | reset count: 0 | final loss: 0.5707 at epoch 4
[strategy_integration.py] set_seed(2018-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5733)training: 2/10 (0.5733)validation : 2/10 (0.5733)training: 3/10 (0.5733)validation : 3/10 (0.5733)training: 4/10 (0.5733)validation : 4/10 (0.5698)training: 5/10 (0.5698)validation : 5/10 (0.5698)training: 6/10 (0.5698)validation : 6/10 (0.5698)training: 7/10 (0.5698)validation : 7/10 (0.5695)training: 8/10 (0.5695)validation : 8/10 (0.5693)training: 9/10 (0.5693)validation : 9/10 (0.5693)training: 10/10 (0.5693)validation : 10/10 (0.5693)early stopping at 10 with loss 0.5693
AttentionModel-training is done: 10/10
2018-08-31 | reset count: 0 | final loss: 0.5693 at epoch 8
making sample. (inference: True, date:2018-10-31 00:00:00, date_number: 382)
making sample. (inference: False, date:2018-09-30 00:00:00, date_number: 381)
[AttentionModel/2018-10-31 | get_logger | INFO]: ====================AttentionModel/2018-10-31 start====================
making sample. (inference: True, date:2018-11-30 00:00:00, date_number: 383)
making sample. (inference: False, date:2018-10-31 00:00:00, date_number: 382)
[AttentionModel/2018-11-30 | get_logger | INFO]: ====================AttentionModel/2018-11-30 start====================
[strategy_integration.py] set_seed(2018-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5828)training: 2/10 (0.5828)validation : 2/10 (0.5826)training: 3/10 (0.5826)validation : 3/10 (0.5795)training: 4/10 (0.5795)validation : 4/10 (0.5795)training: 5/10 (0.5795)validation : 5/10 (0.5791)training: 6/10 (0.5791)validation : 6/10 (0.5791)early stopping at 6 with loss 0.5791
AttentionModel-training is done: 6/10
2018-10-31 | reset count: 0 | final loss: 0.5791 at epoch 5
making sample. (inference: True, date:2018-12-31 00:00:00, date_number: 384)
making sample. (inference: False, date:2018-11-30 00:00:00, date_number: 383)
[AttentionModel/2018-12-31 | get_logger | INFO]: ====================AttentionModel/2018-12-31 start====================
[strategy_integration.py] set_seed(2018-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5841)training: 2/10 (0.5841)validation : 2/10 (0.5827)training: 3/10 (0.5827)validation : 3/10 (0.5825)training: 4/10 (0.5825)validation : 4/10 (0.5825)training: 5/10 (0.5825)validation : 5/10 (0.5823)training: 6/10 (0.5823)validation : 6/10 (0.5805)training: 7/10 (0.5805)validation : 7/10 (0.5805)training: 8/10 (0.5805)validation : 8/10 (0.5805)early stopping at 8 with loss 0.5805
AttentionModel-training is done: 8/10
2018-11-30 | reset count: 0 | final loss: 0.5805 at epoch 6
making sample. (inference: True, date:2019-01-31 00:00:00, date_number: 385)
making sample. (inference: False, date:2018-12-31 00:00:00, date_number: 384)
[AttentionModel/2019-01-31 | get_logger | INFO]: ====================AttentionModel/2019-01-31 start====================
[strategy_integration.py] set_seed(2018-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5842)training: 2/10 (0.5842)validation : 2/10 (0.5750)training: 3/10 (0.5750)validation : 3/10 (0.5738)training: 4/10 (0.5738)validation : 4/10 (0.5738)training: 5/10 (0.5738)validation : 5/10 (0.5738)training: 6/10 (0.5738)validation : 6/10 (0.5738)early stopping at 6 with loss 0.5738
AttentionModel-training is done: 6/10
2018-12-31 | reset count: 0 | final loss: 0.5738 at epoch 3
making sample. (inference: True, date:2019-02-28 00:00:00, date_number: 386)
making sample. (inference: False, date:2019-01-31 00:00:00, date_number: 385)
[AttentionModel/2019-02-28 | get_logger | INFO]: ====================AttentionModel/2019-02-28 start====================
[strategy_integration.py] set_seed(2019-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5870)training: 2/10 (0.5870)validation : 2/10 (0.5870)training: 3/10 (0.5870)validation : 3/10 (0.5870)training: 4/10 (0.5870)validation : 4/10 (0.5870)training: 5/10 (0.5870)validation : 5/10 (0.5856)training: 6/10 (0.5856)validation : 6/10 (0.5843)training: 7/10 (0.5843)validation : 7/10 (0.5843)training: 8/10 (0.5843)validation : 8/10 (0.5843)early stopping at 8 with loss 0.5843
AttentionModel-training is done: 8/10
2019-01-31 | reset count: 0 | final loss: 0.5843 at epoch 6
[strategy_integration.py] set_seed(2019-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5828)training: 2/10 (0.5828)validation : 2/10 (0.5828)training: 3/10 (0.5828)validation : 3/10 (0.5828)training: 4/10 (0.5828)validation : 4/10 (0.5828)training: 5/10 (0.5828)validation : 5/10 (0.5828)early stopping at 5 with loss 0.5828
AttentionModel-training is done: 5/10
2019-02-28 | reset count: 0 | final loss: 0.5828 at epoch 1
making sample. (inference: True, date:2019-03-31 00:00:00, date_number: 387)
making sample. (inference: False, date:2019-02-28 00:00:00, date_number: 386)
[AttentionModel/2019-03-31 | get_logger | INFO]: ====================AttentionModel/2019-03-31 start====================
making sample. (inference: True, date:2019-04-30 00:00:00, date_number: 388)
making sample. (inference: False, date:2019-03-31 00:00:00, date_number: 387)
[AttentionModel/2019-04-30 | get_logger | INFO]: ====================AttentionModel/2019-04-30 start====================
[strategy_integration.py] set_seed(2019-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5899)training: 2/10 (0.5899)validation : 2/10 (0.5852)training: 3/10 (0.5852)validation : 3/10 (0.5852)training: 4/10 (0.5852)validation : 4/10 (0.5852)training: 5/10 (0.5852)validation : 5/10 (0.5852)training: 6/10 (0.5852)validation : 6/10 (0.5852)early stopping at 6 with loss 0.5852
AttentionModel-training is done: 6/10
2019-03-31 | reset count: 0 | final loss: 0.5852 at epoch 2
[strategy_integration.py] set_seed(2019-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5808)training: 2/10 (0.5808)validation : 2/10 (0.5793)training: 3/10 (0.5793)validation : 3/10 (0.5793)training: 4/10 (0.5793)validation : 4/10 (0.5793)training: 5/10 (0.5793)validation : 5/10 (0.5793)early stopping at 5 with loss 0.5793
AttentionModel-training is done: 5/10
2019-04-30 | reset count: 0 | final loss: 0.5793 at epoch 2
making sample. (inference: True, date:2019-05-31 00:00:00, date_number: 389)
making sample. (inference: False, date:2019-04-30 00:00:00, date_number: 388)
[AttentionModel/2019-05-31 | get_logger | INFO]: ====================AttentionModel/2019-05-31 start====================
making sample. (inference: True, date:2019-06-30 00:00:00, date_number: 390)
making sample. (inference: False, date:2019-05-31 00:00:00, date_number: 389)
[AttentionModel/2019-06-30 | get_logger | INFO]: ====================AttentionModel/2019-06-30 start====================
[strategy_integration.py] set_seed(2019-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5797)training: 2/10 (0.5797)validation : 2/10 (0.5779)training: 3/10 (0.5779)validation : 3/10 (0.5779)training: 4/10 (0.5779)validation : 4/10 (0.5779)training: 5/10 (0.5779)validation : 5/10 (0.5779)training: 6/10 (0.5779)validation : 6/10 (0.5779)training: 7/10 (0.5779)validation : 7/10 (0.5779)early stopping at 7 with loss 0.5779
AttentionModel-training is done: 7/10
2019-05-31 | reset count: 0 | final loss: 0.5779 at epoch 2
[strategy_integration.py] set_seed(2019-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5799)training: 2/10 (0.5799)validation : 2/10 (0.5765)training: 3/10 (0.5765)validation : 3/10 (0.5765)training: 4/10 (0.5765)validation : 4/10 (0.5765)training: 5/10 (0.5765)validation : 5/10 (0.5765)training: 6/10 (0.5765)validation : 6/10 (0.5751)training: 7/10 (0.5751)validation : 7/10 (0.5751)early stopping at 7 with loss 0.5751
AttentionModel-training is done: 7/10
2019-06-30 | reset count: 0 | final loss: 0.5751 at epoch 6
making sample. (inference: True, date:2019-07-31 00:00:00, date_number: 391)
making sample. (inference: False, date:2019-06-30 00:00:00, date_number: 390)
[AttentionModel/2019-07-31 | get_logger | INFO]: ====================AttentionModel/2019-07-31 start====================
making sample. (inference: True, date:2019-08-31 00:00:00, date_number: 392)
making sample. (inference: False, date:2019-07-31 00:00:00, date_number: 391)
[AttentionModel/2019-08-31 | get_logger | INFO]: ====================AttentionModel/2019-08-31 start====================
[strategy_integration.py] set_seed(2019-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5906)training: 2/10 (0.5906)validation : 2/10 (0.5856)training: 3/10 (0.5856)validation : 3/10 (0.5833)training: 4/10 (0.5833)validation : 4/10 (0.5833)training: 5/10 (0.5833)validation : 5/10 (0.5833)training: 6/10 (0.5833)validation : 6/10 (0.5833)early stopping at 6 with loss 0.5833
AttentionModel-training is done: 6/10
2019-07-31 | reset count: 0 | final loss: 0.5833 at epoch 3
[strategy_integration.py] set_seed(2019-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5901)training: 2/10 (0.5901)validation : 2/10 (0.5888)training: 3/10 (0.5888)validation : 3/10 (0.5847)training: 4/10 (0.5847)validation : 4/10 (0.5832)training: 5/10 (0.5832)validation : 5/10 (0.5832)training: 6/10 (0.5832)validation : 6/10 (0.5832)early stopping at 6 with loss 0.5832
AttentionModel-training is done: 6/10
2019-08-31 | reset count: 0 | final loss: 0.5832 at epoch 4
making sample. (inference: True, date:2019-09-30 00:00:00, date_number: 393)
making sample. (inference: False, date:2019-08-31 00:00:00, date_number: 392)
[AttentionModel/2019-09-30 | get_logger | INFO]: ====================AttentionModel/2019-09-30 start====================
making sample. (inference: True, date:2019-10-31 00:00:00, date_number: 394)
making sample. (inference: False, date:2019-09-30 00:00:00, date_number: 393)
[AttentionModel/2019-10-31 | get_logger | INFO]: ====================AttentionModel/2019-10-31 start====================
[strategy_integration.py] set_seed(2019-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5858)training: 2/10 (0.5858)validation : 2/10 (0.5839)training: 3/10 (0.5839)validation : 3/10 (0.5839)training: 4/10 (0.5839)validation : 4/10 (0.5839)training: 5/10 (0.5839)validation : 5/10 (0.5839)early stopping at 5 with loss 0.5839
AttentionModel-training is done: 5/10
2019-09-30 | reset count: 0 | final loss: 0.5839 at epoch 2
making sample. (inference: True, date:2019-11-30 00:00:00, date_number: 395)
making sample. (inference: False, date:2019-10-31 00:00:00, date_number: 394)
[AttentionModel/2019-11-30 | get_logger | INFO]: ====================AttentionModel/2019-11-30 start====================
[strategy_integration.py] set_seed(2019-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5963)training: 2/10 (0.5963)validation : 2/10 (0.5938)training: 3/10 (0.5938)validation : 3/10 (0.5909)training: 4/10 (0.5909)validation : 4/10 (0.5909)training: 5/10 (0.5909)validation : 5/10 (0.5878)training: 6/10 (0.5878)validation : 6/10 (0.5878)training: 7/10 (0.5878)validation : 7/10 (0.5878)training: 8/10 (0.5878)validation : 8/10 (0.5878)early stopping at 8 with loss 0.5878
AttentionModel-training is done: 8/10
2019-10-31 | reset count: 0 | final loss: 0.5878 at epoch 5
making sample. (inference: True, date:2019-12-31 00:00:00, date_number: 396)
making sample. (inference: False, date:2019-11-30 00:00:00, date_number: 395)
[strategy_integration.py] set_seed(2019-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.6003)training: 2/10 (0.6003)validation : 2/10 (0.5957)training: 3/10 (0.5957)validation : 3/10 (0.5957)training: 4/10 (0.5957)validation : 4/10 (0.5955)training: 5/10 (0.5955)validation : 5/10 (0.5952)training: 6/10 (0.5952)validation : 6/10 (0.5952)training: 7/10 (0.5952)validation : 7/10 (0.5949)training: 8/10 (0.5949)validation : 8/10 (0.5949)early stopping at 8 with loss 0.5949
AttentionModel-training is done: 8/10
2019-11-30 | reset count: 0 | final loss: 0.5949 at epoch 7
[AttentionModel/2019-12-31 | get_logger | INFO]: ====================AttentionModel/2019-12-31 start====================
making sample. (inference: True, date:2020-01-31 00:00:00, date_number: 397)
making sample. (inference: False, date:2019-12-31 00:00:00, date_number: 396)
[AttentionModel/2020-01-31 | get_logger | INFO]: ====================AttentionModel/2020-01-31 start====================
[strategy_integration.py] set_seed(2019-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5839)training: 2/10 (0.5839)validation : 2/10 (0.5762)training: 3/10 (0.5762)validation : 3/10 (0.5762)training: 4/10 (0.5762)validation : 4/10 (0.5762)training: 5/10 (0.5762)validation : 5/10 (0.5762)early stopping at 5 with loss 0.5762
AttentionModel-training is done: 5/10
2019-12-31 | reset count: 0 | final loss: 0.5762 at epoch 2
making sample. (inference: True, date:2020-02-29 00:00:00, date_number: 398)
making sample. (inference: False, date:2020-01-31 00:00:00, date_number: 397)
[AttentionModel/2020-02-29 | get_logger | INFO]: ====================AttentionModel/2020-02-29 start====================
[strategy_integration.py] set_seed(2020-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.6028)training: 2/10 (0.6028)validation : 2/10 (0.5972)training: 3/10 (0.5972)validation : 3/10 (0.5969)training: 4/10 (0.5969)validation : 4/10 (0.5969)training: 5/10 (0.5969)validation : 5/10 (0.5920)training: 6/10 (0.5920)validation : 6/10 (0.5920)early stopping at 6 with loss 0.5920
AttentionModel-training is done: 6/10
2020-01-31 | reset count: 0 | final loss: 0.5920 at epoch 5
making sample. (inference: True, date:2020-03-31 00:00:00, date_number: 399)
making sample. (inference: False, date:2020-02-29 00:00:00, date_number: 398)
[AttentionModel/2020-03-31 | get_logger | INFO]: ====================AttentionModel/2020-03-31 start====================
[strategy_integration.py] set_seed(2020-02-29) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5881)training: 2/10 (0.5881)validation : 2/10 (0.5881)training: 3/10 (0.5881)validation : 3/10 (0.5878)training: 4/10 (0.5878)validation : 4/10 (0.5866)training: 5/10 (0.5866)validation : 5/10 (0.5827)training: 6/10 (0.5827)validation : 6/10 (0.5827)training: 7/10 (0.5827)validation : 7/10 (0.5827)training: 8/10 (0.5827)validation : 8/10 (0.5827)early stopping at 8 with loss 0.5827
AttentionModel-training is done: 8/10
2020-02-29 | reset count: 0 | final loss: 0.5827 at epoch 5
[strategy_integration.py] set_seed(2020-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5831)training: 2/10 (0.5831)validation : 2/10 (0.5831)training: 3/10 (0.5831)validation : 3/10 (0.5831)training: 4/10 (0.5831)validation : 4/10 (0.5809)training: 5/10 (0.5809)validation : 5/10 (0.5809)training: 6/10 (0.5809)validation : 6/10 (0.5809)early stopping at 6 with loss 0.5809
AttentionModel-training is done: 6/10
2020-03-31 | reset count: 0 | final loss: 0.5809 at epoch 4
making sample. (inference: True, date:2020-04-30 00:00:00, date_number: 400)
making sample. (inference: False, date:2020-03-31 00:00:00, date_number: 399)
[AttentionModel/2020-04-30 | get_logger | INFO]: ====================AttentionModel/2020-04-30 start====================
making sample. (inference: True, date:2020-05-31 00:00:00, date_number: 401)
making sample. (inference: False, date:2020-04-30 00:00:00, date_number: 400)
[AttentionModel/2020-05-31 | get_logger | INFO]: ====================AttentionModel/2020-05-31 start====================
[strategy_integration.py] set_seed(2020-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5806)training: 2/10 (0.5806)validation : 2/10 (0.5742)training: 3/10 (0.5742)validation : 3/10 (0.5742)training: 4/10 (0.5742)validation : 4/10 (0.5742)training: 5/10 (0.5742)validation : 5/10 (0.5728)training: 6/10 (0.5728)validation : 6/10 (0.5728)training: 7/10 (0.5728)validation : 7/10 (0.5728)early stopping at 7 with loss 0.5728
AttentionModel-training is done: 7/10
2020-04-30 | reset count: 0 | final loss: 0.5728 at epoch 5
[strategy_integration.py] set_seed(2020-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5793)training: 2/10 (0.5793)validation : 2/10 (0.5776)training: 3/10 (0.5776)validation : 3/10 (0.5745)training: 4/10 (0.5745)validation : 4/10 (0.5745)training: 5/10 (0.5745)validation : 5/10 (0.5736)training: 6/10 (0.5736)validation : 6/10 (0.5736)early stopping at 6 with loss 0.5736
AttentionModel-training is done: 6/10
2020-05-31 | reset count: 0 | final loss: 0.5736 at epoch 5
making sample. (inference: True, date:2020-06-30 00:00:00, date_number: 402)
making sample. (inference: False, date:2020-05-31 00:00:00, date_number: 401)
[AttentionModel/2020-06-30 | get_logger | INFO]: ====================AttentionModel/2020-06-30 start====================
making sample. (inference: True, date:2020-07-31 00:00:00, date_number: 403)
making sample. (inference: False, date:2020-06-30 00:00:00, date_number: 402)
[AttentionModel/2020-07-31 | get_logger | INFO]: ====================AttentionModel/2020-07-31 start====================
[strategy_integration.py] set_seed(2020-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5829)training: 2/10 (0.5829)validation : 2/10 (0.5801)training: 3/10 (0.5801)validation : 3/10 (0.5801)training: 4/10 (0.5801)validation : 4/10 (0.5792)training: 5/10 (0.5792)validation : 5/10 (0.5792)training: 6/10 (0.5792)validation : 6/10 (0.5792)early stopping at 6 with loss 0.5792
AttentionModel-training is done: 6/10
2020-06-30 | reset count: 0 | final loss: 0.5792 at epoch 4
[strategy_integration.py] set_seed(2020-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5778)training: 2/10 (0.5778)validation : 2/10 (0.5776)training: 3/10 (0.5776)validation : 3/10 (0.5771)training: 4/10 (0.5771)validation : 4/10 (0.5771)training: 5/10 (0.5771)validation : 5/10 (0.5755)training: 6/10 (0.5755)validation : 6/10 (0.5755)early stopping at 6 with loss 0.5755
AttentionModel-training is done: 6/10
2020-07-31 | reset count: 0 | final loss: 0.5755 at epoch 5
making sample. (inference: True, date:2020-08-31 00:00:00, date_number: 404)
making sample. (inference: False, date:2020-07-31 00:00:00, date_number: 403)
[AttentionModel/2020-08-31 | get_logger | INFO]: ====================AttentionModel/2020-08-31 start====================
making sample. (inference: True, date:2020-09-30 00:00:00, date_number: 405)
making sample. (inference: False, date:2020-08-31 00:00:00, date_number: 404)
[AttentionModel/2020-09-30 | get_logger | INFO]: ====================AttentionModel/2020-09-30 start====================
[strategy_integration.py] set_seed(2020-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5809)training: 2/10 (0.5809)validation : 2/10 (0.5796)training: 3/10 (0.5796)validation : 3/10 (0.5796)training: 4/10 (0.5796)validation : 4/10 (0.5796)training: 5/10 (0.5796)validation : 5/10 (0.5796)early stopping at 5 with loss 0.5796
AttentionModel-training is done: 5/10
2020-08-31 | reset count: 0 | final loss: 0.5796 at epoch 2
making sample. (inference: True, date:2020-10-31 00:00:00, date_number: 406)
making sample. (inference: False, date:2020-09-30 00:00:00, date_number: 405)
[AttentionModel/2020-10-31 | get_logger | INFO]: ====================AttentionModel/2020-10-31 start====================
[strategy_integration.py] set_seed(2020-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5716)training: 2/10 (0.5716)validation : 2/10 (0.5681)training: 3/10 (0.5681)validation : 3/10 (0.5681)training: 4/10 (0.5681)validation : 4/10 (0.5681)training: 5/10 (0.5681)validation : 5/10 (0.5681)training: 6/10 (0.5681)validation : 6/10 (0.5681)training: 7/10 (0.5681)validation : 7/10 (0.5663)training: 8/10 (0.5663)validation : 8/10 (0.5663)early stopping at 8 with loss 0.5663
AttentionModel-training is done: 8/10
2020-09-30 | reset count: 0 | final loss: 0.5663 at epoch 7
[strategy_integration.py] set_seed(2020-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5749)training: 2/10 (0.5749)validation : 2/10 (0.5749)training: 3/10 (0.5749)validation : 3/10 (0.5749)training: 4/10 (0.5749)validation : 4/10 (0.5749)training: 5/10 (0.5749)validation : 5/10 (0.5749)early stopping at 5 with loss 0.5749
AttentionModel-training is done: 5/10
2020-10-31 | reset count: 0 | final loss: 0.5749 at epoch 1
making sample. (inference: True, date:2020-11-30 00:00:00, date_number: 407)
making sample. (inference: False, date:2020-10-31 00:00:00, date_number: 406)
[AttentionModel/2020-11-30 | get_logger | INFO]: ====================AttentionModel/2020-11-30 start====================
making sample. (inference: True, date:2020-12-31 00:00:00, date_number: 408)
making sample. (inference: False, date:2020-11-30 00:00:00, date_number: 407)
[AttentionModel/2020-12-31 | get_logger | INFO]: ====================AttentionModel/2020-12-31 start====================
[strategy_integration.py] set_seed(2020-11-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5705)training: 2/10 (0.5705)validation : 2/10 (0.5692)training: 3/10 (0.5692)validation : 3/10 (0.5692)training: 4/10 (0.5692)validation : 4/10 (0.5679)training: 5/10 (0.5679)validation : 5/10 (0.5679)early stopping at 5 with loss 0.5679
AttentionModel-training is done: 5/10
2020-11-30 | reset count: 0 | final loss: 0.5679 at epoch 4
making sample. (inference: True, date:2021-01-31 00:00:00, date_number: 409)
making sample. (inference: False, date:2020-12-31 00:00:00, date_number: 408)
[AttentionModel/2021-01-31 | get_logger | INFO]: ====================AttentionModel/2021-01-31 start====================
[strategy_integration.py] set_seed(2020-12-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5731)training: 2/10 (0.5731)validation : 2/10 (0.5731)training: 3/10 (0.5731)validation : 3/10 (0.5711)training: 4/10 (0.5711)validation : 4/10 (0.5711)training: 5/10 (0.5711)validation : 5/10 (0.5711)training: 6/10 (0.5711)validation : 6/10 (0.5711)early stopping at 6 with loss 0.5711
AttentionModel-training is done: 6/10
2020-12-31 | reset count: 0 | final loss: 0.5711 at epoch 3
making sample. (inference: True, date:2021-02-28 00:00:00, date_number: 410)
making sample. (inference: False, date:2021-01-31 00:00:00, date_number: 409)
[AttentionModel/2021-02-28 | get_logger | INFO]: ====================AttentionModel/2021-02-28 start====================
[strategy_integration.py] set_seed(2021-01-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5505)training: 2/10 (0.5505)validation : 2/10 (0.5480)training: 3/10 (0.5480)validation : 3/10 (0.5480)training: 4/10 (0.5480)validation : 4/10 (0.5480)training: 5/10 (0.5480)validation : 5/10 (0.5480)training: 6/10 (0.5480)validation : 6/10 (0.5480)training: 7/10 (0.5480)validation : 7/10 (0.5472)training: 8/10 (0.5472)validation : 8/10 (0.5472)training: 9/10 (0.5472)validation : 9/10 (0.5466)training: 10/10 (0.5466)validation : 10/10 (0.5466)early stopping at 10 with loss 0.5466
AttentionModel-training is done: 10/10
2021-01-31 | reset count: 0 | final loss: 0.5466 at epoch 9
[strategy_integration.py] set_seed(2021-02-28) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5690)training: 2/10 (0.5690)validation : 2/10 (0.5668)training: 3/10 (0.5668)validation : 3/10 (0.5668)training: 4/10 (0.5668)validation : 4/10 (0.5618)training: 5/10 (0.5618)validation : 5/10 (0.5618)training: 6/10 (0.5618)validation : 6/10 (0.5618)training: 7/10 (0.5618)validation : 7/10 (0.5618)early stopping at 7 with loss 0.5618
AttentionModel-training is done: 7/10
2021-02-28 | reset count: 0 | final loss: 0.5618 at epoch 4
making sample. (inference: True, date:2021-03-31 00:00:00, date_number: 411)
making sample. (inference: False, date:2021-02-28 00:00:00, date_number: 410)
[AttentionModel/2021-03-31 | get_logger | INFO]: ====================AttentionModel/2021-03-31 start====================
making sample. (inference: True, date:2021-04-30 00:00:00, date_number: 412)
making sample. (inference: False, date:2021-03-31 00:00:00, date_number: 411)
[AttentionModel/2021-04-30 | get_logger | INFO]: ====================AttentionModel/2021-04-30 start====================
[strategy_integration.py] set_seed(2021-03-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5773)training: 2/10 (0.5773)validation : 2/10 (0.5769)training: 3/10 (0.5769)validation : 3/10 (0.5734)training: 4/10 (0.5734)validation : 4/10 (0.5734)training: 5/10 (0.5734)validation : 5/10 (0.5734)training: 6/10 (0.5734)validation : 6/10 (0.5734)early stopping at 6 with loss 0.5734
AttentionModel-training is done: 6/10
2021-03-31 | reset count: 0 | final loss: 0.5734 at epoch 3
[strategy_integration.py] set_seed(2021-04-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5784)training: 2/10 (0.5784)validation : 2/10 (0.5769)training: 3/10 (0.5769)validation : 3/10 (0.5752)training: 4/10 (0.5752)validation : 4/10 (0.5752)training: 5/10 (0.5752)validation : 5/10 (0.5752)early stopping at 5 with loss 0.5752
AttentionModel-training is done: 5/10
2021-04-30 | reset count: 0 | final loss: 0.5752 at epoch 3
making sample. (inference: True, date:2021-05-31 00:00:00, date_number: 413)
making sample. (inference: False, date:2021-04-30 00:00:00, date_number: 412)
[AttentionModel/2021-05-31 | get_logger | INFO]: ====================AttentionModel/2021-05-31 start====================
making sample. (inference: True, date:2021-06-30 00:00:00, date_number: 414)
making sample. (inference: False, date:2021-05-31 00:00:00, date_number: 413)
[AttentionModel/2021-06-30 | get_logger | INFO]: ====================AttentionModel/2021-06-30 start====================
[strategy_integration.py] set_seed(2021-05-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5608)training: 2/10 (0.5608)validation : 2/10 (0.5558)training: 3/10 (0.5558)validation : 3/10 (0.5558)training: 4/10 (0.5558)validation : 4/10 (0.5542)training: 5/10 (0.5542)validation : 5/10 (0.5542)training: 6/10 (0.5542)validation : 6/10 (0.5542)training: 7/10 (0.5542)validation : 7/10 (0.5542)early stopping at 7 with loss 0.5542
AttentionModel-training is done: 7/10
2021-05-31 | reset count: 0 | final loss: 0.5542 at epoch 4
[strategy_integration.py] set_seed(2021-06-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5681)training: 2/10 (0.5681)validation : 2/10 (0.5681)training: 3/10 (0.5681)validation : 3/10 (0.5681)training: 4/10 (0.5681)validation : 4/10 (0.5681)training: 5/10 (0.5681)validation : 5/10 (0.5681)early stopping at 5 with loss 0.5681
AttentionModel-training is done: 5/10
2021-06-30 | reset count: 0 | final loss: 0.5681 at epoch 1
making sample. (inference: True, date:2021-07-31 00:00:00, date_number: 415)
making sample. (inference: False, date:2021-06-30 00:00:00, date_number: 414)
[AttentionModel/2021-07-31 | get_logger | INFO]: ====================AttentionModel/2021-07-31 start====================
making sample. (inference: True, date:2021-08-31 00:00:00, date_number: 416)
making sample. (inference: False, date:2021-07-31 00:00:00, date_number: 415)
[AttentionModel/2021-08-31 | get_logger | INFO]: ====================AttentionModel/2021-08-31 start====================
[strategy_integration.py] set_seed(2021-07-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5798)training: 2/10 (0.5798)validation : 2/10 (0.5770)training: 3/10 (0.5770)validation : 3/10 (0.5770)training: 4/10 (0.5770)validation : 4/10 (0.5770)training: 5/10 (0.5770)validation : 5/10 (0.5745)training: 6/10 (0.5745)validation : 6/10 (0.5701)training: 7/10 (0.5701)validation : 7/10 (0.5701)training: 8/10 (0.5701)validation : 8/10 (0.5701)early stopping at 8 with loss 0.5701
AttentionModel-training is done: 8/10
2021-07-31 | reset count: 0 | final loss: 0.5701 at epoch 6
[strategy_integration.py] set_seed(2021-08-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5660)training: 2/10 (0.5660)validation : 2/10 (0.5660)training: 3/10 (0.5660)validation : 3/10 (0.5660)training: 4/10 (0.5660)validation : 4/10 (0.5660)training: 5/10 (0.5660)validation : 5/10 (0.5660)early stopping at 5 with loss 0.5660
AttentionModel-training is done: 5/10
2021-08-31 | reset count: 0 | final loss: 0.5660 at epoch 1
making sample. (inference: True, date:2021-09-30 00:00:00, date_number: 417)
making sample. (inference: False, date:2021-08-31 00:00:00, date_number: 416)
[AttentionModel/2021-09-30 | get_logger | INFO]: ====================AttentionModel/2021-09-30 start====================
making sample. (inference: True, date:2021-10-31 00:00:00, date_number: 418)
making sample. (inference: False, date:2021-09-30 00:00:00, date_number: 417)
[AttentionModel/2021-10-31 | get_logger | INFO]: ====================AttentionModel/2021-10-31 start====================
[strategy_integration.py] set_seed(2021-09-30) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5903)training: 2/10 (0.5903)validation : 2/10 (0.5823)training: 3/10 (0.5823)validation : 3/10 (0.5808)training: 4/10 (0.5808)validation : 4/10 (0.5808)training: 5/10 (0.5808)validation : 5/10 (0.5808)training: 6/10 (0.5808)validation : 6/10 (0.5808)early stopping at 6 with loss 0.5808
AttentionModel-training is done: 6/10
2021-09-30 | reset count: 0 | final loss: 0.5808 at epoch 3
[strategy_integration.py] set_seed(2021-10-31) is working on static torch seed, True.
AttentionModel model initialized
training: 1/10 (100000000.0000)validation : 1/10 (0.5799)training: 2/10 (0.5799)validation : 2/10 (0.5790)training: 3/10 (0.5790)validation : 3/10 (0.5790)training: 4/10 (0.5790)validation : 4/10 (0.5790)training: 5/10 (0.5790)validation : 5/10 (0.5790)early stopping at 5 with loss 0.5790
AttentionModel-training is done: 5/10
2021-10-31 | reset count: 0 | final loss: 0.5790 at epoch 2
[strategy | get_logger | INFO]: ====================strategy start====================
[strategy | _get_infer_dirs | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick350_test03/infer 이 strategy 대상에 추가됨
[strategy | _backtest | INFO]: /home/sronly/sr-storage/Harvest_Green_concept001_pick350_test03/infer is starting
⠋ [Compustat API] : Loading meta table✓ [Compustat API] : Loading meta table done in 0.161 secs
⠋ [Compustat API] : Loading masking data⠋ [Compustat API] : Loading get_historic_universe from cache⠙ [Compustat API] : Loading masking data⠙ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading get_historic_universe from cache⠹ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading masking data⠸ [Compustat API] : Loading get_historic_universe from cache⠼ [Compustat API] : Loading masking data⠼ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading get_historic_universe from cache⠴ [Compustat API] : Loading masking data✓ [Compustat API] : Loading get_historic_universe from cache done in 1.783 secs
⠦ [Compustat API] : Loading masking data✓ [Compustat API] : Loading masking data done in 2.084 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.063 secs
⠋ [Compustat API] : Loading get_monthly_total_return from cache✓ [Compustat API] : Loading get_monthly_total_return from cache done in 0.062 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.076 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.072 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.07 secs
⠋ [Compustat API] : Loading get_monthly_volume_data from cache✓ [Compustat API] : Loading get_monthly_volume_data from cache done in 0.08 secs
[strategy | _allocating | INFO]: [STRATEGY] allocating start
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] infer_weight_analysis.html is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] performance.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] combined.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] weight.csv is saved.
[strategy | _backtest_with_given_directory | INFO]: [STRATEGY] universe.csv is saved.
[comparison | get_logger | INFO]: ====================comparison start====================
[comparison | compare | INFO]: [5. COMPARISON] comparison is saved.
[comparison | compare | INFO]:                        Harvest_Green_concept001_pick350_test03_price_return  ...  SPCOMP_total_win
2016-01-31_2021-04-30                                           0.223163     ...          0.765625

[1 rows x 27 columns]
2015-12-31
2016-01-31
2016-02-29
2016-03-31
2016-04-30
2016-05-31
2016-06-30
2016-07-31
2016-08-31
2016-09-30
2016-10-31
2016-11-30
2016-12-31
2017-01-31
2017-02-28
2017-03-31
2017-04-30
2017-05-31
2017-06-30
2017-07-31
2017-08-31
2017-09-30
2017-10-31
2017-11-30
2017-12-31
2018-01-31
2018-02-28
2018-03-31
2018-04-30
2018-05-31
2018-06-30
2018-07-31
2018-08-31
2018-09-30
2018-10-31
2018-11-30
2018-12-31
2019-01-31
2019-02-28
2019-03-31
2019-04-30
2019-05-31
2019-06-30
2019-07-31
2019-08-31
2019-09-30
2019-10-31
2019-11-30
2019-12-31
2020-01-31
2020-02-29
2020-03-31
2020-04-30
2020-05-31
2020-06-30
2020-07-31
2020-08-31
2020-09-30
2020-10-31
2020-11-30
2020-12-31
2021-01-31
2021-02-28
2021-03-31
2021-04-30
2021-05-31
2021-06-30
2021-07-31
2021-08-31
2021-09-30
2021-10-31
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.046 secs
⠋ [Compustat API] : Loading get_historical_gics from cache✓ [Compustat API] : Loading get_historical_gics from cache done in 0.044 secs
⠋ [Compustat API] : Loading get_monthly_price_return from cache✓ [Compustat API] : Loading get_monthly_price_return from cache done in 0.063 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.077 secs
⠋ [Compustat API] : Loading get_monthly_price_data from cache✓ [Compustat API] : Loading get_monthly_price_data from cache done in 0.071 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.679 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.677 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.678 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.68 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.681 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.076 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.666 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.665 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.663 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.659 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.657 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.078 secs
⠋ [Compustat API] : Loading get_fundamental_data from cache⠙ [Compustat API] : Loading get_fundamental_data from cache✓ [Compustat API] : Loading get_fundamental_data from cache done in 0.679 secs
⠋ [Compustat API] : Loading get_monthly_market_value from cache✓ [Compustat API] : Loading get_monthly_market_value from cache done in 0.075 secs
